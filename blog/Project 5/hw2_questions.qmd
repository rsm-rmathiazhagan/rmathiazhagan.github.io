---
title: "Poisson Regression Examples"
author: "Rishikumar Mathiazhagan"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.

:::: {.callout-note collapse="true"}
### Data
```{r load-data, message=FALSE, warning=FALSE}
#_todo: Read in data._

# Load necessary libraries
library(tidyverse)

# Read in the Blueprinty dataset
blueprinty_data <- read_csv("blueprinty.csv")

# Preview the first few rows
head(blueprinty_data)
```

```{r patents-by-customer, message=FALSE, warning=FALSE}
library(tidyverse)

# Histogram of patents by customer status
ggplot(blueprinty_data, aes(x = patents, fill = as.factor(iscustomer))) +
  geom_histogram(binwidth = 1, position = "dodge", color = "black") +
  scale_fill_manual(values = c("#999999", "#0072B2"),
                    name = "Customer Status",
                    labels = c("Non-Customer", "Customer")) +
  labs(title = "Distribution of Patents by Customer Status",
       x = "Number of Patents (Last 5 Years)",
       y = "Number of Firms") +
  theme_minimal()
```

```{r mean-patents}
# Mean patents by customer status
blueprinty_data %>%
  group_by(iscustomer) %>%
  summarise(mean_patents = mean(patents), .groups = "drop")
```
The histogram shows that both Blueprinty customers and non-customers are most frequently awarded between 2 to 6 patents over the last 5 years. However, customers tend to have a rightward shift in the distributionâ€”meaning they are more likely to fall into higher patent count bins. This is consistent with the summary statistics, which show that the average number of patents among non-customers is 3.47, while for customers it is 4.13.

This suggests that, on average, firms using Blueprinty's software are granted more patents. However, this relationship is purely descriptive at this stage and does not account for potential confounders such as firm age or geographic location. Further modeling is needed to isolate the effect of software usage.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{r age-histogram, message=FALSE, warning=FALSE}
ggplot(blueprinty_data, aes(x = age, fill = as.factor(iscustomer))) +
  geom_histogram(alpha = 0.6, position = "identity", binwidth = 2, color = "black") +
  scale_fill_manual(values = c("#999999", "#0072B2"),
                    name = "Customer Status",
                    labels = c("Non-Customer", "Customer")) +
  labs(title = "Distribution of Firm Age by Customer Status",
       x = "Years Since Incorporation",
       y = "Number of Firms") +
  theme_minimal()
```


```{r region-barplot}
# Proportional bar plot of region by customer status
ggplot(blueprinty_data, aes(x = region, fill = as.factor(iscustomer))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(values = c("#999999", "#0072B2"),
                    name = "Customer Status",
                    labels = c("Non-Customer", "Customer")) +
  labs(title = "Regional Distribution by Customer Status",
       x = "Region",
       y = "Proportion of Firms") +
  theme_minimal()
```

From the histogram of firm age, we see that Blueprinty customers tend to be slightly older on average than non-customers. While both groups are centered around 25â€“30 years since incorporation, the distribution for customers appears more spread out into the higher-age range (35â€“45 years), whereas non-customers are more concentrated around the mean. This suggests that firm age could be a confounding factor in our analysis and may need to be controlled for when estimating the effect of using Blueprinty software.

The regional bar chart shows that Blueprintyâ€™s customer base is not evenly distributed across regions. Notably, a majority of firms in the Northeast region are customers, while other regions like the Midwest, Northwest, South, and Southwest have lower adoption rates. This uneven distribution suggests that regional effectsâ€”such as varying patenting activity or industry concentrationsâ€”might influence both Blueprinty adoption and patent success, and should be accounted for in any causal analysis.
::::

:::: {.callout-note collapse="true"}
### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.



### Likelihood Function for the Poisson Model

```{r, echo=FALSE, results='asis'}
cat("$$
L(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}
$$")
```

```{r poisson-loglikelihood, message=FALSE, warning=FALSE}
# Define the Poisson log-likelihood function
poisson_loglikelihood <- function(lambda, Y) {
  if (lambda <= 0) {
    return(-Inf)  # log-likelihood is undefined for non-positive lambda
  }
  sum(-lambda + Y * log(lambda) - lfactorial(Y))
}

# Apply it to the observed patent counts
Y_vals <- blueprinty_data$patents

# Evaluate the log-likelihood at the sample mean as a starting point
poisson_loglikelihood(lambda = mean(Y_vals), Y = Y_vals)
```
The Poisson log-likelihood curve rises steeply for small values of 
ðœ†
Î», reaches a peak, and then gradually declines. This confirms the log-likelihood function is unimodal, meaning it has a single maximumâ€”ideal for maximum likelihood estimation. The red dashed line indicates the value of 
ðœ†
Î» that maximizes the log-likelihood, representing the maximum likelihood estimate (MLE). This value closely matches the sample mean of the observed patent counts, which is consistent with the theoretical property that the MLE of 
ðœ†
Î» in a simple Poisson model equals the sample mean.

```{r loglikelihood-plot, message=FALSE, warning=FALSE}
# Define a sequence of lambda values to evaluate
lambda_vals <- seq(0.5, 8, by = 0.1)

# Calculate log-likelihood for each lambda
logliks <- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y_vals))

# Create a data frame for plotting
loglik_df <- data.frame(lambda = lambda_vals, loglik = logliks)

# Plot
ggplot(loglik_df, aes(x = lambda, y = loglik)) +
  geom_line(color = "#0072B2", size = 1) +
  geom_vline(xintercept = lambda_vals[which.max(logliks)], linetype = "dashed", color = "red") +
  labs(title = "Poisson Log-Likelihood Curve",
       x = expression(lambda),
       y = "Log-Likelihood") +
  theme_minimal()
```


### Deriving the MLE for the Poisson Model

We begin with the log-likelihood function for independent observations \( Y_1, \dots, Y_n \sim \text{Poisson}(\lambda) \):

$$
\log L(\lambda) = \sum_{i=1}^n \left[ -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right]
$$

This simplifies to:

$$
\log L(\lambda) = -n\lambda + \left( \sum_{i=1}^n Y_i \right) \log(\lambda) + \text{const}
$$

Now, take the derivative with respect to \( \lambda \):

$$
\frac{d}{d\lambda} \log L(\lambda) = -n + \frac{1}{\lambda} \sum_{i=1}^n Y_i
$$

Set the derivative equal to zero:

$$
-n + \frac{1}{\lambda} \sum_{i=1}^n Y_i = 0
$$

Solving for \( \lambda \) gives:

$$
\hat{\lambda}_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
$$

Thus, the maximum likelihood estimator of \( \lambda \) is the **sample mean** of the observed data.

```{r mle-with-optim, message=FALSE, warning=FALSE}
# Define the negative log-likelihood to minimize
neg_loglikelihood <- function(lambda, Y) {
  if (lambda <= 0) {
    return(Inf)  # invalid values return high cost
  }
  -poisson_loglikelihood(lambda, Y)
}

# Use optim() to minimize the negative log-likelihood
optim_result <- optim(par = 1,  # initial guess for lambda
                      fn = neg_loglikelihood,
                      Y = Y_vals,
                      method = "Brent",
                      lower = 0.01,
                      upper = 10)

# Extract MLE
lambda_hat_optim <- optim_result$par
lambda_hat_optim
```

To estimate the Poisson rate parameter 
ðœ†
Î», we used Râ€™s optim() function to numerically maximize the log-likelihood. Since optim() minimizes by default, we passed it the negative of our log-likelihood function. Using Brentâ€™s method with bounds between 0.01 and 10, the optimizer returned a value of 
ðœ†
Î» that aligns closely with the sample mean of the data. This confirms both our analytical derivation and visual inspection from the log-likelihood plot.
::::

:::: {.callout-note collapse="true"}
### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.


```{r poisson-regression-loglikelihood, message=FALSE, warning=FALSE}
# Define the log-likelihood function for Poisson regression
poisson_regression_loglikelihood <- function(beta, Y, X) {
  # Compute lambda_i for all observations
  lambda <- exp(X %*% beta)
  
  # Compute the log-likelihood
  sum(-lambda + Y * log(lambda) - lfactorial(Y))
}
```


```{r poisson-regression-setup, message=FALSE, warning=FALSE}
# Create model matrix (design matrix) with intercept, age, age^2, region dummies, and iscustomer
blueprinty_data <- blueprinty_data %>%
  mutate(age_sq = age^2)

X <- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty_data)
Y <- blueprinty_data$patents
```
```{r poisson-regression-optim, message=FALSE, warning=FALSE}
# Use optim to estimate beta
poisson_regression_loglikelihood <- function(beta, Y, X) {
  lambda <- exp(X %*% beta)
  -sum(-lambda + Y * log(lambda) - lfactorial(Y))
}

# Run optimization
optim_result <- optim(par = rep(0, ncol(X)),
                      fn = poisson_regression_loglikelihood,
                      Y = Y, X = X,
                      hessian = TRUE,
                      method = "BFGS",
                      control = list(reltol = 1e-10))

# Extract coefficients
beta_hat <- optim_result$par

# Compute standard errors using the inverse of the Hessian
hessian <- optim_result$hessian
var_cov_matrix <- solve(hessian)
std_errors <- sqrt(diag(var_cov_matrix))
```

```{r poisson-regression-standardize, message=FALSE, warning=FALSE}
# Summarize coefficients and standard errors in a table
coef_table <- data.frame(
  Term = colnames(X),
  Estimate = round(beta_hat, 4),
  Std_Error = round(std_errors, 4)
)

knitr::kable(coef_table, caption = "Poisson Regression Estimates and Standard Errors")
```


```{r poisson-regression-glm, message=FALSE, warning=FALSE}
# Fit the Poisson regression model using glm()
glm_model <- glm(patents ~ age + I(age^2) + region + iscustomer, 
                 data = blueprinty_data, 
                 family = poisson())

# Display a summary
summary(glm_model)
```


The results from both the optim() function and the glm() function are generally consistent, which confirms that the manual maximum likelihood estimation was implemented correctly. The coefficient for the iscustomer variable is positive and statistically significant in the glm() output (estimate = 0.208, p < 0.001). This means that, holding other variables constant, firms that use Blueprinty's software tend to receive more patents than non-customers. Since the model uses a log link function, this coefficient translates to approximately a 23% increase in the expected number of patents (because exp(0.208) is about 1.23).

The firmâ€™s age also plays a significant role. The positive coefficient on age and the negative coefficient on age squared suggest a concave relationship, where patent activity increases with age up to a certain point and then declines. This is typical in lifecycle patterns of firm innovation.

The region variables are not statistically significant, which implies that, after accounting for age and customer status, there are no strong regional differences in patent outcomes.

In summary, the glm() results confirm the manual estimation and support the conclusion that using Blueprintyâ€™s software is associated with a higher number of patents, even after adjusting for firm age and location.


```{r blueprinty-effect-prediction, message=FALSE, warning=FALSE}
# Make copies of the data
data_0 <- blueprinty_data
data_1 <- blueprinty_data

# Set customer status to 0 and 1 respectively
data_0$iscustomer <- 0
data_1$iscustomer <- 1

# Use the fitted glm model to predict expected patent counts
y_pred_0 <- predict(glm_model, newdata = data_0, type = "response")
y_pred_1 <- predict(glm_model, newdata = data_1, type = "response")

# Compute the average effect
average_difference <- mean(y_pred_1 - y_pred_0)
average_difference
```
Using the fitted Poisson regression model, we estimated the expected number of patents for each firm under two hypothetical scenarios: one where no firms use Blueprintyâ€™s software and one where all firms do. The average difference between these two sets of predictions is approximately 0.79 patents per firm over five years. This suggests that, controlling for age and region, firms that use Blueprintyâ€™s software are expected to receive nearly one additional patent on average compared to similar firms that do not use the software. This result provides strong evidence that Blueprinty's tool may be associated with a meaningful improvement in patenting success.


::::

## AirBnB Case Study
### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._

:::: {.callout-note collapse="true"}
### Exploratory Data Analysis

```{r summary-table-custom}
library(tidyverse)

# Load the data
airbnb <- read_csv("airbnb.csv")

# View variable summaries
summary(airbnb)
```
```{r}
# Check for missing values in relevant columns
airbnb %>%
  select(number_of_reviews, bathrooms, bedrooms, price,
         review_scores_cleanliness, review_scores_location,
         review_scores_value, instant_bookable, room_type) %>%
  summarise_all(~sum(is.na(.)))

# Drop rows with any missing values in relevant variables
airbnb_clean <- airbnb %>%
  filter(!is.na(number_of_reviews),
         !is.na(bathrooms),
         !is.na(bedrooms),
         !is.na(price),
         !is.na(review_scores_cleanliness),
         !is.na(review_scores_location),
         !is.na(review_scores_value),
         !is.na(instant_bookable),
         !is.na(room_type))
```
```{r row-counts-before-after, message=FALSE}
# Number of rows before cleaning
n_before <- nrow(airbnb)

# Number of rows after cleaning
n_after <- nrow(airbnb_clean)

# Create a simple comparison table
tibble(
  Stage = c("Before Cleaning", "After Cleaning"),
  Rows = c(n_before, n_after)
) %>%
  knitr::kable(caption = "Number of Rows Before and After Dropping Missing Values")
```
```{r airbnb-histograms, message=FALSE, warning=FALSE}
# Histogram of number of reviews
ggplot(airbnb_clean, aes(x = number_of_reviews)) +
  geom_histogram(binwidth = 10, fill = "#0072B2", color = "black") +
  labs(title = "Distribution of Number of Reviews",
       x = "Number of Reviews",
       y = "Count of Listings") +
  theme_minimal()

# Histogram of price
ggplot(airbnb_clean, aes(x = price)) +
  geom_histogram(binwidth = 25, fill = "#009E73", color = "black") +
  labs(title = "Distribution of Price per Night",
       x = "Price (USD)",
       y = "Count of Listings") +
  theme_minimal()

# Histogram of bedrooms
ggplot(airbnb_clean, aes(x = bedrooms)) +
  geom_histogram(binwidth = 1, fill = "#D55E00", color = "black") +
  labs(title = "Distribution of Bedrooms",
       x = "Number of Bedrooms",
       y = "Count of Listings") +
  theme_minimal()
```
::::

:::: {.callout-note collapse="true"}
### Modeling

```{r airbnb-poisson-model, message=FALSE, warning=FALSE}
# Fit a Poisson regression model
poisson_model <- glm(number_of_reviews ~ room_type + bathrooms + bedrooms + price +
                       review_scores_cleanliness + review_scores_location +
                       review_scores_value + instant_bookable,
                     data = airbnb_clean,
                     family = poisson())

# Display model summary
summary(poisson_model)
```

The Poisson model assumes that the mean and variance of the outcome are equal. However, in practice (especially with review counts), overdispersion is common â€” meaning the variance is greater than the mean.

To account for this, we fit a Negative Binomial Regression, which introduces a dispersion parameter to relax the equal-mean-variance constraint. It's a natural extension of the Poisson model.

```{r airbnb-negative-binomial, message=FALSE, warning=FALSE}
# Load MASS for negative binomial
library(MASS)

# Fit negative binomial model
nb_model <- glm.nb(number_of_reviews ~ room_type + bathrooms + bedrooms + price +
                     review_scores_cleanliness + review_scores_location +
                     review_scores_value + instant_bookable,
                   data = airbnb_clean)

# Display model summary
summary(nb_model)
```

Both the Poisson and Negative Binomial regression models aim to explain variation in the number of Airbnb reviews (used as a proxy for bookings) as a function of property characteristics.

Key Findings:
Room Type: Compared to the baseline category (Entire home/apt), listings that are Private rooms or Shared rooms consistently receive fewer reviews. In the Poisson model, these have large negative coefficients, and in the Negative Binomial model, Shared room remains significantly negative, while Private room becomes statistically insignificant. This suggests that whole units are more popular/booked more frequently.

Bathrooms: The coefficient for bathrooms is negative and statistically significant in both models. This may reflect diminishing returns â€” beyond a certain point, more bathrooms don't contribute to more bookings.

Bedrooms: The number of bedrooms has a strong, positive, and significant effect. Each additional bedroom is associated with a substantial increase in the expected number of reviews, suggesting that larger accommodations are more frequently booked.

Price: The effect of price is small and not consistently significant. In both models, the coefficients are near zero and not statistically meaningful, implying that price alone is not a strong predictor of bookings when other factors are accounted for.

Review Scores:

Cleanliness has a positive and highly significant effect, indicating that higher cleanliness ratings are associated with more reviews/bookings.

Location and Value scores both show significant negative coefficients, which may seem counterintuitive. However, these scores are often compressed near the upper limit (e.g., most listings get 8â€“10), and the negative relationship may reflect lower variance or interactions with other unobserved factors.

Instant Bookable: Listings that are instantly bookable receive significantly more reviews. In the Negative Binomial model, being instant bookable is associated with about a 38% increase in expected review count (exp(0.325) â‰ˆ 1.38), holding all else constant.

Why Use Negative Binomial?
The Negative Binomial model is preferred in this case due to overdispersion â€” the variance in number of reviews greatly exceeds the mean. The Negative Binomial model accounts for this with a dispersion parameter and provides a better fit, as shown by a substantially lower AIC (Poisson AIC: ~1,058,014 vs. NB AIC: ~242,052).
::::






