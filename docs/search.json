[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishikumar Mathiazhagan",
    "section": "",
    "text": "I’m currently pursuing my Master of Science in Business Analytics (MSBA) as part of the 2025 cohort at UC San Diego’s Rady School of Management. With a strong foundation in data analysis, Python, SQL, and data visualization tools like Tableau, I enjoy uncovering insights that drive smarter decisions. I’m also an avid sports enthusiast, always inspired by the intersection of data and performance—whether it’s in business or on the field."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html",
    "href": "blog/Project 3/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo better understand the demand side of charitable giving, Karlan and List used a natural field experiment involving over 50,000 previous donors to a politically liberal nonprofit organization. The letters were randomized to test three main dimensions: whether a matching grant was offered, the ratio of the match (1:1, 2:1, or 3:1), and the size of the matching fund ($25,000, $50,000, $100,000, or unstated). In addition, suggested donation amounts were varied using multipliers of the recipient’s previous highest contribution. Their findings revealed that simply offering a match significantly increased both the likelihood of donating and the average amount raised per letter. However, increasing the match ratio beyond 1:1 did not yield additional benefits, challenging conventional fundraising wisdom. Notably, the effect of matching offers was more pronounced in conservative-leaning (“red”) states, suggesting that political context may shape donor responsiveness.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#introduction",
    "href": "blog/Project 3/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo better understand the demand side of charitable giving, Karlan and List used a natural field experiment involving over 50,000 previous donors to a politically liberal nonprofit organization. The letters were randomized to test three main dimensions: whether a matching grant was offered, the ratio of the match (1:1, 2:1, or 3:1), and the size of the matching fund ($25,000, $50,000, $100,000, or unstated). In addition, suggested donation amounts were varied using multipliers of the recipient’s previous highest contribution. Their findings revealed that simply offering a match significantly increased both the likelihood of donating and the average amount raised per letter. However, increasing the match ratio beyond 1:1 did not yield additional benefits, challenging conventional fundraising wisdom. Notably, the effect of matching offers was more pronounced in conservative-leaning (“red”) states, suggesting that political context may shape donor responsiveness.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#data",
    "href": "blog/Project 3/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nlibrary(haven)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(summarytools)\n\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\nglimpse(data)\n\nRows: 50,083\nColumns: 51\n$ treatment          &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, …\n$ control            &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, …\n$ ratio              &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1,…\n$ ratio2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ ratio3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ size               &lt;dbl+lbl&gt; 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4,…\n$ size25             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ size50             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ size100            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sizeno             &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ask                &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1,…\n$ askd1              &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ askd2              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ askd3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ ask1               &lt;dbl&gt; 55, 25, 55, 55, 35, 95, 125, 75, 250, 150, 125, 25,…\n$ ask2               &lt;dbl&gt; 70, 35, 70, 70, 45, 120, 160, 95, 315, 190, 160, 35…\n$ ask3               &lt;dbl&gt; 85, 50, 85, 85, 55, 145, 190, 120, 375, 225, 190, 5…\n$ amount             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gave               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ amountchange       &lt;dbl&gt; -45, -25, -50, -25, -15, -45, -50, -65, -100, -125,…\n$ hpa                &lt;dbl&gt; 45, 25, 50, 50, 25, 90, 100, 65, 200, 125, 100, 5, …\n$ ltmedmra           &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ freq               &lt;dbl&gt; 2, 2, 3, 15, 42, 20, 12, 13, 28, 4, 1, 1, 2, 80, 3,…\n$ years              &lt;dbl&gt; 4, 3, 2, 8, 95, 10, 8, 16, 19, 7, 3, 1, 6, 19, 3, 1…\n$ year5              &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, …\n$ mrm2               &lt;dbl&gt; 31, 5, 6, 1, 24, 3, 4, 4, 6, 35, 41, 8, 28, 15, 5, …\n$ dormant            &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ female             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ couple             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state50one         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nonlit             &lt;dbl&gt; 5, 0, 3, 1, 1, 0, 0, 4, 1, 4, 4, 1, 1, 4, 0, 3, 6, …\n$ cases              &lt;dbl&gt; 4, 2, 1, 2, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 2, …\n$ statecnt           &lt;dbl&gt; 4.5002995, 2.9822462, 9.6070213, 3.2814682, 2.30201…\n$ stateresponse      &lt;dbl&gt; 0.01994681, 0.02608696, 0.02304817, 0.02066869, 0.0…\n$ stateresponset     &lt;dbl&gt; 0.019502353, 0.027833002, 0.022158911, 0.024702653,…\n$ stateresponsec     &lt;dbl&gt; 0.020806242, 0.022494888, 0.024743512, 0.012681159,…\n$ stateresponsetminc &lt;dbl&gt; -0.001303889, 0.005338114, -0.002584601, 0.01202149…\n$ perbush            &lt;dbl&gt; 0.4900000, 0.4646465, 0.4081633, 0.4646465, 0.52525…\n$ close25            &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ red0               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, …\n$ blue0              &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, …\n$ redcty             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ bluecty            &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ pwhite             &lt;dbl&gt; 0.4464934, NA, 0.9357064, 0.8883309, 0.7590141, 0.8…\n$ pblack             &lt;dbl&gt; 0.527769208, NA, 0.011948366, 0.010760401, 0.127420…\n$ page18_39          &lt;dbl&gt; 0.3175913, NA, 0.2761282, 0.2794118, 0.4423889, 0.3…\n$ ave_hh_sz          &lt;dbl&gt; 2.10, NA, 2.48, 2.65, 1.85, 2.92, 2.10, 2.47, 2.49,…\n$ median_hhincome    &lt;dbl&gt; 28517, NA, 51175, 79269, 40908, 61779, 54655, 14152…\n$ powner             &lt;dbl&gt; 0.4998072, NA, 0.7219406, 0.9204314, 0.4160721, 0.9…\n$ psch_atlstba       &lt;dbl&gt; 0.32452780, NA, 0.19266793, 0.41214216, 0.43996516,…\n$ pop_propurban      &lt;dbl&gt; 1.0000000, NA, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\ndfSummary(data)\n\nData Frame Summary  \ndata  \nDimensions: 50083 x 51  \nDuplicates: 30  \n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nNo   Variable             Label                                      Stats / Values                  Freqs (% of Valid)      Graph                  Valid      Missing  \n---- -------------------- ------------------------------------------ ------------------------------- ----------------------- ---------------------- ---------- ---------\n1    treatment            Treatment                                  Min  : 0                        0 : 16687 (33.3%)       IIIIII                 50083      0        \n     [numeric]                                                       Mean : 0.7                      1 : 33396 (66.7%)       IIIIIIIIIIIII          (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n2    control              Control                                    Min  : 0                        0 : 33396 (66.7%)       IIIIIIIIIIIII          50083      0        \n     [numeric]                                                       Mean : 0.3                      1 : 16687 (33.3%)       IIIIII                 (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n3    ratio                Match ratio                                Mean (sd) : 1.3 (1.2)           0 : 16687 (33.3%)       IIIIII                 50083      0        \n     [haven_labelled,                                                min &lt; med &lt; max:                1 : 11133 (22.2%)       IIII                   (100.0%)   (0.0%)   \n     vctrs_vctr,                                                     0 &lt; 1 &lt; 3                       2 : 11134 (22.2%)       IIII                                       \n     double]                                                         IQR (CV) : 2 (0.9)              3 : 11129 (22.2%)       IIII                                       \n\n4    ratio2               2:1 match ratio                            Min  : 0                        0 : 38949 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]                                                       Mean : 0.2                      1 : 11134 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n5    ratio3               3:1 match ratio                            Min  : 0                        0 : 38954 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]                                                       Mean : 0.2                      1 : 11129 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n6    size                 Match threshold                            1. [0] Control                  16687 (33.3%)           IIIIII                 50083      0        \n     [haven_labelled,                                                2. [1] $25,000                   8350 (16.7%)           III                    (100.0%)   (0.0%)   \n     vctrs_vctr,                                                     3. [2] $50,000                   8345 (16.7%)           III                                        \n     double]                                                         4. [3] $100,000                  8350 (16.7%)           III                                        \n                                                                     5. [4] Unstated                  8351 (16.7%)           III                                        \n\n7    size25               $25,000 match threshold                    Min  : 0                        0 : 41733 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8350 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n8    size50               $50,000 match threshold                    Min  : 0                        0 : 41738 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8345 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n9    size100              $100,000 match threshold                   Min  : 0                        0 : 41733 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8350 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n10   sizeno               Unstated match threshold                   Min  : 0                        0 : 41732 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8351 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n11   ask                  Suggested donation amount                  1. [0] Control                  16687 (33.3%)           IIIIII                 50083      0        \n     [haven_labelled,                                                2. [1] 1x                       11134 (22.2%)           IIII                   (100.0%)   (0.0%)   \n     vctrs_vctr,                                                     3. [2] 1.25x                    11133 (22.2%)           IIII                                       \n     double]                                                         4. [3] 1.50x                    11129 (22.2%)           IIII                                       \n\n12   askd1                Suggested donation was highest previous    Min  : 0                        0 : 38949 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]            contribution                               Mean : 0.2                      1 : 11134 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n13   askd2                Suggested donation was 1.25 x highest      Min  : 0                        0 : 38950 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]            previous contribution                      Mean : 0.2                      1 : 11133 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n14   askd3                Suggested donation was 1.50 x highest      Min  : 0                        0 : 38954 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]            previous contribution                      Mean : 0.2                      1 : 11129 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n15   ask1                 Highest previous contribution (for         Mean (sd) : 71.5 (101.7)        18 distinct values      :                      50083      0        \n     [numeric]            suggestion)                                min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     25 &lt; 45 &lt; 1500                                          :                                          \n                                                                     IQR (CV) : 30 (1.4)                                     :                                          \n                                                                                                                             :                                          \n\n16   ask2                 1.25 x highest previous contribution       Mean (sd) : 91.8 (127.3)        18 distinct values      :                      50083      0        \n     [numeric]            (for suggestion)                           min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     35 &lt; 60 &lt; 1875                                          :                                          \n                                                                     IQR (CV) : 40 (1.4)                                     :                                          \n                                                                                                                             :                                          \n\n17   ask3                 1.50 x highest previous contribution       Mean (sd) : 111 (151.7)         18 distinct values      :                      50083      0        \n     [numeric]            (for suggestion)                           min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     50 &lt; 70 &lt; 2250                                          :                                          \n                                                                     IQR (CV) : 45 (1.4)                                     :                                          \n                                                                                                                             :                                          \n\n18   amount               Dollars given                              Mean (sd) : 0.9 (8.7)           43 distinct values      :                      50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 400                                             :                                          \n                                                                     IQR (CV) : 0 (9.5)                                      :                                          \n                                                                                                                             :                                          \n\n19   gave                 Gave anything                              Min  : 0                        0 : 49049 (97.9%)       IIIIIIIIIIIIIIIIIII    50083      0        \n     [numeric]                                                       Mean : 0                        1 :  1034 ( 2.1%)                              (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n20   amountchange         Change in amount given                     Mean (sd) : -52.7 (1267.2)      240 distinct values                       :    50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                                          :    (100.0%)   (0.0%)   \n                                                                     -200412.1 &lt; -30 &lt; 275                                                     :                        \n                                                                     IQR (CV) : 25 (-24.1)                                                     :                        \n                                                                                                                                               :                        \n\n21   hpa                  Highest previous contribution              Mean (sd) : 59.4 (71.2)         243 distinct values     :                      50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 45 &lt; 1000                                           :                                          \n                                                                     IQR (CV) : 30 (1.2)                                     :                                          \n                                                                                                                             : .                                        \n\n22   ltmedmra             Small prior donor: last gift was less      Min  : 0                        0 : 25356 (50.6%)       IIIIIIIIII             50083      0        \n     [numeric]            than median $35                            Mean : 0.5                      1 : 24727 (49.4%)       IIIIIIIII              (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n23   freq                 Number of prior donations                  Mean (sd) : 8 (11.4)            144 distinct values     :                      50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 4 &lt; 218                                             :                                          \n                                                                     IQR (CV) : 8 (1.4)                                      :                                          \n                                                                                                                             : .                                        \n\n24   years                Number of years since initial donation     Mean (sd) : 6.1 (5.5)           22 distinct values      :                      50082      1        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 5 &lt; 95                                              :                                          \n                                                                     IQR (CV) : 7 (0.9)                                      : .                                        \n                                                                                                                             : :                                        \n\n25   year5                At least 5 years since initial donation    Min  : 0                        0 : 24600 (49.1%)       IIIIIIIII              50083      0        \n     [numeric]                                                       Mean : 0.5                      1 : 25483 (50.9%)       IIIIIIIIII             (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n26   mrm2                 Number of months since last donation       Mean (sd) : 13 (12.1)           67 distinct values      :                      50082      1        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 8 &lt; 168                                             :                                          \n                                                                     IQR (CV) : 15 (0.9)                                     : .                                        \n                                                                                                                             : : .                                      \n\n27   dormant              Already donated in 2005                    Min  : 0                        0 : 23866 (47.7%)       IIIIIIIII              50083      0        \n     [numeric]                                                       Mean : 0.5                      1 : 26217 (52.3%)       IIIIIIIIII             (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n28   female               Female                                     Min  : 0                        0 : 35374 (72.2%)       IIIIIIIIIIIIII         48972      1111     \n     [numeric]                                                       Mean : 0.3                      1 : 13598 (27.8%)       IIIII                  (97.8%)    (2.2%)   \n                                                                     Max  : 1                                                                                           \n\n29   couple               Couple                                     Min  : 0                        0 : 44438 (90.8%)       IIIIIIIIIIIIIIIIII     48935      1148     \n     [numeric]                                                       Mean : 0.1                      1 :  4497 ( 9.2%)       I                      (97.7%)    (2.3%)   \n                                                                     Max  : 1                                                                                           \n\n30   state50one           State tag: 1 for one observation of each   Min  : 0                        0 : 50033 (99.9%)       IIIIIIIIIIIIIIIIIII    50083      0        \n     [numeric]            of 50 states; 0 otherwise                  Mean : 0                        1 :    50 ( 0.1%)                              (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n31   nonlit               Nonlitigation                              Mean (sd) : 2.5 (2)             0 : 10873 (21.9%)       IIII                   49631      452      \n     [numeric]                                                       min &lt; med &lt; max:                1 : 11944 (24.1%)       IIII                   (99.1%)    (0.9%)   \n                                                                     0 &lt; 3 &lt; 6                       2 :  1163 ( 2.3%)                                                  \n                                                                     IQR (CV) : 3 (0.8)              3 :  5148 (10.4%)       II                                         \n                                                                                                     4 : 11557 (23.3%)       IIII                                       \n                                                                                                     5 :  6835 (13.8%)       II                                         \n                                                                                                     6 :  2111 ( 4.3%)                                                  \n\n32   cases                Court cases from state in 2004-5 in        Mean (sd) : 1.5 (1.2)           0 : 10682 (21.5%)       IIII                   49631      452      \n     [numeric]            which organization was involved            min &lt; med &lt; max:                1 : 17435 (35.1%)       IIIIIII                (99.1%)    (0.9%)   \n                                                                     0 &lt; 1 &lt; 4                       2 :  9796 (19.7%)       III                                        \n                                                                     IQR (CV) : 1 (0.8)              3 :  9464 (19.1%)       III                                        \n                                                                                                     4 :  2254 ( 4.5%)                                                  \n\n33   statecnt             Percent of sample from state               Mean (sd) : 6 (5.7)             57 distinct values        :                    50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                          : .                  (100.0%)   (0.0%)   \n                                                                     0 &lt; 3.5 &lt; 17.4                                          : : :             :                        \n                                                                     IQR (CV) : 7.8 (1)                                      : : :     .       :                        \n                                                                                                                             : : : :   :       :                        \n\n34   stateresponse        Proportion of sample from the state who    Mean (sd) : 0 (0)               48 distinct values          :                  50083      0        \n     [numeric]            gave                                       min &lt; med &lt; max:                                            :                  (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                                 :                                      \n                                                                     IQR (CV) : 0 (0.3)                                          :                                      \n                                                                                                                               . : :                                    \n\n35   stateresponset       Proportion of treated sample from the      Mean (sd) : 0 (0)               47 distinct values        :                    50083      0        \n     [numeric]            state who gave                             min &lt; med &lt; max:                                          :                    (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                               : .                                      \n                                                                     IQR (CV) : 0 (0.3)                                        : :                                      \n                                                                                                                               : :                                      \n\n36   stateresponsec       Proportion of control sample from the      Mean (sd) : 0 (0)               41 distinct values          . :                50080      3        \n     [numeric]            state who gave                             min &lt; med &lt; max:                                            : :                (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                                 : : .                                  \n                                                                     IQR (CV) : 0 (0.4)                                          : : :                                  \n                                                                                                                             :   : : : .                                \n\n37   stateresponsetminc   stateresponset - stateresponsec            Mean (sd) : 0 (0)               49 distinct values            :                50080      3        \n     [numeric]                                                       min &lt; med &lt; max:                                            . :                (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                                 : :                                    \n                                                                     IQR (CV) : 0 (2.1)                                          : :                                    \n                                                                                                                                 : : .                                  \n\n38   perbush              State vote share for Bush                  Mean (sd) : 0.5 (0.1)           41 distinct values                :            50048      35       \n     [numeric]                                                       min &lt; med &lt; max:                                                  : :          (99.9%)    (0.1%)   \n                                                                     0.1 &lt; 0.5 &lt; 0.7                                                   : :                              \n                                                                     IQR (CV) : 0.1 (0.2)                                            : : : :                            \n                                                                                                                                     : : : : :                          \n\n39   close25              State vote share for Bush between 47.5%    Min  : 0                        0 : 40754 (81.4%)       IIIIIIIIIIIIIIII       50048      35       \n     [numeric]            and 52.5%                                  Mean : 0.2                      1 :  9294 (18.6%)       III                    (99.9%)    (0.1%)   \n                                                                     Max  : 1                                                                                           \n\n40   red0                 Red state                                  Min  : 0                        0 : 29806 (59.6%)       IIIIIIIIIII            50048      35       \n     [numeric]                                                       Mean : 0.4                      1 : 20242 (40.4%)       IIIIIIII               (99.9%)    (0.1%)   \n                                                                     Max  : 1                                                                                           \n\n41   blue0                Blue state                                 Min  : 0                        0 : 20242 (40.4%)       IIIIIIII               50048      35       \n     [numeric]                                                       Mean : 0.6                      1 : 29806 (59.6%)       IIIIIIIIIII            (99.9%)    (0.1%)   \n                                                                     Max  : 1                                                                                           \n\n42   redcty               Red county                                 Min  : 0                        0 : 24477 (49.0%)       IIIIIIIII              49978      105      \n     [numeric]                                                       Mean : 0.5                      1 : 25501 (51.0%)       IIIIIIIIII             (99.8%)    (0.2%)   \n                                                                     Max  : 1                                                                                           \n\n43   bluecty              Blue county                                Min  : 0                        0 : 25553 (51.1%)       IIIIIIIIII             49978      105      \n     [numeric]                                                       Mean : 0.5                      1 : 24425 (48.9%)       IIIIIIIII              (99.8%)    (0.2%)   \n                                                                     Max  : 1                                                                                           \n\n44   pwhite               Proportion white within zip code           Mean (sd) : 0.8 (0.2)           10729 distinct values                     :    48217      1866     \n     [numeric]                                                       min &lt; med &lt; max:                                                        . :    (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.9 &lt; 1                                                             : :                        \n                                                                     IQR (CV) : 0.2 (0.2)                                                  . : :                        \n                                                                                                                                     . . : : : :                        \n\n45   pblack               Proportion black within zip code           Mean (sd) : 0.1 (0.1)           10540 distinct values   :                      48047      2036     \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (95.9%)    (4.1%)   \n                                                                     0 &lt; 0 &lt; 1                                               :                                          \n                                                                     IQR (CV) : 0.1 (1.6)                                    :                                          \n                                                                                                                             : : .                                      \n\n46   page18_39            Proportion age 18-39 within zip code       Mean (sd) : 0.3 (0.1)           10788 distinct values       : .                48217      1866     \n     [numeric]                                                       min &lt; med &lt; max:                                            : :                (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.3 &lt; 1                                                 : :                                    \n                                                                     IQR (CV) : 0.1 (0.3)                                        : : .                                  \n                                                                                                                               : : : : .                                \n\n47   ave_hh_sz            Average household size within zip code     Mean (sd) : 2.4 (0.4)           296 distinct values             :              48221      1862     \n     [numeric]                                                       min &lt; med &lt; max:                                                :              (96.3%)    (3.7%)   \n                                                                     0 &lt; 2.4 &lt; 5.3                                                   : .                                \n                                                                     IQR (CV) : 0.5 (0.2)                                          . : :                                \n                                                                                                                                   : : :                                \n\n48   median_hhincome      Median household income within zip code    Mean (sd) : 54815.7 (22027.3)   9569 distinct values      : :                  48209      1874     \n     [numeric]                                                       min &lt; med &lt; max:                                          : :                  (96.3%)    (3.7%)   \n                                                                     5000 &lt; 50673 &lt; 200001                                     : : .                                    \n                                                                     IQR (CV) : 26824 (0.4)                                    : : :                                    \n                                                                                                                             . : : : : .                                \n\n49   powner               Proportion house owner within zip code     Mean (sd) : 0.7 (0.2)           10794 distinct values                 : .      48214      1869     \n     [numeric]                                                       min &lt; med &lt; max:                                                    . : :      (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.7 &lt; 1                                                       . : : :                          \n                                                                     IQR (CV) : 0.3 (0.3)                                          . . : : : : .                        \n                                                                                                                               . : : : : : : : :                        \n\n50   psch_atlstba         Proportion who finished college within     Mean (sd) : 0.4 (0.2)           10790 distinct values     . : :                48215      1868     \n     [numeric]            zip code                                   min &lt; med &lt; max:                                          : : : : .            (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.4 &lt; 1                                               : : : : : .                              \n                                                                     IQR (CV) : 0.3 (0.5)                                      : : : : : : :                            \n                                                                                                                             . : : : : : : :                            \n\n51   pop_propurban        Proportion of population urban within      Mean (sd) : 0.9 (0.3)           5607 distinct values                      :    48217      1866     \n     [numeric]            zip code                                   min &lt; med &lt; max:                                                          :    (96.3%)    (3.7%)   \n                                                                     0 &lt; 1 &lt; 1                                                                 :                        \n                                                                     IQR (CV) : 0.1 (0.3)                                                      :                        \n                                                                                                                             .             . . :                        \n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n#todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load dataset\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Split data by treatment\ntreat &lt;- filter(data, treatment == 1)\ncontrol &lt;- filter(data, treatment == 0)\n\n# Variables to test\nvars &lt;- c(\"mrm2\", \"hpa\", \"freq\",\"page18_39\",\"red0\")\n\n# Function to report results\nfor (var in vars) {\n  cat(\"\\n==============================\\n\")\n  cat(\"Variable:\", var, \"\\n\")\n  \n  # Means and SDs\n  treat_mean &lt;- mean(treat[[var]], na.rm = TRUE)\n  control_mean &lt;- mean(control[[var]], na.rm = TRUE)\n  treat_sd &lt;- sd(treat[[var]], na.rm = TRUE)\n  control_sd &lt;- sd(control[[var]], na.rm = TRUE)\n  overall_mean &lt;- mean(data[[var]], na.rm = TRUE)\n  overall_sd &lt;- sd(data[[var]], na.rm = TRUE)\n  \n  cat(\"Treatment Mean:\", round(treat_mean, 3), \"SD:\", round(treat_sd, 3), \"\\n\")\n  cat(\"Control Mean:  \", round(control_mean, 3), \"SD:\", round(control_sd, 3), \"\\n\")\n  \n  # T-test\n  ttest &lt;- t.test(as.formula(paste(var, \"~ treatment\")), data = data)\n  cat(\"T-test p-value:\", round(ttest$p.value, 4), \"\\n\")\n  \n  # Regression\n  model &lt;- lm(as.formula(paste(var, \"~ treatment\")), data = data)\n  coef_summary &lt;- tidy(model) %&gt;% filter(term == \"treatment\")\n  cat(\"Regression Coefficient:\", round(coef_summary$estimate, 4), \"\\n\")\n  cat(\"Regression p-value:    \", round(coef_summary$p.value, 4), \"\\n\")\n}\n\n\n==============================\nVariable: mrm2 \nTreatment Mean: 13.012 SD: 12.086 \nControl Mean:   12.998 SD: 12.074 \nT-test p-value: 0.9049 \nRegression Coefficient: 0.0137 \nRegression p-value:     0.9049 \n\n==============================\nVariable: hpa \nTreatment Mean: 59.597 SD: 73.052 \nControl Mean:   58.96 SD: 67.269 \nT-test p-value: 0.3318 \nRegression Coefficient: 0.6371 \nRegression p-value:     0.3451 \n\n==============================\nVariable: freq \nTreatment Mean: 8.035 SD: 11.39 \nControl Mean:   8.047 SD: 11.404 \nT-test p-value: 0.9117 \nRegression Coefficient: -0.012 \nRegression p-value:     0.9117 \n\n==============================\nVariable: page18_39 \nTreatment Mean: 0.322 SD: 0.103 \nControl Mean:   0.322 SD: 0.103 \nT-test p-value: 0.9011 \nRegression Coefficient: -1e-04 \nRegression p-value:     0.901 \n\n==============================\nVariable: red0 \nTreatment Mean: 0.407 SD: 0.491 \nControl Mean:   0.399 SD: 0.49 \nT-test p-value: 0.0605 \nRegression Coefficient: 0.0087 \nRegression p-value:     0.0608 \n\n\nAll tested variables show very similar means and standard deviations across treatment and control groups, as expected under successful randomization. The t-test and regression p-values are nearly identical and all above 0.05, indicating no statistically significant imbalance. These results replicate Table 1 in the Karlan & List (2007) paper, which is included to show that randomization produced comparable groups on observable characteristics."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#experimental-results",
    "href": "blog/Project 3/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Calculate proportion who donated by treatment group\nresponse_rate &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(proportion_donated = mean(gave, na.rm = TRUE)) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\n# Plot\nggplot(response_rate, aes(x = group, y = proportion_donated, fill = group)) +\n  geom_col(width = 0.5) +\n  labs(\n    title = \"Proportion of People Who Donated\",\n    x = \"\",\n    y = \"Proportion\",\n    fill = \"\"\n  ) +\n  ylim(0, max(response_rate$proportion_donated) * 1.1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# T-test\nttest_gave &lt;- t.test(gave ~ treatment, data = data)\ncat(\"T-test result:\\n\")\n\nT-test result:\n\nprint(ttest_gave)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n# Regression\nmodel_gave &lt;- lm(gave ~ treatment, data = data)\nreg_summary &lt;- tidy(model_gave)\ncat(\"\\nLinear regression result:\\n\")\n\n\nLinear regression result:\n\nprint(reg_summary)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.0179    0.00110     16.2  4.78e-59\n2 treatment    0.00418   0.00135      3.10 1.93e- 3\n\n\nThe t-test comparing the proportion of individuals who made any charitable donation between the treatment and control groups shows a small but statistically significant increase in response rate in the treatment group. This confirms the finding in Table 2A, Panel A, where the response rate increases from 1.8% to 2.2% when matching is introduced.\nThe bivariate linear regression yields an equivalent result: the coefficient on treatment is positive and significant, indicating that simply offering a match increases the likelihood of donation.\nInterpretation: Even a small increase in donation probability — from just under 2% to just over 2% — is meaningful in a large fundraising campaign. This suggests that people are psychologically responsive to matching offers, even when the absolute value of the match is not very large. The perception of increased impact (“my $10 becomes $20”) may be enough to nudge more people into giving. This supports the idea that framing and context matter in charitable behavior, not just individual preferences.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(margins)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Probit regression: gave ~ treatment\nprobit_model &lt;- glm(gave ~ treatment, data = data, family = binomial(link = \"probit\"))\n\n# Summary of the model\nsummary(probit_model)\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n# Tidy output (for coefficient and SE)\ntidy(probit_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -2.10      0.0233    -90.1  0      \n2 treatment     0.0868    0.0279      3.11 0.00185\n\n# Optional: Marginal effects (not required for Table 3, but helpful)\nmargins::margins(probit_model)\n\nAverage marginal effects\n\n\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"),     data = data)\n\n\n treatment\n  0.004313\n\nmodel_full &lt;- glm(\n  gave ~ treatment * ratio2 +\n         treatment * ratio3 +\n         treatment * size25 +\n         treatment * size50 +\n         treatment * size100 +\n         treatment * askd2 +\n         treatment * askd3,\n  data = data,\n  family = binomial(link = \"probit\")\n)\n\nlibrary(broom)\ntidy(model_full)\n\n# A tibble: 16 × 5\n   term              estimate std.error statistic p.value\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)       -2.10       0.0233  -90.1      0    \n 2 treatment          0.0475     0.0496    0.958    0.338\n 3 ratio2             0.0364     0.0377    0.967    0.334\n 4 ratio3             0.0384     0.0376    1.02     0.308\n 5 size25            -0.0117     0.0434   -0.270    0.787\n 6 size50             0.00460    0.0431    0.107    0.915\n 7 size100           -0.00221    0.0432   -0.0512   0.959\n 8 askd2              0.0192     0.0377    0.509    0.611\n 9 askd3              0.0293     0.0375    0.782    0.434\n10 treatment:ratio2  NA         NA        NA       NA    \n11 treatment:ratio3  NA         NA        NA       NA    \n12 treatment:size25  NA         NA        NA       NA    \n13 treatment:size50  NA         NA        NA       NA    \n14 treatment:size100 NA         NA        NA       NA    \n15 treatment:askd2   NA         NA        NA       NA    \n16 treatment:askd3   NA         NA        NA       NA    \n\n\n\n\nInterpretation of Probit Regression Result\nThe probit regression produced a positive and statistically significant coefficient on the treatment variable, with an estimated effect of 0.0868, a z-value of 3.11, and a p-value below 0.01. This confirms that assignment to the treatment group — those who received a matching donation offer — was associated with a higher probability of making a donation.\nThe result is statistically significant at the 1% level, providing strong evidence that the offer of a match influenced donor behavior. While the numerical value of the coefficient may seem modest, its significance within a nonlinear probit framework reinforces the idea that behavioral prompts like matching gifts can effectively shape outcomes.\nIn essence, this supports the broader finding from Karlan and List (2007): even relatively small psychological nudges can meaningfully impact decision-making in a real-world charitable context.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nlibrary(haven)\nlibrary(dplyr)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Filter only the people who were in a match group (i.e., treatment == 1)\nmatched &lt;- filter(data, treatment == 1)\n\n# Create a variable for 1:1 match (implied if ratio2 and ratio3 are both 0)\nmatched &lt;- matched %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# T-test: 2:1 match vs 1:1 match\nt_21_vs_11 &lt;- t.test(gave ~ ratio2, data = filter(matched, ratio1 == 1 | ratio2 == 1))\n\n# T-test: 3:1 match vs 1:1 match\nt_31_vs_11 &lt;- t.test(gave ~ ratio3, data = filter(matched, ratio1 == 1 | ratio3 == 1))\n\n# Output results\ncat(\"T-test: 2:1 vs 1:1 match rate\\n\")\n\nT-test: 2:1 vs 1:1 match rate\n\nprint(t_21_vs_11)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio2\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 0 mean in group 1 \n     0.02074912      0.02263338 \n\ncat(\"\\nT-test: 3:1 vs 1:1 match rate\\n\")\n\n\nT-test: 3:1 vs 1:1 match rate\n\nprint(t_31_vs_11)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio3\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 0 mean in group 1 \n     0.02074912      0.02273340 \n\n\n\n\nEffectiveness of Different Match Ratios: T-Test Results\nTo evaluate whether more generous match ratios (2:1 and 3:1) increase the likelihood of donating compared to the standard 1:1 match, I conducted two t-tests:\n\n2:1 vs 1:1 match rate\n3:1 vs 1:1 match rate\n\nIn both cases, the t-tests found no statistically significant difference in the probability of donation. The p-value for the 2:1 vs 1:1 comparison was 0.3345, and for the 3:1 vs 1:1 comparison it was 0.3101 — both well above the conventional significance threshold of 0.05.\nThe estimated differences in means were very small, with confidence intervals that include zero, indicating no reliable effect of increasing the match ratio.\nThese results support the authors’ comment on page 8 of the paper that “larger match ratios had no additional impact.” In other words, donors respond to the presence of a match offer, but increasing the match from 1:1 to 2:1 or 3:1 does not lead to a higher response rate.\nThis finding reinforces a key insight from behavioral economics: perceived opportunity or framing (i.e., the existence of a match), rather than the size of the match, drives behavior in charitable giving contexts.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Subset only treatment group (those who received any match)\nmatched &lt;- filter(data, treatment == 1)\n\n# Create ratio1 manually: it's the group not in ratio2 or ratio3\nmatched &lt;- matched %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# Run regression with dummy variables (ratio1 is implicit baseline)\nmodel_ratios &lt;- lm(gave ~ ratio2 + ratio3, data = matched)\n\n# Output tidy summary\ntidy(model_ratios)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.0207    0.00139    14.9   3.98e-50\n2 ratio2       0.00188   0.00197     0.958 3.38e- 1\n3 ratio3       0.00198   0.00197     1.01  3.13e- 1\n\n\n\n\nRegression Analysis: Impact of Match Ratio on Donation Behavior\nTo assess whether the generosity of the match offer affects the likelihood of donating, I ran a linear regression using data from individuals who were in the treatment group. The model includes dummy variables for the 2:1 and 3:1 match offers, with the 1:1 match offer serving as the reference category.\nThe regression results show that: - The coefficient on ratio2 is 0.00188 with a standard error of 0.00197, and a p-value of 0.338. - The coefficient on ratio3 is 0.00198 with a standard error of 0.00197, and a p-value of 0.313.\nThese coefficients represent the difference in donation probability relative to the 1:1 match group. However, both coefficients are small and statistically insignificant, indicating that neither the 2:1 nor 3:1 match rate led to a meaningful increase in the likelihood of donating compared to a 1:1 match.\nThese findings are entirely consistent with earlier t-test results and support the authors’ conclusion that increasing the match ratio has no additional effect. The behavioral impact appears to stem from the presence of a match offer itself, rather than its magnitude. Donors may not be especially sensitive to whether the match is 1:1 or 3:1 — the idea of leverage seems to matter more than the amount.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Keep only treatment group\nmatched &lt;- filter(data, treatment == 1)\n\n# Create 1:1 dummy\nmatched &lt;- matched %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# ----- APPROACH 1: Direct from the data -----\n\n# Compute response rates by match group\nresponse_rates &lt;- matched %&gt;%\n  mutate(ratio_type = case_when(\n    ratio1 == 1 ~ \"1:1\",\n    ratio2 == 1 ~ \"2:1\",\n    ratio3 == 1 ~ \"3:1\"\n  )) %&gt;%\n  group_by(ratio_type) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE))\n\n# Extract values for comparison\nrate_1_1 &lt;- response_rates$response_rate[response_rates$ratio_type == \"1:1\"]\nrate_2_1 &lt;- response_rates$response_rate[response_rates$ratio_type == \"2:1\"]\nrate_3_1 &lt;- response_rates$response_rate[response_rates$ratio_type == \"3:1\"]\n\n# Calculate differences\ndiff_21_11 &lt;- rate_2_1 - rate_1_1\ndiff_31_21 &lt;- rate_3_1 - rate_2_1\n\n# ----- APPROACH 2: From regression coefficients -----\n\n# Run regression using 1:1 (ratio1) as the baseline\nmodel &lt;- lm(gave ~ ratio2 + ratio3, data = matched)\nmodel_out &lt;- tidy(model)\n\n# Coefficient difference (they are relative to 1:1 group)\ncoef_21_11 &lt;- model_out$estimate[model_out$term == \"ratio2\"]\ncoef_31_21 &lt;- model_out$estimate[model_out$term == \"ratio3\"] - coef_21_11\n\n# Output results\ncat(\"== Response Rate Differences from Raw Data ==\\n\")\n\n== Response Rate Differences from Raw Data ==\n\ncat(\"2:1 - 1:1 =\", round(diff_21_11, 5), \"\\n\")\n\n2:1 - 1:1 = 0.00188 \n\ncat(\"3:1 - 2:1 =\", round(diff_31_21, 5), \"\\n\")\n\n3:1 - 2:1 = 1e-04 \n\ncat(\"\\n== Differences in Fitted Coefficients ==\\n\")\n\n\n== Differences in Fitted Coefficients ==\n\ncat(\"2:1 vs 1:1 =\", round(coef_21_11, 5), \"\\n\")\n\n2:1 vs 1:1 = 0.00188 \n\ncat(\"3:1 vs 2:1 =\", round(coef_31_21, 5), \"\\n\")\n\n3:1 vs 2:1 = 1e-04 \n\n\n\n\nMatch Ratio Differences: Response Rate and Regression Analysis\nTo understand whether higher match ratios increase the likelihood of donation, I calculated response rate differences in two ways:\n\n1. Differences in Raw Response Rates\n\n2:1 vs 1:1 match ratio: 0.00188\n3:1 vs 2:1 match ratio: 0.00010\n\n\n\n2. Differences in Fitted Coefficients (Linear Model)\n\n2:1 vs 1:1: 0.00188\n3:1 vs 2:1: 0.00010\n\nBoth approaches yielded the same differences, confirming that the estimated impact of increasing the match ratio is extremely small — around 0.1 to 0.2 percentage points.\n\n\n\nConclusion\nThese findings provide strong evidence that increasing the match ratio from 1:1 to 2:1 or from 2:1 to 3:1 does not significantly increase the probability that someone donates.\nThis aligns with the authors’ conclusion: the presence of a match offer, rather than its generosity, is what motivates donations. Donors respond to the idea of a match, but larger matches do not provide additional behavioral lift.\nIn sum, the marginal effectiveness of more generous matches is negligible — a key insight for fundraisers seeking efficient campaign strategies.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# T-test: Compare average donation amount\nt_test_amount &lt;- t.test(amount ~ treatment, data = data)\ncat(\"== T-test: Donation Amount by Treatment Status ==\\n\")\n\n== T-test: Donation Amount by Treatment Status ==\n\nprint(t_test_amount)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = -1.9183, df = 36216, p-value = 0.05509\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.310555423  0.003344493\nsample estimates:\nmean in group 0 mean in group 1 \n      0.8132678       0.9668733 \n\n# Regression: Amount ~ Treatment\nmodel_amount &lt;- lm(amount ~ treatment, data = data)\ncat(\"\\n== Linear Regression: Amount ~ Treatment ==\\n\")\n\n\n== Linear Regression: Amount ~ Treatment ==\n\ntidy(model_amount)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.813    0.0674     12.1  1.84e-33\n2 treatment      0.154    0.0826      1.86 6.28e- 2\n\n\n\n\nEffect of Match Offers on Donation Amounts\nTo assess whether being offered a matching donation opportunity affects the amount individuals donate, I conducted both a t-test and a bivariate linear regression using the full dataset.\nThe t-test comparing average donation amounts between treatment and control groups yielded a p-value of 0.055, which is just above the traditional 5% significance threshold. The average donation was $0.81 in the control group and $0.97 in the treatment group — a difference of approximately $0.15.\nThe linear regression of donation amount on treatment status produced a coefficient of 0.154 with a p-value of 0.0628, again marginally above the 5% threshold. This suggests a small, positive effect of treatment on donation amount, but not strong enough to be deemed statistically significant at conventional levels.\n\n\nInterpretation\nThese results indicate that being offered a match may slightly increase the average amount donated, but the evidence is not strong enough to make a confident causal claim. The p-values suggest that the observed difference might be due to chance, though the direction of the effect (positive) is consistent with the broader narrative of the study.\nIn summary, while matching offers clearly increase the likelihood that someone donates, the evidence that they increase the size of the donation (unconditionally) is suggestive but not definitive.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load the data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Filter to donors only (those who gave something)\ndonors_only &lt;- filter(data, gave == 1)\n\n# T-test: Compare average amount given among donors\nt_test_donors &lt;- t.test(amount ~ treatment, data = donors_only)\ncat(\"== T-test: Donation Amount by Treatment Status (Donors Only) ==\\n\")\n\n== T-test: Donation Amount by Treatment Status (Donors Only) ==\n\nprint(t_test_donors)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = 0.58461, df = 557.46, p-value = 0.559\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -3.937240  7.274027\nsample estimates:\nmean in group 0 mean in group 1 \n       45.54027        43.87188 \n\n# Regression: Amount ~ Treatment (among donors)\nmodel_donors &lt;- lm(amount ~ treatment, data = donors_only)\ncat(\"\\n== Linear Regression: Amount ~ Treatment (Donors Only) ==\\n\")\n\n\n== Linear Regression: Amount ~ Treatment (Donors Only) ==\n\ntidy(model_donors)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    45.5       2.42    18.8   5.47e-68\n2 treatment      -1.67      2.87    -0.581 5.61e- 1\n\n\n\n\nConditional Effect of Treatment on Donation Amount (Among Donors Only)\nTo examine whether match offers influenced how much people donated — conditional on having donated — I restricted the dataset to only those individuals who gave a positive amount.\n\nT-Test Results\nThe average donation among the control group was $45.54, while the average in the treatment group was $43.87. The t-test revealed no statistically significant difference between the two groups (p = 0.559). The 95% confidence interval for the difference in means includes zero, reinforcing this conclusion.\n\n\nRegression Results\nThe linear regression of donation amount on treatment (among donors only) produced a coefficient of -1.67, indicating that those in the treatment group donated slightly less on average. However, the effect is not statistically significant (p = 0.561).\n\n\nInterpretation and Causal Insights\nThese results suggest that being offered a match does not influence the size of the donation, once someone has decided to donate. In fact, if anything, treated individuals gave slightly less, though the difference is not meaningful.\nImportantly, this analysis is conditional on donation behavior, which was itself affected by the treatment. Because we are conditioning on a post-treatment variable (gave), the regression coefficient does not have a causal interpretation. The group of donors in the treatment group may differ in unobservable ways from donors in the control group, and this could bias the estimate.\nIn summary, this analysis tells us that the treatment affects the extensive margin (whether people give), not the intensive margin (how much they give) — a conclusion that aligns with the broader findings of Karlan and List (2007).\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Filter to donors only\ndonors &lt;- filter(data, gave == 1)\n\n# Calculate group means\ngroup_means &lt;- donors %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(mean_amount = mean(amount, na.rm = TRUE))\n\n# Plot for Treatment Group\nggplot(filter(donors, treatment == 1), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = group_means$mean_amount[group_means$treatment == 1],\n             color = \"red\", size = 1) +\n  labs(\n    title = \"Donation Amounts (Treatment Group)\",\n    x = \"Amount Donated\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n# Plot for Control Group\nggplot(filter(donors, treatment == 0), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"lightgreen\", color = \"white\") +\n  geom_vline(xintercept = group_means$mean_amount[group_means$treatment == 0],\n             color = \"red\", size = 1) +\n  labs(\n    title = \"Donation Amounts (Control Group)\",\n    x = \"Amount Donated\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDonation Amount Distributions: Treatment vs. Control (Donors Only)\nThe two histograms above show the distribution of donation amounts among individuals who made a donation, separated by treatment assignment. The red vertical line in each plot marks the average donation amount within that group.\n\nIn the treatment group, the distribution of donations is right-skewed, with most donations falling between $10 and $100. The average donation is just below $44.\nIn the control group, the pattern is similar, though the average donation is slightly higher, around $45.5.\n\nVisually, the distributions are quite similar. Both groups show large clustering at common donation levels (e.g., $25, $50, $100), suggesting donors may be responding to standard ask amounts or psychological anchors.\n\n\nInterpretation\nThese plots support the earlier statistical analysis: being offered a match does not appear to increase the average amount donated among those who give. In fact, the average donation is marginally lower in the treatment group. However, this difference is not statistically significant, and the shapes of the two distributions are nearly identical.\nThis visualization reinforces the paper’s broader conclusion that matching gifts increase the likelihood of giving, but not the size of the donation once that decision has been made. The behavioral nudge seems to influence the extensive margin (give vs. not give), not the intensive margin (how much to give)."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#simulation-experiment",
    "href": "blog/Project 3/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate 100,000 control group donations (mean = 0.813, sd = 10)\ncontrol &lt;- rnorm(100000, mean = 0.813, sd = 10)\n\n# Simulate 10,000 treatment group donations (mean = 0.966, sd = 10)\ntreatment &lt;- rnorm(10000, mean = 0.966, sd = 10)\n\n# Calculate 10,000 differences (each treatment value - sampled control value)\ndiffs &lt;- treatment - sample(control, size = 10000)\n\n# Compute cumulative average\ncum_avg &lt;- cumsum(diffs) / seq_along(diffs)\n\n# Create data for plotting\nplot_data &lt;- data.frame(\n  x = 1:10000,\n  cum_avg = cum_avg\n)\n\n# Plot the convergence\nlibrary(ggplot2)\nggplot(plot_data, aes(x = x, y = cum_avg)) +\n  geom_line(linewidth = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = mean(treatment) - mean(control), color = \"red\", linetype = \"dashed\") +\n  coord_cartesian(ylim = c(-1, 1)) +  # focus the Y-axis for cleaner view\n  labs(\n    title = \"Cumulative Average of Treatment-Control Differences\",\n    subtitle = \"Simulating the Law of Large Numbers\",\n    x = \"Sample Size\",\n    y = \"Cumulative Average Difference\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSimulating the Law of Large Numbers\nThe plot above shows the cumulative average of 10,000 differences between randomly drawn treatment and control values. Specifically, we simulate 100,000 values from the control distribution and 10,000 from the treatment distribution, then compute the difference in means one pair at a time. The blue line shows the evolving average of those differences as the sample size increases.\nAt the beginning of the plot, we see high variability — the cumulative average fluctuates wildly with the first few draws. This reflects the influence of random noise when the sample size is small. However, as the number of observations increases, the cumulative average begins to stabilize, eventually converging toward the red dashed line, which represents the true difference in means between the treatment and control groups.\nThis behavior is a textbook demonstration of the Law of Large Numbers: as sample size increases, the sample average becomes a more accurate estimate of the population average. In practical terms, it tells us that while individual comparisons may be noisy, large samples can reveal reliable patterns. Here, the cumulative average confirms the treatment group tends to donate more than the control group — and that this effect emerges clearly as sample size grows.\n\n\nCentral Limit Theorem\n\nset.seed(123)\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Simulate sampling distribution of differences\nsimulate_diff_means &lt;- function(n, reps = 1000, mu_c = 0.813, mu_t = 0.966, sd = 10) {\n  diffs &lt;- replicate(reps, {\n    mean(rnorm(n, mean = mu_t, sd = sd)) - mean(rnorm(n, mean = mu_c, sd = sd))\n  })\n  data.frame(diff = diffs, sample_size = paste0(\"n = \", n))\n}\n\n# Generate datasets\nsizes &lt;- c(50, 200, 500, 1000)\nsim_data &lt;- do.call(rbind, lapply(sizes, simulate_diff_means))\n\n# Plot histograms\nggplot(sim_data, aes(x = diff)) +\n  geom_histogram(bins = 40, fill = \"steelblue\", color = \"white\") +\n  facet_wrap(~sample_size, scales = \"free\", ncol = 2) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Sampling Distribution of Differences in Means\",\n    subtitle = \"Simulated at Varying Sample Sizes\",\n    x = \"Difference in Means (Treatment - Control)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSampling Distributions at Different Sample Sizes\nThe four histograms above show the sampling distribution of the difference in means between treatment and control groups at sample sizes of 50, 200, 500, and 1000. Each plot represents 1,000 simulations where, for each simulation, we draw n observations from each group, compute their sample means, and record the difference.\n\nAt n = 50, the distribution is wide and noisy, with a substantial spread. In this case, zero is near the center of the distribution, meaning we would often observe a difference close to zero just by chance.\nAt n = 200, the distribution is more concentrated, though still displays some variability. Zero is still within the range of plausible outcomes, but less frequently near the center.\nAt n = 500, the sampling distribution narrows significantly, and zero starts to move toward the edge of the distribution — suggesting stronger evidence of a real difference.\nAt n = 1000, the distribution is tight and centered around the true mean difference. Zero is now well into the tails, meaning it’s unlikely we would observe a difference this close to zero if the treatment and control distributions were truly the same.\n\n\n\nWhat We Learn\nThese plots visually demonstrate the power of larger sample sizes. As the sample size increases: - The sampling variability decreases - The sampling distribution becomes more precise - We are better able to detect small true differences in population means\nThis is a direct illustration of the Central Limit Theorem and the Law of Large Numbers in action. At larger sample sizes, we can distinguish signal from noise more effectively — and zero becomes an unlikely outcome when a real treatment effect exists."
  },
  {
    "objectID": "blog/Project 1/index.html",
    "href": "blog/Project 1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n\nlibrary(tidyverse)\n\nmtcars |&gt;\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()"
  },
  {
    "objectID": "blog/Project 1/index.html#section-1-data",
    "href": "blog/Project 1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/Project 1/index.html#section-2-analysis",
    "href": "blog/Project 1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n\nlibrary(tidyverse)\n\nmtcars |&gt;\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting GLM Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nRishikumar Mathiazhagan\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nRishikumar Mathiazhagan\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nRishikumar Mathiazhagan\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Scratch to Insight: Manual ML Algorithms in Action\n\n\n\n\n\n\nRishikumar Mathiazhagan\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Project 4/inclass.html",
    "href": "blog/Project 4/inclass.html",
    "title": "Fitting GLM Regression",
    "section": "",
    "text": "library(readr)\n\n# Read the CSV file\ndata &lt;- read_csv(\"purchase.csv\")\n\nRows: 2000 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): idx, purchase\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View the first few rows\nhead(data)\n\n# A tibble: 6 × 2\n    idx purchase\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  50          0\n2  75          0\n3  28          0\n4  76          0\n5  50          0\n6   8.9        0\n\n\n\n# Load data\ndata &lt;- read.csv(\"purchase.csv\")\n\n# Fit a GLM: replace 'response' and 'predictor' with actual column names\nmodel &lt;- glm(purchase ~ idx, data = data, family = binomial(link = \"logit\"))\n\n# Summary of model\nsummary(model)\n\n\nCall:\nglm(formula = purchase ~ idx, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.219622   0.159587  -20.18   &lt;2e-16 ***\nidx          0.032527   0.002919   11.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1669.9  on 1999  degrees of freedom\nResidual deviance: 1538.6  on 1998  degrees of freedom\nAIC: 1542.6\n\nNumber of Fisher Scoring iterations: 5\n\n# Confidence interval for coefficients\nconfint(model)\n\nWaiting for profiling to be done...\n\n\n                  2.5 %      97.5 %\n(Intercept) -3.53919601 -2.91328111\nidx          0.02685243  0.03830481\n\n\n\n# Load data\ndata &lt;- read.csv(\"purchase.csv\")\nx &lt;- data$idx\ny &lt;- data$purchase  # should be 0/1\n\n# Define the negative log-likelihood for logistic regression\nneg_log_likelihood_logit &lt;- function(par, x, y) {\n  beta0 &lt;- par[1]\n  beta1 &lt;- par[2]\n  eta &lt;- beta0 + beta1 * x\n  p &lt;- 1 / (1 + exp(-eta))  # logistic link\n  -sum(y * log(p) + (1 - y) * log(1 - p))  # negative log-likelihood\n}\n\n# Initial values\nstart_par &lt;- c(0, 0)\n\n# Optimize\nfit &lt;- optim(par = start_par,\n             fn = neg_log_likelihood_logit,\n             x = x, y = y,\n             hessian = TRUE,\n             method = \"BFGS\")\n\n# Extract estimates and SEs\nestimates &lt;- fit$par\nhessian &lt;- fit$hessian\ncov_matrix &lt;- solve(hessian)\nse &lt;- sqrt(diag(cov_matrix))\n\n# CI for slope (parameter 2)\nz &lt;- qnorm(0.975)\nci_slope &lt;- c(\n  estimates[2] - z * se[2],\n  estimates[2] + z * se[2]\n)\n\nlist(Estimate = estimates[2], SE = se[2], CI = ci_slope)\n\n$Estimate\n[1] 0.03250968\n\n$SE\n[1] 0.002920978\n\n$CI\n[1] 0.02678467 0.03823469\n\n\n\n# Load data\ndata &lt;- read.csv(\"purchase.csv\")\nx &lt;- data$idx\ny &lt;- data$purchase\ndf &lt;- data.frame(x = x, y = y)\n\nB &lt;- 1000\nslope_estimates &lt;- numeric(B)\nset.seed(123)\n\nfor (i in 1:B) {\n  sample_indices &lt;- sample(1:nrow(df), replace = TRUE)\n  sample_data &lt;- df[sample_indices, ]\n  fit &lt;- glm(y ~ x, data = sample_data, family = binomial(link = \"logit\"))\n  slope_estimates[i] &lt;- coef(fit)[\"x\"]\n}\n\n# 1. CI using standard deviation approach\nslope_mean &lt;- mean(slope_estimates)\nslope_sd &lt;- sd(slope_estimates)\nz &lt;- qnorm(0.975)\nci_sd &lt;- c(slope_mean - z * slope_sd, slope_mean + z * slope_sd)\n\n# 2. CI using quantile approach\nci_quantile &lt;- quantile(slope_estimates, probs = c(0.025, 0.975))\n\nlist(CI_SD = ci_sd, CI_Quantile = ci_quantile)\n\n$CI_SD\n[1] 0.02689833 0.03843410\n\n$CI_Quantile\n      2.5%      97.5% \n0.02705333 0.03848676"
  },
  {
    "objectID": "blog/Project 5/hw2_questions.html",
    "href": "blog/Project 5/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n#_todo: Read in data._\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read in the Blueprinty dataset\nblueprinty_data &lt;- read_csv(\"blueprinty.csv\")\n\n# Preview the first few rows\nhead(blueprinty_data)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\nlibrary(tidyverse)\n\n# Histogram of patents by customer status\nggplot(blueprinty_data, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Patents by Customer Status\",\n       x = \"Number of Patents (Last 5 Years)\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean patents by customer status\nblueprinty_data %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  iscustomer mean_patents\n       &lt;dbl&gt;        &lt;dbl&gt;\n1          0         3.47\n2          1         4.13\n\n\nThe histogram shows that both Blueprinty customers and non-customers are most frequently awarded between 2 to 6 patents over the last 5 years. However, customers tend to have a rightward shift in the distribution—meaning they are more likely to fall into higher patent count bins. This is consistent with the summary statistics, which show that the average number of patents among non-customers is 3.47, while for customers it is 4.13.\nThis suggests that, on average, firms using Blueprinty’s software are granted more patents. However, this relationship is purely descriptive at this stage and does not account for potential confounders such as firm age or geographic location. Further modeling is needed to isolate the effect of software usage.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nggplot(blueprinty_data, aes(x = age, fill = as.factor(iscustomer))) +\n  geom_histogram(alpha = 0.6, position = \"identity\", binwidth = 2, color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Firm Age by Customer Status\",\n       x = \"Years Since Incorporation\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Proportional bar plot of region by customer status\nggplot(blueprinty_data, aes(x = region, fill = as.factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Regional Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the histogram of firm age, we see that Blueprinty customers tend to be slightly older on average than non-customers. While both groups are centered around 25–30 years since incorporation, the distribution for customers appears more spread out into the higher-age range (35–45 years), whereas non-customers are more concentrated around the mean. This suggests that firm age could be a confounding factor in our analysis and may need to be controlled for when estimating the effect of using Blueprinty software.\nThe regional bar chart shows that Blueprinty’s customer base is not evenly distributed across regions. Notably, a majority of firms in the Northeast region are customers, while other regions like the Midwest, Northwest, South, and Southwest have lower adoption rates. This uneven distribution suggests that regional effects—such as varying patenting activity or industry concentrations—might influence both Blueprinty adoption and patent success, and should be accounted for in any causal analysis.\n\n\n\n\n\n\n\n\n\nEstimation of Simple Poisson Model\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  }\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Apply it to the observed patent counts\nY_vals &lt;- blueprinty_data$patents\n\n# Evaluate the log-likelihood at the sample mean as a starting point\npoisson_loglikelihood(lambda = mean(Y_vals), Y = Y_vals)\n\n[1] -3367.684\n\n\nThe Poisson log-likelihood curve rises steeply for small values of 𝜆 λ, reaches a peak, and then gradually declines. This confirms the log-likelihood function is unimodal, meaning it has a single maximum—ideal for maximum likelihood estimation. The red dashed line indicates the value of 𝜆 λ that maximizes the log-likelihood, representing the maximum likelihood estimate (MLE). This value closely matches the sample mean of the observed patent counts, which is consistent with the theoretical property that the MLE of 𝜆 λ in a simple Poisson model equals the sample mean.\n\n# Define a sequence of lambda values to evaluate\nlambda_vals &lt;- seq(0.5, 8, by = 0.1)\n\n# Calculate log-likelihood for each lambda\nlogliks &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y_vals))\n\n# Create a data frame for plotting\nloglik_df &lt;- data.frame(lambda = lambda_vals, loglik = logliks)\n\n# Plot\nggplot(loglik_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#0072B2\", size = 1) +\n  geom_vline(xintercept = lambda_vals[which.max(logliks)], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Poisson Log-Likelihood Curve\",\n       x = expression(lambda),\n       y = \"Log-Likelihood\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nWe begin with the log-likelihood function for independent observations ( Y_1, , Y_n () ):\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nThis simplifies to:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log(\\lambda) + \\text{const}\n\\]\nNow, take the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolving for ( ) gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator of ( ) is the sample mean of the observed data.\n\n# Define the negative log-likelihood to minimize\nneg_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(Inf)  # invalid values return high cost\n  }\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# Use optim() to minimize the negative log-likelihood\noptim_result &lt;- optim(par = 1,  # initial guess for lambda\n                      fn = neg_loglikelihood,\n                      Y = Y_vals,\n                      method = \"Brent\",\n                      lower = 0.01,\n                      upper = 10)\n\n# Extract MLE\nlambda_hat_optim &lt;- optim_result$par\nlambda_hat_optim\n\n[1] 3.684667\n\n\nTo estimate the Poisson rate parameter 𝜆 λ, we used R’s optim() function to numerically maximize the log-likelihood. Since optim() minimizes by default, we passed it the negative of our log-likelihood function. Using Brent’s method with bounds between 0.01 and 10, the optimizer returned a value of 𝜆 λ that aligns closely with the sample mean of the data. This confirms both our analytical derivation and visual inspection from the log-likelihood plot.\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Define the log-likelihood function for Poisson regression\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Compute lambda_i for all observations\n  lambda &lt;- exp(X %*% beta)\n  \n  # Compute the log-likelihood\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n\n# Create model matrix (design matrix) with intercept, age, age^2, region dummies, and iscustomer\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty_data)\nY &lt;- blueprinty_data$patents\n\n\n# Use optim to estimate beta\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)\n  -sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Run optimization\noptim_result &lt;- optim(par = rep(0, ncol(X)),\n                      fn = poisson_regression_loglikelihood,\n                      Y = Y, X = X,\n                      hessian = TRUE,\n                      method = \"BFGS\",\n                      control = list(reltol = 1e-10))\n\n# Extract coefficients\nbeta_hat &lt;- optim_result$par\n\n# Compute standard errors using the inverse of the Hessian\nhessian &lt;- optim_result$hessian\nvar_cov_matrix &lt;- solve(hessian)\nstd_errors &lt;- sqrt(diag(var_cov_matrix))\n\n\n# Summarize coefficients and standard errors in a table\ncoef_table &lt;- data.frame(\n  Term = colnames(X),\n  Estimate = round(beta_hat, 4),\n  Std_Error = round(std_errors, 4)\n)\n\nknitr::kable(coef_table, caption = \"Poisson Regression Estimates and Standard Errors\")\n\n\nPoisson Regression Estimates and Standard Errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nI(age^2)\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\n# Fit the Poisson regression model using glm()\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, \n                 data = blueprinty_data, \n                 family = poisson())\n\n# Display a summary\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty_data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe results from both the optim() function and the glm() function are generally consistent, which confirms that the manual maximum likelihood estimation was implemented correctly. The coefficient for the iscustomer variable is positive and statistically significant in the glm() output (estimate = 0.208, p &lt; 0.001). This means that, holding other variables constant, firms that use Blueprinty’s software tend to receive more patents than non-customers. Since the model uses a log link function, this coefficient translates to approximately a 23% increase in the expected number of patents (because exp(0.208) is about 1.23).\nThe firm’s age also plays a significant role. The positive coefficient on age and the negative coefficient on age squared suggest a concave relationship, where patent activity increases with age up to a certain point and then declines. This is typical in lifecycle patterns of firm innovation.\nThe region variables are not statistically significant, which implies that, after accounting for age and customer status, there are no strong regional differences in patent outcomes.\nIn summary, the glm() results confirm the manual estimation and support the conclusion that using Blueprinty’s software is associated with a higher number of patents, even after adjusting for firm age and location.\n\n# Make copies of the data\ndata_0 &lt;- blueprinty_data\ndata_1 &lt;- blueprinty_data\n\n# Set customer status to 0 and 1 respectively\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\n# Use the fitted glm model to predict expected patent counts\ny_pred_0 &lt;- predict(glm_model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = data_1, type = \"response\")\n\n# Compute the average effect\naverage_difference &lt;- mean(y_pred_1 - y_pred_0)\naverage_difference\n\n[1] 0.7927681\n\n\nUsing the fitted Poisson regression model, we estimated the expected number of patents for each firm under two hypothetical scenarios: one where no firms use Blueprinty’s software and one where all firms do. The average difference between these two sets of predictions is approximately 0.79 patents per firm over five years. This suggests that, controlling for age and region, firms that use Blueprinty’s software are expected to receive nearly one additional patent on average compared to similar firms that do not use the software. This result provides strong evidence that Blueprinty’s tool may be associated with a meaningful improvement in patenting success."
  },
  {
    "objectID": "blog/Project 5/hw2_questions.html#blueprinty-case-study",
    "href": "blog/Project 5/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n#_todo: Read in data._\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read in the Blueprinty dataset\nblueprinty_data &lt;- read_csv(\"blueprinty.csv\")\n\n# Preview the first few rows\nhead(blueprinty_data)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\nlibrary(tidyverse)\n\n# Histogram of patents by customer status\nggplot(blueprinty_data, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Patents by Customer Status\",\n       x = \"Number of Patents (Last 5 Years)\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean patents by customer status\nblueprinty_data %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  iscustomer mean_patents\n       &lt;dbl&gt;        &lt;dbl&gt;\n1          0         3.47\n2          1         4.13\n\n\nThe histogram shows that both Blueprinty customers and non-customers are most frequently awarded between 2 to 6 patents over the last 5 years. However, customers tend to have a rightward shift in the distribution—meaning they are more likely to fall into higher patent count bins. This is consistent with the summary statistics, which show that the average number of patents among non-customers is 3.47, while for customers it is 4.13.\nThis suggests that, on average, firms using Blueprinty’s software are granted more patents. However, this relationship is purely descriptive at this stage and does not account for potential confounders such as firm age or geographic location. Further modeling is needed to isolate the effect of software usage.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nggplot(blueprinty_data, aes(x = age, fill = as.factor(iscustomer))) +\n  geom_histogram(alpha = 0.6, position = \"identity\", binwidth = 2, color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Firm Age by Customer Status\",\n       x = \"Years Since Incorporation\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Proportional bar plot of region by customer status\nggplot(blueprinty_data, aes(x = region, fill = as.factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Regional Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the histogram of firm age, we see that Blueprinty customers tend to be slightly older on average than non-customers. While both groups are centered around 25–30 years since incorporation, the distribution for customers appears more spread out into the higher-age range (35–45 years), whereas non-customers are more concentrated around the mean. This suggests that firm age could be a confounding factor in our analysis and may need to be controlled for when estimating the effect of using Blueprinty software.\nThe regional bar chart shows that Blueprinty’s customer base is not evenly distributed across regions. Notably, a majority of firms in the Northeast region are customers, while other regions like the Midwest, Northwest, South, and Southwest have lower adoption rates. This uneven distribution suggests that regional effects—such as varying patenting activity or industry concentrations—might influence both Blueprinty adoption and patent success, and should be accounted for in any causal analysis.\n\n\n\n\n\n\n\n\n\nEstimation of Simple Poisson Model\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  }\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Apply it to the observed patent counts\nY_vals &lt;- blueprinty_data$patents\n\n# Evaluate the log-likelihood at the sample mean as a starting point\npoisson_loglikelihood(lambda = mean(Y_vals), Y = Y_vals)\n\n[1] -3367.684\n\n\nThe Poisson log-likelihood curve rises steeply for small values of 𝜆 λ, reaches a peak, and then gradually declines. This confirms the log-likelihood function is unimodal, meaning it has a single maximum—ideal for maximum likelihood estimation. The red dashed line indicates the value of 𝜆 λ that maximizes the log-likelihood, representing the maximum likelihood estimate (MLE). This value closely matches the sample mean of the observed patent counts, which is consistent with the theoretical property that the MLE of 𝜆 λ in a simple Poisson model equals the sample mean.\n\n# Define a sequence of lambda values to evaluate\nlambda_vals &lt;- seq(0.5, 8, by = 0.1)\n\n# Calculate log-likelihood for each lambda\nlogliks &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y_vals))\n\n# Create a data frame for plotting\nloglik_df &lt;- data.frame(lambda = lambda_vals, loglik = logliks)\n\n# Plot\nggplot(loglik_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#0072B2\", size = 1) +\n  geom_vline(xintercept = lambda_vals[which.max(logliks)], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Poisson Log-Likelihood Curve\",\n       x = expression(lambda),\n       y = \"Log-Likelihood\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nWe begin with the log-likelihood function for independent observations ( Y_1, , Y_n () ):\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nThis simplifies to:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log(\\lambda) + \\text{const}\n\\]\nNow, take the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolving for ( ) gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator of ( ) is the sample mean of the observed data.\n\n# Define the negative log-likelihood to minimize\nneg_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(Inf)  # invalid values return high cost\n  }\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# Use optim() to minimize the negative log-likelihood\noptim_result &lt;- optim(par = 1,  # initial guess for lambda\n                      fn = neg_loglikelihood,\n                      Y = Y_vals,\n                      method = \"Brent\",\n                      lower = 0.01,\n                      upper = 10)\n\n# Extract MLE\nlambda_hat_optim &lt;- optim_result$par\nlambda_hat_optim\n\n[1] 3.684667\n\n\nTo estimate the Poisson rate parameter 𝜆 λ, we used R’s optim() function to numerically maximize the log-likelihood. Since optim() minimizes by default, we passed it the negative of our log-likelihood function. Using Brent’s method with bounds between 0.01 and 10, the optimizer returned a value of 𝜆 λ that aligns closely with the sample mean of the data. This confirms both our analytical derivation and visual inspection from the log-likelihood plot.\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Define the log-likelihood function for Poisson regression\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Compute lambda_i for all observations\n  lambda &lt;- exp(X %*% beta)\n  \n  # Compute the log-likelihood\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n\n# Create model matrix (design matrix) with intercept, age, age^2, region dummies, and iscustomer\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty_data)\nY &lt;- blueprinty_data$patents\n\n\n# Use optim to estimate beta\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)\n  -sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Run optimization\noptim_result &lt;- optim(par = rep(0, ncol(X)),\n                      fn = poisson_regression_loglikelihood,\n                      Y = Y, X = X,\n                      hessian = TRUE,\n                      method = \"BFGS\",\n                      control = list(reltol = 1e-10))\n\n# Extract coefficients\nbeta_hat &lt;- optim_result$par\n\n# Compute standard errors using the inverse of the Hessian\nhessian &lt;- optim_result$hessian\nvar_cov_matrix &lt;- solve(hessian)\nstd_errors &lt;- sqrt(diag(var_cov_matrix))\n\n\n# Summarize coefficients and standard errors in a table\ncoef_table &lt;- data.frame(\n  Term = colnames(X),\n  Estimate = round(beta_hat, 4),\n  Std_Error = round(std_errors, 4)\n)\n\nknitr::kable(coef_table, caption = \"Poisson Regression Estimates and Standard Errors\")\n\n\nPoisson Regression Estimates and Standard Errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nI(age^2)\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\n# Fit the Poisson regression model using glm()\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, \n                 data = blueprinty_data, \n                 family = poisson())\n\n# Display a summary\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty_data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe results from both the optim() function and the glm() function are generally consistent, which confirms that the manual maximum likelihood estimation was implemented correctly. The coefficient for the iscustomer variable is positive and statistically significant in the glm() output (estimate = 0.208, p &lt; 0.001). This means that, holding other variables constant, firms that use Blueprinty’s software tend to receive more patents than non-customers. Since the model uses a log link function, this coefficient translates to approximately a 23% increase in the expected number of patents (because exp(0.208) is about 1.23).\nThe firm’s age also plays a significant role. The positive coefficient on age and the negative coefficient on age squared suggest a concave relationship, where patent activity increases with age up to a certain point and then declines. This is typical in lifecycle patterns of firm innovation.\nThe region variables are not statistically significant, which implies that, after accounting for age and customer status, there are no strong regional differences in patent outcomes.\nIn summary, the glm() results confirm the manual estimation and support the conclusion that using Blueprinty’s software is associated with a higher number of patents, even after adjusting for firm age and location.\n\n# Make copies of the data\ndata_0 &lt;- blueprinty_data\ndata_1 &lt;- blueprinty_data\n\n# Set customer status to 0 and 1 respectively\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\n# Use the fitted glm model to predict expected patent counts\ny_pred_0 &lt;- predict(glm_model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = data_1, type = \"response\")\n\n# Compute the average effect\naverage_difference &lt;- mean(y_pred_1 - y_pred_0)\naverage_difference\n\n[1] 0.7927681\n\n\nUsing the fitted Poisson regression model, we estimated the expected number of patents for each firm under two hypothetical scenarios: one where no firms use Blueprinty’s software and one where all firms do. The average difference between these two sets of predictions is approximately 0.79 patents per firm over five years. This suggests that, controlling for age and region, firms that use Blueprinty’s software are expected to receive nearly one additional patent on average compared to similar firms that do not use the software. This result provides strong evidence that Blueprinty’s tool may be associated with a meaningful improvement in patenting success."
  },
  {
    "objectID": "blog/Project 5/hw2_questions.html#airbnb-case-study",
    "href": "blog/Project 5/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\nlibrary(tidyverse)\n\n# Load the data\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# View variable summaries\nsummary(airbnb)\n\n      ...1             id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    1   Length:40628      \n 1st Qu.:10158   1st Qu.: 4889868   1st Qu.:  542   Class :character  \n Median :20315   Median : 9862878   Median :  996   Mode  :character  \n Mean   :20315   Mean   : 9698889   Mean   : 1102                     \n 3rd Qu.:30471   3rd Qu.:14667894   3rd Qu.: 1535                     \n Max.   :40628   Max.   :18009669   Max.   :42828                     \n                                                                      \n  host_since         room_type           bathrooms        bedrooms     \n Length:40628       Length:40628       Min.   :0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Mode  :character   Median :1.000   Median : 1.000  \n                                       Mean   :1.125   Mean   : 1.147  \n                                       3rd Qu.:1.000   3rd Qu.: 1.000  \n                                       Max.   :8.000   Max.   :10.000  \n                                       NA's   :160     NA's   :76      \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  0.0     Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  1.0     1st Qu.: 9.000           \n Median :  100.0   Median :  4.0     Median :10.000           \n Mean   :  144.8   Mean   : 15.9     Mean   : 9.198           \n 3rd Qu.:  170.0   3rd Qu.: 17.0     3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.0     Max.   :10.000           \n                                     NA's   :10195            \n review_scores_location review_scores_value instant_bookable\n Min.   : 2.000         Min.   : 2.000      Mode :logical   \n 1st Qu.: 9.000         1st Qu.: 9.000      FALSE:32759     \n Median :10.000         Median :10.000      TRUE :7869      \n Mean   : 9.414         Mean   : 9.332                      \n 3rd Qu.:10.000         3rd Qu.:10.000                      \n Max.   :10.000         Max.   :10.000                      \n NA's   :10254          NA's   :10256                       \n\n\n\n# Check for missing values in relevant columns\nairbnb %&gt;%\n  select(number_of_reviews, bathrooms, bedrooms, price,\n         review_scores_cleanliness, review_scores_location,\n         review_scores_value, instant_bookable, room_type) %&gt;%\n  summarise_all(~sum(is.na(.)))\n\n# A tibble: 1 × 9\n  number_of_reviews bathrooms bedrooms price review_scores_cleanliness\n              &lt;int&gt;     &lt;int&gt;    &lt;int&gt; &lt;int&gt;                     &lt;int&gt;\n1                 0       160       76     0                     10195\n# ℹ 4 more variables: review_scores_location &lt;int&gt;, review_scores_value &lt;int&gt;,\n#   instant_bookable &lt;int&gt;, room_type &lt;int&gt;\n\n# Drop rows with any missing values in relevant variables\nairbnb_clean &lt;- airbnb %&gt;%\n  filter(!is.na(number_of_reviews),\n         !is.na(bathrooms),\n         !is.na(bedrooms),\n         !is.na(price),\n         !is.na(review_scores_cleanliness),\n         !is.na(review_scores_location),\n         !is.na(review_scores_value),\n         !is.na(instant_bookable),\n         !is.na(room_type))\n\n\n# Number of rows before cleaning\nn_before &lt;- nrow(airbnb)\n\n# Number of rows after cleaning\nn_after &lt;- nrow(airbnb_clean)\n\n# Create a simple comparison table\ntibble(\n  Stage = c(\"Before Cleaning\", \"After Cleaning\"),\n  Rows = c(n_before, n_after)\n) %&gt;%\n  knitr::kable(caption = \"Number of Rows Before and After Dropping Missing Values\")\n\n\nNumber of Rows Before and After Dropping Missing Values\n\n\nStage\nRows\n\n\n\n\nBefore Cleaning\n40628\n\n\nAfter Cleaning\n30160\n\n\n\n\n\n\n# Histogram of number of reviews\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(binwidth = 10, fill = \"#0072B2\", color = \"black\") +\n  labs(title = \"Distribution of Number of Reviews\",\n       x = \"Number of Reviews\",\n       y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram of price\nggplot(airbnb_clean, aes(x = price)) +\n  geom_histogram(binwidth = 25, fill = \"#009E73\", color = \"black\") +\n  labs(title = \"Distribution of Price per Night\",\n       x = \"Price (USD)\",\n       y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram of bedrooms\nggplot(airbnb_clean, aes(x = bedrooms)) +\n  geom_histogram(binwidth = 1, fill = \"#D55E00\", color = \"black\") +\n  labs(title = \"Distribution of Bedrooms\",\n       x = \"Number of Bedrooms\",\n       y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\n\n\n\n\n\n# Fit a Poisson regression model\npoisson_model &lt;- glm(number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n                       review_scores_cleanliness + review_scores_location +\n                       review_scores_value + instant_bookable,\n                     data = airbnb_clean,\n                     family = poisson())\n\n# Display model summary\nsummary(poisson_model)\n\n\nCall:\nglm(formula = number_of_reviews ~ room_type + bathrooms + bedrooms + \n    price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson(), \n    data = airbnb_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\ninstant_bookableTRUE       3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe Poisson model assumes that the mean and variance of the outcome are equal. However, in practice (especially with review counts), overdispersion is common — meaning the variance is greater than the mean.\nTo account for this, we fit a Negative Binomial Regression, which introduces a dispersion parameter to relax the equal-mean-variance constraint. It’s a natural extension of the Poisson model.\n\n# Load MASS for negative binomial\nlibrary(MASS)\n\n# Fit negative binomial model\nnb_model &lt;- glm.nb(number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n                     review_scores_cleanliness + review_scores_location +\n                     review_scores_value + instant_bookable,\n                   data = airbnb_clean)\n\n# Display model summary\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = number_of_reviews ~ room_type + bathrooms + \n    bedrooms + price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, data = airbnb_clean, \n    init.theta = 0.7014236159, link = log)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                4.200e+00  9.429e-02  44.548  &lt; 2e-16 ***\nroom_typePrivate room      8.269e-03  1.514e-02   0.546    0.585    \nroom_typeShared room      -2.221e-01  4.351e-02  -5.105 3.30e-07 ***\nbathrooms                 -1.150e-01  2.029e-02  -5.669 1.44e-08 ***\nbedrooms                   7.404e-02  1.135e-02   6.522 6.96e-11 ***\nprice                     -1.253e-06  4.128e-05  -0.030    0.976    \nreview_scores_cleanliness  1.981e-01  8.045e-03  24.626  &lt; 2e-16 ***\nreview_scores_location    -1.099e-01  9.419e-03 -11.670  &lt; 2e-16 ***\nreview_scores_value       -2.119e-01  1.048e-02 -20.217  &lt; 2e-16 ***\ninstant_bookableTRUE       3.254e-01  1.766e-02  18.429  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.7014) family taken to be 1)\n\n    Null deviance: 35578  on 30159  degrees of freedom\nResidual deviance: 34576  on 30150  degrees of freedom\nAIC: 242052\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.70142 \n          Std. Err.:  0.00522 \n\n 2 x log-likelihood:  -242029.73500 \n\n\nBoth the Poisson and Negative Binomial regression models aim to explain variation in the number of Airbnb reviews (used as a proxy for bookings) as a function of property characteristics.\nKey Findings: Room Type: Compared to the baseline category (Entire home/apt), listings that are Private rooms or Shared rooms consistently receive fewer reviews. In the Poisson model, these have large negative coefficients, and in the Negative Binomial model, Shared room remains significantly negative, while Private room becomes statistically insignificant. This suggests that whole units are more popular/booked more frequently.\nBathrooms: The coefficient for bathrooms is negative and statistically significant in both models. This may reflect diminishing returns — beyond a certain point, more bathrooms don’t contribute to more bookings.\nBedrooms: The number of bedrooms has a strong, positive, and significant effect. Each additional bedroom is associated with a substantial increase in the expected number of reviews, suggesting that larger accommodations are more frequently booked.\nPrice: The effect of price is small and not consistently significant. In both models, the coefficients are near zero and not statistically meaningful, implying that price alone is not a strong predictor of bookings when other factors are accounted for.\nReview Scores:\nCleanliness has a positive and highly significant effect, indicating that higher cleanliness ratings are associated with more reviews/bookings.\nLocation and Value scores both show significant negative coefficients, which may seem counterintuitive. However, these scores are often compressed near the upper limit (e.g., most listings get 8–10), and the negative relationship may reflect lower variance or interactions with other unobserved factors.\nInstant Bookable: Listings that are instantly bookable receive significantly more reviews. In the Negative Binomial model, being instant bookable is associated with about a 38% increase in expected review count (exp(0.325) ≈ 1.38), holding all else constant.\nWhy Use Negative Binomial? The Negative Binomial model is preferred in this case due to overdispersion — the variance in number of reviews greatly exceeds the mean. The Negative Binomial model accounts for this with a dispersion parameter and provides a better fit, as shown by a substantially lower AIC (Poisson AIC: ~1,058,014 vs. NB AIC: ~242,052)."
  },
  {
    "objectID": "blog/Project 6/hw3_questions.html",
    "href": "blog/Project 6/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/Project 6/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/Project 6/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Project 6/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/Project 6/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))\n\n\n\n\n\n\n\n\n\n\n3. Preparing the Data for Estimation\n\n\n\n\n\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# Convert brand and ad to dummy variables\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\n# Create dummy variables: reference categories are Hulu for brand, and Ad-Free for ads\nmnl_data &lt;- conjoint_data %&gt;%\n  mutate(\n    brand_N = ifelse(brand == \"N\", 1, 0),\n    brand_P = ifelse(brand == \"P\", 1, 0),\n    ad_yes  = ifelse(ad == \"Yes\", 1, 0)\n  ) %&gt;%\n  select(resp, task, brand_N, brand_P, ad_yes, price, choice)\n\n# Convert to grouped format for each choice task (each respondent x task x alternative)\nhead(mnl_data)\n\n    resp task brand_N brand_P ad_yes price choice\n31     1    1       1       0      1    28      1\n15     1    1       0       0      1    16      0\n14     1    1       0       1      1    16      0\n37     1    2       1       0      1    32      0\n141    1    2       0       1      1    16      1\n25     1    2       1       0      1    24      0\n\n\nTo prepare the data for estimation, we create dummy variables for categorical attributes. Netflix and Amazon Prime are coded as binary indicators, with Hulu as the base. Similarly, the presence of ads is coded as a binary variable, with ad-free as the baseline. This reshaped data structure allows us to easily evaluate the utility of each alternative per choice task.\n\n\n\n\n\n\n\n\n\n4. Estimation via Maximum Likelihood\n\n\n\n\n\nWe define the log-likelihood function for the multinomial logit model. This function takes a vector of coefficients (beta) and computes the utility of each alternative in each choice task. Using the exponentiated utilities, we calculate the choice probabilities and compute the log-likelihood as the sum of log-probabilities for the chosen alternatives.\nTo facilitate numerical stability and easy optimization, we return the negative log-likelihood, which can be minimized using numerical solvers.\n\n# Load necessary packages\nlibrary(dplyr)\n\n# Define log-likelihood function\nlog_likelihood &lt;- function(beta, data) {\n  # Unpack beta vector\n  b_brand_N &lt;- beta[1]\n  b_brand_P &lt;- beta[2]\n  b_ad_yes  &lt;- beta[3]\n  b_price   &lt;- beta[4]\n  \n  # Compute utility\n  data &lt;- data %&gt;%\n    mutate(\n      v = b_brand_N * brand_N +\n          b_brand_P * brand_P +\n          b_ad_yes  * ad_yes +\n          b_price   * price\n    )\n  \n  # Normalize within each choice task using log-sum-exp trick\n  data &lt;- data %&gt;%\n    group_by(resp, task) %&gt;%\n    mutate(\n      exp_v = exp(v),\n      sum_exp_v = sum(exp_v),\n      prob = exp_v / sum_exp_v\n    ) %&gt;%\n    ungroup()\n  \n  # Log-likelihood: sum of log probs for chosen alternatives\n  ll &lt;- sum(log(data$prob[data$choice == 1]))\n  return(-ll)  # negative for minimization\n}\n\n# Test the function with initial guesses\ninit_beta &lt;- c(0, 0, 0, 0)  # starting values\nlog_likelihood(init_beta, mnl_data)\n\n[1] 1098.612\n\n\nWe estimate the parameters of the multinomial logit model using the BFGS optimization method via optim(). The algorithm returns the MLEs of the four utility parameters and the Hessian matrix, which we invert to obtain the variance-covariance matrix.\nFrom the diagonal of the variance-covariance matrix, we compute standard errors, and then construct 95% confidence intervals using the normal approximation.\nThe table below summarizes the results:\n\n# Estimate parameters using optim\nmle_result &lt;- optim(\n  par = c(0, 0, 0, 0),         # initial guesses\n  fn = log_likelihood,\n  data = mnl_data,\n  method = \"BFGS\",\n  hessian = TRUE\n)\n\n# Extract results\nbeta_hat &lt;- mle_result$par\nvcov_matrix &lt;- solve(mle_result$hessian)   # inverse Hessian is the variance-covariance matrix\nse &lt;- sqrt(diag(vcov_matrix))              # standard errors\nz_score &lt;- qnorm(0.975)                    # for 95% confidence interval\n\n# Compute 95% confidence intervals\nci_lower &lt;- beta_hat - z_score * se\nci_upper &lt;- beta_hat + z_score * se\n\n# Combine into a tidy summary table\nmle_summary &lt;- data.frame(\n  Parameter = c(\"β_Netflix\", \"β_Prime\", \"β_Ads\", \"β_Price\"),\n  Estimate = beta_hat,\n  Std_Error = se,\n  CI_Lower = ci_lower,\n  CI_Upper = ci_upper\n)\n\n# Display results\nlibrary(knitr)\nkable(mle_summary, digits = 3, caption = \"MLE Estimates with 95% Confidence Intervals\")\n\n\nMLE Estimates with 95% Confidence Intervals\n\n\nParameter\nEstimate\nStd_Error\nCI_Lower\nCI_Upper\n\n\n\n\nβ_Netflix\n0.941\n0.111\n0.724\n1.159\n\n\nβ_Prime\n0.502\n0.111\n0.284\n0.719\n\n\nβ_Ads\n-0.732\n0.088\n-0.904\n-0.560\n\n\nβ_Price\n-0.099\n0.006\n-0.112\n-0.087\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Estimation via Bayesian Methods\n\n\n\n\n\nWe implement a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the MNL model parameters. We use weakly informative normal priors: N(0,5) for the binary variables and N(0,1) for the price coefficient. The proposal distribution adds small random noise independently to each parameter.\nThe sampler runs for 11,000 steps, discards the first 1,000 as burn-in, and retains 10,000 posterior draws. The resulting samples approximate the joint posterior distribution over the four coefficients.\n\n# Log-prior function (sum of log-Normal densities)\nlog_prior &lt;- function(beta) {\n  sum(dnorm(beta[1:3], mean = 0, sd = sqrt(5), log = TRUE)) +  # binary variables\n    dnorm(beta[4], mean = 0, sd = 1, log = TRUE)               # price\n}\n\n# Log-posterior: sum of log-likelihood and log-prior\nlog_posterior &lt;- function(beta, data) {\n  -log_likelihood(beta, data) + log_prior(beta)\n}\n\n# Metropolis-Hastings MCMC function\nmh_sampler &lt;- function(start, n_iter, proposal_sd, data) {\n  beta_curr &lt;- start\n  post_curr &lt;- log_posterior(beta_curr, data)\n  chain &lt;- matrix(NA, nrow = n_iter, ncol = 4)\n  \n  for (i in 1:n_iter) {\n    # Propose new beta values\n    beta_prop &lt;- beta_curr + rnorm(4, mean = 0, sd = proposal_sd)\n    post_prop &lt;- log_posterior(beta_prop, data)\n    \n    # Acceptance probability\n    alpha &lt;- exp(post_prop - post_curr)\n    if (runif(1) &lt; alpha) {\n      beta_curr &lt;- beta_prop\n      post_curr &lt;- post_prop\n    }\n    \n    # Save draw\n    chain[i, ] &lt;- beta_curr\n  }\n  \n  colnames(chain) &lt;- c(\"β_Netflix\", \"β_Prime\", \"β_Ads\", \"β_Price\")\n  return(chain)\n}\n\n# Run the sampler\nset.seed(42)\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\nmh_chain &lt;- mh_sampler(start = c(0, 0, 0, 0), n_iter = 11000, proposal_sd = proposal_sd, data = mnl_data)\n\n# Discard burn-in\nmh_chain_post &lt;- mh_chain[1001:11000, ]\n\nBelow are the trace plot and posterior histogram for the β_Netflix parameter.\nThe trace plot helps assess convergence and mixing of the chain. It should look like a “fat fuzzy caterpillar” without major drifts or stickiness.\nThe histogram represents the marginal posterior distribution, providing an estimate of the parameter and its uncertainty.\n\nlibrary(ggplot2)\n\n# Extract draws for β_Netflix\nbeta_netflix &lt;- mh_chain_post[, \"β_Netflix\"]\ndraws &lt;- 1:length(beta_netflix)\n\n# Trace plot\ntrace_df &lt;- data.frame(Draw = draws, Beta = beta_netflix)\nggplot(trace_df, aes(x = Draw, y = Beta)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"Trace Plot for β_Netflix\",\n       x = \"MCMC Draw\",\n       y = expression(beta[Netflix])) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram of posterior\nhist_df &lt;- data.frame(Beta = beta_netflix)\nggplot(hist_df, aes(x = Beta)) +\n  geom_histogram(bins = 40, fill = \"darkorange\", color = \"white\") +\n  labs(title = \"Posterior Distribution of β_Netflix\",\n       x = expression(beta[Netflix]),\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe table below compares the parameter estimates from the Maximum Likelihood Estimation (MLE) and the Bayesian posterior summaries obtained via Metropolis-Hastings MCMC.\nThe Bayesian estimates are similar to the MLEs in point estimates, indicating consistency.\nThe standard deviations of the posterior distributions play a similar role to standard errors in MLE, and the 95% credible intervals are often slightly wider, reflecting additional uncertainty from the prior.\nThis comparison highlights the value of Bayesian methods in not only estimating the parameters but also fully characterizing their uncertainty.\n\n# Compute posterior summaries\nposterior_summary &lt;- apply(mh_chain_post, 2, function(param) {\n  mean_val &lt;- mean(param)\n  sd_val &lt;- sd(param)\n  ci &lt;- quantile(param, probs = c(0.025, 0.975))\n  c(Mean = mean_val, SD = sd_val, CI_Lower = ci[1], CI_Upper = ci[2])\n})\n\n# Convert to data frame for display\nposterior_summary_df &lt;- as.data.frame(t(posterior_summary))\nposterior_summary_df$Parameter &lt;- rownames(posterior_summary_df)\nrownames(posterior_summary_df) &lt;- NULL\n\n# Add MLE estimates and SEs for comparison\nmle_summary$Method &lt;- \"MLE\"\nposterior_summary_df$Method &lt;- \"Bayesian\"\n\n# Standardize column names for merging\ncolnames(posterior_summary_df)[1:4] &lt;- c(\"Estimate\", \"Std_Error\", \"CI_Lower\", \"CI_Upper\")\n\n# Combine both summaries into one table\ncombined_results &lt;- rbind(mle_summary, posterior_summary_df)\n\n# Display side-by-side\nlibrary(knitr)\nkable(combined_results[, c(\"Parameter\", \"Method\", \"Estimate\", \"Std_Error\", \"CI_Lower\", \"CI_Upper\")],\n      digits = 3,\n      caption = \"Comparison of MLE and Bayesian Posterior Summaries\")\n\n\nComparison of MLE and Bayesian Posterior Summaries\n\n\nParameter\nMethod\nEstimate\nStd_Error\nCI_Lower\nCI_Upper\n\n\n\n\nβ_Netflix\nMLE\n0.941\n0.111\n0.724\n1.159\n\n\nβ_Prime\nMLE\n0.502\n0.111\n0.284\n0.719\n\n\nβ_Ads\nMLE\n-0.732\n0.088\n-0.904\n-0.560\n\n\nβ_Price\nMLE\n-0.099\n0.006\n-0.112\n-0.087\n\n\nβ_Netflix\nBayesian\n0.942\n0.108\n0.731\n1.146\n\n\nβ_Prime\nBayesian\n0.495\n0.111\n0.282\n0.709\n\n\nβ_Ads\nBayesian\n-0.736\n0.090\n-0.915\n-0.557\n\n\nβ_Price\nBayesian\n-0.100\n0.006\n-0.113\n-0.088\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Discussion\n\n\n\n\n\n\n6.1 Interpreting Parameter Estimates Without Simulation\nIf we hadn’t simulated the data ourselves, we would still observe that the parameter estimates yield intuitive results:\nThe estimate for \\(\\beta_\\text{Netflix}\\) is consistently larger than \\(\\beta_\\text{Prime}\\) under both MLE and Bayesian methods. This indicates that, on average, respondents derive more utility from Netflix than from Amazon Prime, all else being equal.\nThe estimate for \\(\\beta_\\text{price}\\) is negative, which is exactly what we would expect. It implies that higher prices reduce utility, and consumers prefer lower-cost options.\nThese findings align well with economic theory and expected consumer behavior, suggesting that the model is functioning properly and capturing realistic preferences.\n\n\n6.2 Extending to Hierarchical (Multi-Level) Models\nIn this assignment, we used a pooled (fixed-effect) model, assuming a single set of coefficients applies to all individuals. This is a useful starting point but oversimplifies real-world behavior.\nIn practice, people differ in their preferences. A multi-level (hierarchical or random-effects) model addresses this by allowing each respondent to have their own set of coefficients:\nWe would simulate individual-level parameters by drawing each respondent’s \\(\\beta_i\\) from a population distribution (e.g., \\(\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\)).\nTo estimate such a model, we would need to infer both the population-level parameters (mean and covariance) and the individual-level betas.\nEstimation typically requires fully Bayesian approaches, such as Gibbs sampling or Hamiltonian Monte Carlo (HMC) via Stan, since the posterior involves many latent variables (one set of betas per individual).\nThis hierarchical approach better captures preference heterogeneity and is commonly used for analyzing real-world conjoint data."
  },
  {
    "objectID": "blog/Project 6/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/Project 6/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data\n\n# Convert brand and ad to dummy variables\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\n# Create dummy variables: reference categories are Hulu for brand, and Ad-Free for ads\nmnl_data &lt;- conjoint_data %&gt;%\n  mutate(\n    brand_N = ifelse(brand == \"N\", 1, 0),\n    brand_P = ifelse(brand == \"P\", 1, 0),\n    ad_yes  = ifelse(ad == \"Yes\", 1, 0)\n  ) %&gt;%\n  select(resp, task, brand_N, brand_P, ad_yes, price, choice)\n\n# Convert to grouped format for each choice task (each respondent x task x alternative)\nhead(mnl_data)\n\n    resp task brand_N brand_P ad_yes price choice\n31     1    1       1       0      1    28      1\n15     1    1       0       0      1    16      0\n14     1    1       0       1      1    16      0\n37     1    2       1       0      1    32      0\n141    1    2       0       1      1    16      1\n25     1    2       1       0      1    24      0"
  },
  {
    "objectID": "blog/Project 6/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/Project 6/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\n\n# Load necessary packages\nlibrary(dplyr)\n\n# Define log-likelihood function\nlog_likelihood &lt;- function(beta, data) {\n  # Unpack beta vector\n  b_brand_N &lt;- beta[1]\n  b_brand_P &lt;- beta[2]\n  b_ad_yes  &lt;- beta[3]\n  b_price   &lt;- beta[4]\n  \n  # Compute utility\n  data &lt;- data %&gt;%\n    mutate(\n      v = b_brand_N * brand_N +\n          b_brand_P * brand_P +\n          b_ad_yes  * ad_yes +\n          b_price   * price\n    )\n  \n  # Normalize within each choice task using log-sum-exp trick\n  data &lt;- data %&gt;%\n    group_by(resp, task) %&gt;%\n    mutate(\n      exp_v = exp(v),\n      sum_exp_v = sum(exp_v),\n      prob = exp_v / sum_exp_v\n    ) %&gt;%\n    ungroup()\n  \n  # Log-likelihood: sum of log probs for chosen alternatives\n  ll &lt;- sum(log(data$prob[data$choice == 1]))\n  return(-ll)  # negative for minimization\n}\n\n# Test the function with initial guesses\ninit_beta &lt;- c(0, 0, 0, 0)  # starting values\nlog_likelihood(init_beta, mnl_data)\n\n[1] 1098.612\n\n\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval.\n\n# Estimate parameters using optim\nmle_result &lt;- optim(\n  par = c(0, 0, 0, 0),         # initial guesses\n  fn = log_likelihood,\n  data = mnl_data,\n  method = \"BFGS\",\n  hessian = TRUE\n)\n\n# Extract results\nbeta_hat &lt;- mle_result$par\nvcov_matrix &lt;- solve(mle_result$hessian)   # inverse Hessian is the variance-covariance matrix\nse &lt;- sqrt(diag(vcov_matrix))              # standard errors\nz_score &lt;- qnorm(0.975)                    # for 95% confidence interval\n\n# Compute 95% confidence intervals\nci_lower &lt;- beta_hat - z_score * se\nci_upper &lt;- beta_hat + z_score * se\n\n# Combine into a tidy summary table\nmle_summary &lt;- data.frame(\n  Parameter = c(\"β_Netflix\", \"β_Prime\", \"β_Ads\", \"β_Price\"),\n  Estimate = beta_hat,\n  Std_Error = se,\n  CI_Lower = ci_lower,\n  CI_Upper = ci_upper\n)\n\n# Display results\nlibrary(knitr)\nkable(mle_summary, digits = 3, caption = \"MLE Estimates with 95% Confidence Intervals\")\n\n\nMLE Estimates with 95% Confidence Intervals\n\n\nParameter\nEstimate\nStd_Error\nCI_Lower\nCI_Upper\n\n\n\n\nβ_Netflix\n0.941\n0.111\n0.724\n1.159\n\n\nβ_Prime\n0.502\n0.111\n0.284\n0.719\n\n\nβ_Ads\n-0.732\n0.088\n-0.904\n-0.560\n\n\nβ_Price\n-0.099\n0.006\n-0.112\n-0.087"
  },
  {
    "objectID": "blog/Project 6/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/Project 6/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\n\n# Log-prior function (sum of log-Normal densities)\nlog_prior &lt;- function(beta) {\n  sum(dnorm(beta[1:3], mean = 0, sd = sqrt(5), log = TRUE)) +  # binary variables\n    dnorm(beta[4], mean = 0, sd = 1, log = TRUE)               # price\n}\n\n# Log-posterior: sum of log-likelihood and log-prior\nlog_posterior &lt;- function(beta, data) {\n  -log_likelihood(beta, data) + log_prior(beta)\n}\n\n# Metropolis-Hastings MCMC function\nmh_sampler &lt;- function(start, n_iter, proposal_sd, data) {\n  beta_curr &lt;- start\n  post_curr &lt;- log_posterior(beta_curr, data)\n  chain &lt;- matrix(NA, nrow = n_iter, ncol = 4)\n  \n  for (i in 1:n_iter) {\n    # Propose new beta values\n    beta_prop &lt;- beta_curr + rnorm(4, mean = 0, sd = proposal_sd)\n    post_prop &lt;- log_posterior(beta_prop, data)\n    \n    # Acceptance probability\n    alpha &lt;- exp(post_prop - post_curr)\n    if (runif(1) &lt; alpha) {\n      beta_curr &lt;- beta_prop\n      post_curr &lt;- post_prop\n    }\n    \n    # Save draw\n    chain[i, ] &lt;- beta_curr\n  }\n  \n  colnames(chain) &lt;- c(\"β_Netflix\", \"β_Prime\", \"β_Ads\", \"β_Price\")\n  return(chain)\n}\n\n# Run the sampler\nset.seed(42)\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\nmh_chain &lt;- mh_sampler(start = c(0, 0, 0, 0), n_iter = 11000, proposal_sd = proposal_sd, data = mnl_data)\n\n# Discard burn-in\nmh_chain_post &lt;- mh_chain[1001:11000, ]\n\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\n\nlibrary(ggplot2)\n\n# Extract draws for β_Netflix\nbeta_netflix &lt;- mh_chain_post[, \"β_Netflix\"]\ndraws &lt;- 1:length(beta_netflix)\n\n# Trace plot\ntrace_df &lt;- data.frame(Draw = draws, Beta = beta_netflix)\nggplot(trace_df, aes(x = Draw, y = Beta)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"Trace Plot for β_Netflix\",\n       x = \"MCMC Draw\",\n       y = expression(beta[Netflix])) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram of posterior\nhist_df &lt;- data.frame(Beta = beta_netflix)\nggplot(hist_df, aes(x = Beta)) +\n  geom_histogram(bins = 40, fill = \"darkorange\", color = \"white\") +\n  labs(title = \"Posterior Distribution of β_Netflix\",\n       x = expression(beta[Netflix]),\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach.\n\n# Compute posterior summaries\nposterior_summary &lt;- apply(mh_chain_post, 2, function(param) {\n  mean_val &lt;- mean(param)\n  sd_val &lt;- sd(param)\n  ci &lt;- quantile(param, probs = c(0.025, 0.975))\n  c(Mean = mean_val, SD = sd_val, CI_Lower = ci[1], CI_Upper = ci[2])\n})\n\n# Convert to data frame for display\nposterior_summary_df &lt;- as.data.frame(t(posterior_summary))\nposterior_summary_df$Parameter &lt;- rownames(posterior_summary_df)\nrownames(posterior_summary_df) &lt;- NULL\n\n# Add MLE estimates and SEs for comparison\nmle_summary$Method &lt;- \"MLE\"\nposterior_summary_df$Method &lt;- \"Bayesian\"\n\n# Standardize column names for merging\ncolnames(posterior_summary_df)[1:4] &lt;- c(\"Estimate\", \"Std_Error\", \"CI_Lower\", \"CI_Upper\")\n\n# Combine both summaries into one table\ncombined_results &lt;- rbind(mle_summary, posterior_summary_df)\n\n# Display side-by-side\nlibrary(knitr)\nkable(combined_results[, c(\"Parameter\", \"Method\", \"Estimate\", \"Std_Error\", \"CI_Lower\", \"CI_Upper\")],\n      digits = 3,\n      caption = \"Comparison of MLE and Bayesian Posterior Summaries\")\n\n\nComparison of MLE and Bayesian Posterior Summaries\n\n\nParameter\nMethod\nEstimate\nStd_Error\nCI_Lower\nCI_Upper\n\n\n\n\nβ_Netflix\nMLE\n0.941\n0.111\n0.724\n1.159\n\n\nβ_Prime\nMLE\n0.502\n0.111\n0.284\n0.719\n\n\nβ_Ads\nMLE\n-0.732\n0.088\n-0.904\n-0.560\n\n\nβ_Price\nMLE\n-0.099\n0.006\n-0.112\n-0.087\n\n\nβ_Netflix\nBayesian\n0.942\n0.108\n0.731\n1.146\n\n\nβ_Prime\nBayesian\n0.495\n0.111\n0.282\n0.709\n\n\nβ_Ads\nBayesian\n-0.736\n0.090\n-0.915\n-0.557\n\n\nβ_Price\nBayesian\n-0.100\n0.006\n-0.113\n-0.088"
  },
  {
    "objectID": "blog/Project 6/hw3_questions.html#discussion",
    "href": "blog/Project 6/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  },
  {
    "objectID": "blog/Project 7/hw4_questions.html",
    "href": "blog/Project 7/hw4_questions.html",
    "title": "From Scratch to Insight: Manual ML Algorithms in Action",
    "section": "",
    "text": "This report presents hands-on applications of core machine learning and statistical modeling techniques using both real-world and synthetic datasets. The exercises cover four major areas:\n\nUnsupervised Learning with K-Means Clustering: including a manual implementation and comparison with built-in functions using the Palmer Penguins dataset.\nSupervised Learning via K-Nearest Neighbors (KNN): with synthetic data generation, model implementation, and accuracy analysis.\n\nEach section combines custom algorithm development, visual analytics, and statistical performance assessment to deepen understanding of when and how to apply these tools effectively in practice.\n\n\n\n\n\n\nK-Means\n\n\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read the CSV\npenguins &lt;- read_csv(\"palmer_penguins.csv\")\n\n# Select relevant variables and remove missing values\npenguins_clean &lt;- penguins %&gt;%\n  select(bill_length_mm, flipper_length_mm) %&gt;%\n  drop_na()\n\n\n# Scatterplot of raw data\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(color = \"steelblue\", size = 2, alpha = 0.7) +\n  labs(title = \"Raw Penguin Data\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n5 ### Manual Implementation of K-Means Algorithm\nThe kmeans_manual function implements the K-Means clustering algorithm from scratch in R. It begins by randomly selecting k initial centroids from the dataset. For each iteration, the algorithm calculates the Euclidean distance from every data point to each centroid, assigns each point to the nearest centroid, and then updates centroids by computing the mean of all points assigned to each cluster. The loop continues until the centroids stabilize or a maximum number of iterations is reached. This implementation includes safeguards, such as retaining the previous centroid if a cluster becomes empty. The output includes final cluster assignments and centroid coordinates, which are then used to label the data for visualization. This approach mirrors the internal logic of R’s kmeans() function and offers valuable insight into how unsupervised learning algorithms operate step-by-step.\n\nkmeans_manual &lt;- function(data, k = 3, max_iter = 10) {\n  set.seed(123)\n  centroids &lt;- data[sample(nrow(data), k), ]\n  \n  for (iter in 1:max_iter) {\n    # Step 1: Compute distance to each centroid\n    distances &lt;- sapply(1:k, function(i) {\n      rowSums((data - matrix(rep(as.numeric(centroids[i, ]), each = nrow(data)), nrow = nrow(data)))^2)\n    })\n    \n    # Step 2: Assign each point to the nearest centroid\n    cluster_assignment &lt;- apply(distances, 1, which.min)\n    \n    # Step 3: Update centroids\n    new_centroids &lt;- data.frame()\n    for (i in 1:k) {\n      cluster_points &lt;- data[which(cluster_assignment == i), ]\n      if (nrow(cluster_points) &gt; 0) {\n        new_centroids &lt;- rbind(new_centroids, colMeans(cluster_points))\n      } else {\n        new_centroids &lt;- rbind(new_centroids, centroids[i, ])  # keep old if cluster is empty\n      }\n    }\n    \n    # Stop if centroids didn't change\n    if (all(round(as.matrix(centroids), 6) == round(as.matrix(new_centroids), 6))) {\n      break\n    }\n    centroids &lt;- new_centroids\n  }\n  \n  return(list(clusters = cluster_assignment, centroids = centroids))\n}\nmanual_result &lt;- kmeans_manual(penguins_clean, k = 3)\npenguins_clean$cluster_manual &lt;- as.factor(manual_result$clusters)\n\n\n\n\n\nmanual_centroids &lt;- manual_result$centroids\ncolnames(manual_centroids) &lt;- c(\"bill_length_mm\", \"flipper_length_mm\")\n\n\n# Plot manual k-means result (fixed)\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_manual)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = manual_centroids,\n             aes(x = bill_length_mm, y = flipper_length_mm),\n             color = \"black\", shape = 4, size = 4, stroke = 1.5, inherit.aes = FALSE) +\n  labs(title = \"Manual K-Means Clustering (k=3)\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n# Run built-in kmeans\nset.seed(123)\nkmeans_builtin &lt;- kmeans(penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")], centers = 3, nstart = 20)\npenguins_clean$cluster_builtin &lt;- as.factor(kmeans_builtin$cluster)\n\n# Plot built-in clustering\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_builtin)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = as.data.frame(kmeans_builtin$centers),\n             aes(x = bill_length_mm, y = flipper_length_mm),\n             color = \"black\", shape = 4, size = 4, stroke = 1.5) +\n  labs(title = \"Built-in KMeans Clustering (k=3)\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe two plots above compare the results of a manually implemented K-Means algorithm (left) with R’s built-in kmeans() function (right), both applied to the Palmer Penguins dataset using bill length and flipper length as clustering features. Both methods identified similar underlying structure in the data, producing three coherent clusters that align well with natural groupings. The manually implemented algorithm achieved a comparable result, although the specific cluster labels and centroid positions show slight variation due to random initialization and convergence behavior. The consistency between the two visualizations validates the correctness of the custom implementation and illustrates the robustness of K-Means in discovering distinct groups based on geometric distance.\n\n\n\n\n# Load required libraries\nlibrary(cluster)  # for silhouette\n\n# Data to cluster\ndata_kmeans &lt;- penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")]\n\n# Initialize results\nwcss &lt;- c()\nsil_scores &lt;- c()\n\n# Loop through K = 2 to 7\nfor (k in 2:7) {\n  set.seed(42)\n  km &lt;- kmeans(data_kmeans, centers = k, nstart = 20)\n  wcss[k] &lt;- km$tot.withinss\n  \n  # Compute silhouette score\n  sil &lt;- silhouette(km$cluster, dist(data_kmeans))\n  sil_scores[k] &lt;- mean(sil[, 3])\n}\n\n# Create result data frame\nmetrics_df &lt;- tibble(\n  k = 2:7,\n  WCSS = wcss[2:7],\n  Silhouette = sil_scores[2:7]\n)\n\n\n# Plot WCSS\nggplot(metrics_df, aes(x = k, y = WCSS)) +\n  geom_line() + geom_point() +\n  labs(title = \"Within-Cluster Sum of Squares (WCSS)\",\n       x = \"Number of Clusters (k)\",\n       y = \"WCSS\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plot Silhouette Score\nggplot(metrics_df, aes(x = k, y = Silhouette)) +\n  geom_line(color = \"darkgreen\") + geom_point(color = \"darkgreen\") +\n  labs(title = \"Average Silhouette Score\",\n       x = \"Number of Clusters (k)\",\n       y = \"Silhouette Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe two charts show how well the data groups into clusters when using different numbers of clusters. In the first chart, the total distance within each cluster drops sharply from 2 to 3 clusters, then decreases more slowly from 4 to 7 clusters. This suggests that 3 clusters give a good balance before the improvement starts to level off.\nIn the second chart, which shows how clearly separated the clusters are, the best separation happens with 2 clusters. However, the separation becomes slightly worse as the number of clusters increases, especially after 3 clusters. Although 2 clusters are the most distinct, they may not capture all the structure in the data.\nOverall, 3 clusters seem to offer the best balance between keeping points close together within clusters and keeping the clusters clearly separated.\n\n\n\n\n\n\n\n\n\n\nK Nearest Neighbors\n\n\n\n\n\n\n\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\n# Generate synthetic dataset\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nboundary &lt;- sin(4 * x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\n\n\n\n# Base plot\nggplot(dat, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  stat_function(fun = function(x) sin(4 * x) + x, \n                color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Synthetic Data for KNN\",\n       subtitle = \"Dashed line shows the true decision boundary\",\n       x = \"x1\", y = \"x2\", color = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n# Generate test dataset with different seed\nset.seed(99)\nn_test &lt;- 100\nx1_test &lt;- runif(n_test, -3, 3)\nx2_test &lt;- runif(n_test, -3, 3)\nboundary_test &lt;- sin(4 * x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\ntest_data &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\n\n\n\n\nggplot(test_data, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  stat_function(fun = function(x) sin(4 * x) + x, \n                color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Test Data for KNN\",\n       subtitle = \"Dashed line shows the true decision boundary\",\n       x = \"x1\", y = \"x2\", color = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThe first plot shows the training data for the K-Nearest Neighbors (KNN) model. Each point is colored based on its class label: class 0 in pink and class 1 in blue. The black dashed line represents the true decision boundary that separates the two classes. The points are spread around this wavy boundary, making the classification task more challenging, especially where the two classes are close together.\nThe second plot shows a separate test dataset generated in the same way as the training data. Like the training set, the test data also has a mix of class 0 and class 1 points around the same true boundary. This test set will be used to check how accurately the KNN model can predict the class of new, unseen points, especially around the complex boundary region. The similarity in pattern between the two datasets suggests that the test set is a good match for evaluating the model fairly.\n\n\n\nThe code above implements a custom version of the K-Nearest Neighbors (KNN) classification algorithm. It begins by defining a helper function to calculate the Euclidean distance between two points. The main knn_custom function then uses this to classify each point in the test set.\nFor each test point, the function calculates its distance to all training points and identifies the k closest ones. It then checks the class labels of these k neighbors and assigns the most frequent class to the test point. This process is repeated for every point in the test data, and the function returns the full set of predicted labels.\nThis implementation provides a clear, step-by-step view of how the KNN algorithm works, making it easier to understand the logic behind this popular classification method.\n\n# Function to compute Euclidean distance between two points\neuclidean_distance &lt;- function(a, b) {\n  sqrt(sum((a - b)^2))\n}\n\n# Custom KNN function\nknn_custom &lt;- function(train_data, train_labels, test_data, k = 5) {\n  predictions &lt;- c()\n  \n  for (i in 1:nrow(test_data)) {\n    # Compute distances from test point to all training points\n    distances &lt;- apply(train_data, 1, function(row) euclidean_distance(row, test_data[i, ]))\n    \n    # Find the indices of the k nearest neighbors\n    neighbor_indices &lt;- order(distances)[1:k]\n    \n    # Get the labels of the k nearest neighbors\n    neighbor_labels &lt;- train_labels[neighbor_indices]\n    \n    # Predict the most common class\n    predicted_class &lt;- names(sort(table(neighbor_labels), decreasing = TRUE))[1]\n    \n    predictions &lt;- c(predictions, predicted_class)\n  }\n  \n  return(as.factor(predictions))\n}\n\n\n\n\n\n# Prepare training and test input\ntrain_X &lt;- dat[, c(\"x1\", \"x2\")]\ntrain_y &lt;- dat$y\n\ntest_X &lt;- test_data[, c(\"x1\", \"x2\")]\ntest_y &lt;- test_data$y\n\n# Predict using custom KNN\npred_custom &lt;- knn_custom(train_X, train_y, test_X, k = 5)\n\n# Accuracy\naccuracy_custom &lt;- mean(pred_custom == test_y)\npaste(\"Custom KNN accuracy:\", round(accuracy_custom * 100, 2), \"%\")\n\n[1] \"Custom KNN accuracy: 84 %\"\n\n\n\n\n\n\n# Load the class package\nlibrary(class)\n\n# Use built-in knn() function\npred_builtin &lt;- knn(train = train_X,\n                    test = test_X,\n                    cl = train_y,\n                    k = 5)\n\n# Accuracy comparison\naccuracy_builtin &lt;- mean(pred_builtin == test_y)\npaste(\"Built-in KNN accuracy:\", round(accuracy_builtin * 100, 2), \"%\")\n\n[1] \"Built-in KNN accuracy: 84 %\"\n\n\n\n\n\n\n# Initialize vector to store accuracy results\naccuracy_vec &lt;- numeric(30)\n\n# Loop over k from 1 to 30\nfor (k in 1:30) {\n  pred_k &lt;- knn(train = train_X,\n                test = test_X,\n                cl = train_y,\n                k = k)\n  \n  accuracy_vec[k] &lt;- mean(pred_k == test_y)\n}\n\n# Create a data frame for plotting\naccuracy_df &lt;- data.frame(\n  k = 1:30,\n  accuracy = accuracy_vec\n)\n\n\nggplot(accuracy_df, aes(x = k, y = accuracy)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"red\") +\n  labs(title = \"KNN Classification Accuracy vs. k\",\n       x = \"Number of Neighbors (k)\",\n       y = \"Accuracy on Test Data\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis chart shows how the accuracy of the K-Nearest Neighbors (KNN) model changes as the number of neighbors increases from 1 to 30. Each point represents how well the model was able to classify the test data for a given value of k.\nThe accuracy goes up and down as k changes. It reaches high points around k = 5, k = 17, and k = 29, where the model performs best. The highest accuracy is seen at k = 29. This suggests that using more neighbors can sometimes help the model make more reliable predictions by reducing the effect of noisy points.\nThere are also lower points in the chart, especially around k = 10 and k = 26, where the model performs worse. This shows that choosing the right value of k is important, and testing different options is necessary to find the one that gives the best results."
  },
  {
    "objectID": "blog/Project 7/hw4_questions.html#introduction",
    "href": "blog/Project 7/hw4_questions.html#introduction",
    "title": "From Scratch to Insight: Manual ML Algorithms in Action",
    "section": "",
    "text": "This report presents hands-on applications of core machine learning and statistical modeling techniques using both real-world and synthetic datasets. The exercises cover four major areas:\n\nUnsupervised Learning with K-Means Clustering: including a manual implementation and comparison with built-in functions using the Palmer Penguins dataset.\nSupervised Learning via K-Nearest Neighbors (KNN): with synthetic data generation, model implementation, and accuracy analysis.\n\nEach section combines custom algorithm development, visual analytics, and statistical performance assessment to deepen understanding of when and how to apply these tools effectively in practice.\n\n\n\n\n\n\nK-Means\n\n\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read the CSV\npenguins &lt;- read_csv(\"palmer_penguins.csv\")\n\n# Select relevant variables and remove missing values\npenguins_clean &lt;- penguins %&gt;%\n  select(bill_length_mm, flipper_length_mm) %&gt;%\n  drop_na()\n\n\n# Scatterplot of raw data\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(color = \"steelblue\", size = 2, alpha = 0.7) +\n  labs(title = \"Raw Penguin Data\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n5 ### Manual Implementation of K-Means Algorithm\nThe kmeans_manual function implements the K-Means clustering algorithm from scratch in R. It begins by randomly selecting k initial centroids from the dataset. For each iteration, the algorithm calculates the Euclidean distance from every data point to each centroid, assigns each point to the nearest centroid, and then updates centroids by computing the mean of all points assigned to each cluster. The loop continues until the centroids stabilize or a maximum number of iterations is reached. This implementation includes safeguards, such as retaining the previous centroid if a cluster becomes empty. The output includes final cluster assignments and centroid coordinates, which are then used to label the data for visualization. This approach mirrors the internal logic of R’s kmeans() function and offers valuable insight into how unsupervised learning algorithms operate step-by-step.\n\nkmeans_manual &lt;- function(data, k = 3, max_iter = 10) {\n  set.seed(123)\n  centroids &lt;- data[sample(nrow(data), k), ]\n  \n  for (iter in 1:max_iter) {\n    # Step 1: Compute distance to each centroid\n    distances &lt;- sapply(1:k, function(i) {\n      rowSums((data - matrix(rep(as.numeric(centroids[i, ]), each = nrow(data)), nrow = nrow(data)))^2)\n    })\n    \n    # Step 2: Assign each point to the nearest centroid\n    cluster_assignment &lt;- apply(distances, 1, which.min)\n    \n    # Step 3: Update centroids\n    new_centroids &lt;- data.frame()\n    for (i in 1:k) {\n      cluster_points &lt;- data[which(cluster_assignment == i), ]\n      if (nrow(cluster_points) &gt; 0) {\n        new_centroids &lt;- rbind(new_centroids, colMeans(cluster_points))\n      } else {\n        new_centroids &lt;- rbind(new_centroids, centroids[i, ])  # keep old if cluster is empty\n      }\n    }\n    \n    # Stop if centroids didn't change\n    if (all(round(as.matrix(centroids), 6) == round(as.matrix(new_centroids), 6))) {\n      break\n    }\n    centroids &lt;- new_centroids\n  }\n  \n  return(list(clusters = cluster_assignment, centroids = centroids))\n}\nmanual_result &lt;- kmeans_manual(penguins_clean, k = 3)\npenguins_clean$cluster_manual &lt;- as.factor(manual_result$clusters)\n\n\n\n\n\nmanual_centroids &lt;- manual_result$centroids\ncolnames(manual_centroids) &lt;- c(\"bill_length_mm\", \"flipper_length_mm\")\n\n\n# Plot manual k-means result (fixed)\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_manual)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = manual_centroids,\n             aes(x = bill_length_mm, y = flipper_length_mm),\n             color = \"black\", shape = 4, size = 4, stroke = 1.5, inherit.aes = FALSE) +\n  labs(title = \"Manual K-Means Clustering (k=3)\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n# Run built-in kmeans\nset.seed(123)\nkmeans_builtin &lt;- kmeans(penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")], centers = 3, nstart = 20)\npenguins_clean$cluster_builtin &lt;- as.factor(kmeans_builtin$cluster)\n\n# Plot built-in clustering\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_builtin)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = as.data.frame(kmeans_builtin$centers),\n             aes(x = bill_length_mm, y = flipper_length_mm),\n             color = \"black\", shape = 4, size = 4, stroke = 1.5) +\n  labs(title = \"Built-in KMeans Clustering (k=3)\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe two plots above compare the results of a manually implemented K-Means algorithm (left) with R’s built-in kmeans() function (right), both applied to the Palmer Penguins dataset using bill length and flipper length as clustering features. Both methods identified similar underlying structure in the data, producing three coherent clusters that align well with natural groupings. The manually implemented algorithm achieved a comparable result, although the specific cluster labels and centroid positions show slight variation due to random initialization and convergence behavior. The consistency between the two visualizations validates the correctness of the custom implementation and illustrates the robustness of K-Means in discovering distinct groups based on geometric distance.\n\n\n\n\n# Load required libraries\nlibrary(cluster)  # for silhouette\n\n# Data to cluster\ndata_kmeans &lt;- penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")]\n\n# Initialize results\nwcss &lt;- c()\nsil_scores &lt;- c()\n\n# Loop through K = 2 to 7\nfor (k in 2:7) {\n  set.seed(42)\n  km &lt;- kmeans(data_kmeans, centers = k, nstart = 20)\n  wcss[k] &lt;- km$tot.withinss\n  \n  # Compute silhouette score\n  sil &lt;- silhouette(km$cluster, dist(data_kmeans))\n  sil_scores[k] &lt;- mean(sil[, 3])\n}\n\n# Create result data frame\nmetrics_df &lt;- tibble(\n  k = 2:7,\n  WCSS = wcss[2:7],\n  Silhouette = sil_scores[2:7]\n)\n\n\n# Plot WCSS\nggplot(metrics_df, aes(x = k, y = WCSS)) +\n  geom_line() + geom_point() +\n  labs(title = \"Within-Cluster Sum of Squares (WCSS)\",\n       x = \"Number of Clusters (k)\",\n       y = \"WCSS\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plot Silhouette Score\nggplot(metrics_df, aes(x = k, y = Silhouette)) +\n  geom_line(color = \"darkgreen\") + geom_point(color = \"darkgreen\") +\n  labs(title = \"Average Silhouette Score\",\n       x = \"Number of Clusters (k)\",\n       y = \"Silhouette Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe two charts show how well the data groups into clusters when using different numbers of clusters. In the first chart, the total distance within each cluster drops sharply from 2 to 3 clusters, then decreases more slowly from 4 to 7 clusters. This suggests that 3 clusters give a good balance before the improvement starts to level off.\nIn the second chart, which shows how clearly separated the clusters are, the best separation happens with 2 clusters. However, the separation becomes slightly worse as the number of clusters increases, especially after 3 clusters. Although 2 clusters are the most distinct, they may not capture all the structure in the data.\nOverall, 3 clusters seem to offer the best balance between keeping points close together within clusters and keeping the clusters clearly separated.\n\n\n\n\n\n\n\n\n\n\nK Nearest Neighbors\n\n\n\n\n\n\n\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\n# Generate synthetic dataset\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nboundary &lt;- sin(4 * x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\n\n\n\n# Base plot\nggplot(dat, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  stat_function(fun = function(x) sin(4 * x) + x, \n                color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Synthetic Data for KNN\",\n       subtitle = \"Dashed line shows the true decision boundary\",\n       x = \"x1\", y = \"x2\", color = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n# Generate test dataset with different seed\nset.seed(99)\nn_test &lt;- 100\nx1_test &lt;- runif(n_test, -3, 3)\nx2_test &lt;- runif(n_test, -3, 3)\nboundary_test &lt;- sin(4 * x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\ntest_data &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\n\n\n\n\nggplot(test_data, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  stat_function(fun = function(x) sin(4 * x) + x, \n                color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Test Data for KNN\",\n       subtitle = \"Dashed line shows the true decision boundary\",\n       x = \"x1\", y = \"x2\", color = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThe first plot shows the training data for the K-Nearest Neighbors (KNN) model. Each point is colored based on its class label: class 0 in pink and class 1 in blue. The black dashed line represents the true decision boundary that separates the two classes. The points are spread around this wavy boundary, making the classification task more challenging, especially where the two classes are close together.\nThe second plot shows a separate test dataset generated in the same way as the training data. Like the training set, the test data also has a mix of class 0 and class 1 points around the same true boundary. This test set will be used to check how accurately the KNN model can predict the class of new, unseen points, especially around the complex boundary region. The similarity in pattern between the two datasets suggests that the test set is a good match for evaluating the model fairly.\n\n\n\nThe code above implements a custom version of the K-Nearest Neighbors (KNN) classification algorithm. It begins by defining a helper function to calculate the Euclidean distance between two points. The main knn_custom function then uses this to classify each point in the test set.\nFor each test point, the function calculates its distance to all training points and identifies the k closest ones. It then checks the class labels of these k neighbors and assigns the most frequent class to the test point. This process is repeated for every point in the test data, and the function returns the full set of predicted labels.\nThis implementation provides a clear, step-by-step view of how the KNN algorithm works, making it easier to understand the logic behind this popular classification method.\n\n# Function to compute Euclidean distance between two points\neuclidean_distance &lt;- function(a, b) {\n  sqrt(sum((a - b)^2))\n}\n\n# Custom KNN function\nknn_custom &lt;- function(train_data, train_labels, test_data, k = 5) {\n  predictions &lt;- c()\n  \n  for (i in 1:nrow(test_data)) {\n    # Compute distances from test point to all training points\n    distances &lt;- apply(train_data, 1, function(row) euclidean_distance(row, test_data[i, ]))\n    \n    # Find the indices of the k nearest neighbors\n    neighbor_indices &lt;- order(distances)[1:k]\n    \n    # Get the labels of the k nearest neighbors\n    neighbor_labels &lt;- train_labels[neighbor_indices]\n    \n    # Predict the most common class\n    predicted_class &lt;- names(sort(table(neighbor_labels), decreasing = TRUE))[1]\n    \n    predictions &lt;- c(predictions, predicted_class)\n  }\n  \n  return(as.factor(predictions))\n}\n\n\n\n\n\n# Prepare training and test input\ntrain_X &lt;- dat[, c(\"x1\", \"x2\")]\ntrain_y &lt;- dat$y\n\ntest_X &lt;- test_data[, c(\"x1\", \"x2\")]\ntest_y &lt;- test_data$y\n\n# Predict using custom KNN\npred_custom &lt;- knn_custom(train_X, train_y, test_X, k = 5)\n\n# Accuracy\naccuracy_custom &lt;- mean(pred_custom == test_y)\npaste(\"Custom KNN accuracy:\", round(accuracy_custom * 100, 2), \"%\")\n\n[1] \"Custom KNN accuracy: 84 %\"\n\n\n\n\n\n\n# Load the class package\nlibrary(class)\n\n# Use built-in knn() function\npred_builtin &lt;- knn(train = train_X,\n                    test = test_X,\n                    cl = train_y,\n                    k = 5)\n\n# Accuracy comparison\naccuracy_builtin &lt;- mean(pred_builtin == test_y)\npaste(\"Built-in KNN accuracy:\", round(accuracy_builtin * 100, 2), \"%\")\n\n[1] \"Built-in KNN accuracy: 84 %\"\n\n\n\n\n\n\n# Initialize vector to store accuracy results\naccuracy_vec &lt;- numeric(30)\n\n# Loop over k from 1 to 30\nfor (k in 1:30) {\n  pred_k &lt;- knn(train = train_X,\n                test = test_X,\n                cl = train_y,\n                k = k)\n  \n  accuracy_vec[k] &lt;- mean(pred_k == test_y)\n}\n\n# Create a data frame for plotting\naccuracy_df &lt;- data.frame(\n  k = 1:30,\n  accuracy = accuracy_vec\n)\n\n\nggplot(accuracy_df, aes(x = k, y = accuracy)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"red\") +\n  labs(title = \"KNN Classification Accuracy vs. k\",\n       x = \"Number of Neighbors (k)\",\n       y = \"Accuracy on Test Data\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis chart shows how the accuracy of the K-Nearest Neighbors (KNN) model changes as the number of neighbors increases from 1 to 30. Each point represents how well the model was able to classify the test data for a given value of k.\nThe accuracy goes up and down as k changes. It reaches high points around k = 5, k = 17, and k = 29, where the model performs best. The highest accuracy is seen at k = 29. This suggests that using more neighbors can sometimes help the model make more reliable predictions by reducing the effect of noisy points.\nThere are also lower points in the chart, especially around k = 10 and k = 26, where the model performs worse. This shows that choosing the right value of k is important, and testing different options is necessary to find the one that gives the best results."
  },
  {
    "objectID": "blog/Project 7/hw4_questions.html#a.-k-means",
    "href": "blog/Project 7/hw4_questions.html#a.-k-means",
    "title": "Applied ML: Clustering, Choice, Classification & Drivers",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read the CSV\npenguins &lt;- read_csv(\"palmer_penguins.csv\")\n\n# Select relevant variables and remove missing values\npenguins_clean &lt;- penguins %&gt;%\n  select(bill_length_mm, flipper_length_mm) %&gt;%\n  drop_na()\n\n\n# Scatterplot of raw data\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(color = \"steelblue\", size = 2, alpha = 0.7) +\n  labs(title = \"Raw Penguin Data\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nkmeans_manual &lt;- function(data, k = 3, max_iter = 10) {\n  set.seed(123)\n  centroids &lt;- data[sample(nrow(data), k), ]\n  \n  for (iter in 1:max_iter) {\n    # Step 1: Compute distance to each centroid\n    distances &lt;- sapply(1:k, function(i) {\n      rowSums((data - matrix(rep(as.numeric(centroids[i, ]), each = nrow(data)), nrow = nrow(data)))^2)\n    })\n    \n    # Step 2: Assign each point to the nearest centroid\n    cluster_assignment &lt;- apply(distances, 1, which.min)\n    \n    # Step 3: Update centroids\n    new_centroids &lt;- data.frame()\n    for (i in 1:k) {\n      cluster_points &lt;- data[which(cluster_assignment == i), ]\n      if (nrow(cluster_points) &gt; 0) {\n        new_centroids &lt;- rbind(new_centroids, colMeans(cluster_points))\n      } else {\n        new_centroids &lt;- rbind(new_centroids, centroids[i, ])  # keep old if cluster is empty\n      }\n    }\n    \n    # Stop if centroids didn't change\n    if (all(round(as.matrix(centroids), 6) == round(as.matrix(new_centroids), 6))) {\n      break\n    }\n    centroids &lt;- new_centroids\n  }\n  \n  return(list(clusters = cluster_assignment, centroids = centroids))\n}\nmanual_result &lt;- kmeans_manual(penguins_clean, k = 3)\npenguins_clean$cluster_manual &lt;- as.factor(manual_result$clusters)\n\n\nmanual_centroids &lt;- manual_result$centroids\ncolnames(manual_centroids) &lt;- c(\"bill_length_mm\", \"flipper_length_mm\")\n\n\n# Plot manual k-means result (fixed)\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_manual)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = manual_centroids,\n             aes(x = bill_length_mm, y = flipper_length_mm),\n             color = \"black\", shape = 4, size = 4, stroke = 1.5, inherit.aes = FALSE) +\n  labs(title = \"Manual K-Means Clustering (k=3)\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Run built-in kmeans\nset.seed(123)\nkmeans_builtin &lt;- kmeans(penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")], centers = 3, nstart = 20)\npenguins_clean$cluster_builtin &lt;- as.factor(kmeans_builtin$cluster)\n\n# Plot built-in clustering\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_builtin)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = as.data.frame(kmeans_builtin$centers),\n             aes(x = bill_length_mm, y = flipper_length_mm),\n             color = \"black\", shape = 4, size = 4, stroke = 1.5) +\n  labs(title = \"Built-in KMeans Clustering (k=3)\",\n       x = \"Bill Length (mm)\",\n       y = \"Flipper Length (mm)\",\n       color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\n\n# Load required libraries\nlibrary(cluster)  # for silhouette\n\n# Data to cluster\ndata_kmeans &lt;- penguins_clean[, c(\"bill_length_mm\", \"flipper_length_mm\")]\n\n# Initialize results\nwcss &lt;- c()\nsil_scores &lt;- c()\n\n# Loop through K = 2 to 7\nfor (k in 2:7) {\n  set.seed(42)\n  km &lt;- kmeans(data_kmeans, centers = k, nstart = 20)\n  wcss[k] &lt;- km$tot.withinss\n  \n  # Compute silhouette score\n  sil &lt;- silhouette(km$cluster, dist(data_kmeans))\n  sil_scores[k] &lt;- mean(sil[, 3])\n}\n\n# Create result data frame\nmetrics_df &lt;- tibble(\n  k = 2:7,\n  WCSS = wcss[2:7],\n  Silhouette = sil_scores[2:7]\n)\n\n\n# Plot WCSS\nggplot(metrics_df, aes(x = k, y = WCSS)) +\n  geom_line() + geom_point() +\n  labs(title = \"Within-Cluster Sum of Squares (WCSS)\",\n       x = \"Number of Clusters (k)\",\n       y = \"WCSS\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plot Silhouette Score\nggplot(metrics_df, aes(x = k, y = Silhouette)) +\n  geom_line(color = \"darkgreen\") + geom_point(color = \"darkgreen\") +\n  labs(title = \"Average Silhouette Score\",\n       x = \"Number of Clusters (k)\",\n       y = \"Silhouette Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "blog/Project 7/hw4_questions.html#b.-latent-class-mnl",
    "href": "blog/Project 7/hw4_questions.html#b.-latent-class-mnl",
    "title": "Applied ML: Clustering, Choice, Classification & Drivers",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blog/Project 7/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "blog/Project 7/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Applied ML: Clustering, Choice, Classification & Drivers",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\n# Generate synthetic dataset\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nboundary &lt;- sin(4 * x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\n# Base plot\nggplot(dat, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  stat_function(fun = function(x) sin(4 * x) + x, \n                color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Synthetic Data for KNN\",\n       subtitle = \"Dashed line shows the true decision boundary\",\n       x = \"x1\", y = \"x2\", color = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\n\n# Generate test dataset with different seed\nset.seed(99)\nn_test &lt;- 100\nx1_test &lt;- runif(n_test, -3, 3)\nx2_test &lt;- runif(n_test, -3, 3)\nboundary_test &lt;- sin(4 * x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\ntest_data &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\n\nggplot(test_data, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  stat_function(fun = function(x) sin(4 * x) + x, \n                color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Test Data for KNN\",\n       subtitle = \"Dashed line shows the true decision boundary\",\n       x = \"x1\", y = \"x2\", color = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\n\n# Function to compute Euclidean distance between two points\neuclidean_distance &lt;- function(a, b) {\n  sqrt(sum((a - b)^2))\n}\n\n# Custom KNN function\nknn_custom &lt;- function(train_data, train_labels, test_data, k = 5) {\n  predictions &lt;- c()\n  \n  for (i in 1:nrow(test_data)) {\n    # Compute distances from test point to all training points\n    distances &lt;- apply(train_data, 1, function(row) euclidean_distance(row, test_data[i, ]))\n    \n    # Find the indices of the k nearest neighbors\n    neighbor_indices &lt;- order(distances)[1:k]\n    \n    # Get the labels of the k nearest neighbors\n    neighbor_labels &lt;- train_labels[neighbor_indices]\n    \n    # Predict the most common class\n    predicted_class &lt;- names(sort(table(neighbor_labels), decreasing = TRUE))[1]\n    \n    predictions &lt;- c(predictions, predicted_class)\n  }\n  \n  return(as.factor(predictions))\n}\n\n\n# Prepare training and test input\ntrain_X &lt;- dat[, c(\"x1\", \"x2\")]\ntrain_y &lt;- dat$y\n\ntest_X &lt;- test_data[, c(\"x1\", \"x2\")]\ntest_y &lt;- test_data$y\n\n# Predict using custom KNN\npred_custom &lt;- knn_custom(train_X, train_y, test_X, k = 5)\n\n# Accuracy\naccuracy_custom &lt;- mean(pred_custom == test_y)\npaste(\"Custom KNN accuracy:\", round(accuracy_custom * 100, 2), \"%\")\n\n[1] \"Custom KNN accuracy: 84 %\"\n\n\n\n# Load the class package\nlibrary(class)\n\n# Use built-in knn() function\npred_builtin &lt;- knn(train = train_X,\n                    test = test_X,\n                    cl = train_y,\n                    k = 5)\n\n# Accuracy comparison\naccuracy_builtin &lt;- mean(pred_builtin == test_y)\npaste(\"Built-in KNN accuracy:\", round(accuracy_builtin * 100, 2), \"%\")\n\n[1] \"Built-in KNN accuracy: 84 %\"\n\n\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?\n\n# Initialize vector to store accuracy results\naccuracy_vec &lt;- numeric(30)\n\n# Loop over k from 1 to 30\nfor (k in 1:30) {\n  pred_k &lt;- knn(train = train_X,\n                test = test_X,\n                cl = train_y,\n                k = k)\n  \n  accuracy_vec[k] &lt;- mean(pred_k == test_y)\n}\n\n# Create a data frame for plotting\naccuracy_df &lt;- data.frame(\n  k = 1:30,\n  accuracy = accuracy_vec\n)\n\n\nggplot(accuracy_df, aes(x = k, y = accuracy)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"red\") +\n  labs(title = \"KNN Classification Accuracy vs. k\",\n       x = \"Number of Neighbors (k)\",\n       y = \"Accuracy on Test Data\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/Project 7/hw4_questions.html#b.-key-drivers-analysis",
    "href": "blog/Project 7/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Applied ML: Clustering, Choice, Classification & Drivers",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  }
]