[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishikumar Mathiazhagan",
    "section": "",
    "text": "I’m currently pursuing my Master of Science in Business Analytics (MSBA) as part of the 2025 cohort at UC San Diego’s Rady School of Management. With a strong foundation in data analysis, Python, SQL, and data visualization tools like Tableau, I enjoy uncovering insights that drive smarter decisions. I’m also an avid sports enthusiast, always inspired by the intersection of data and performance—whether it’s in business or on the field."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html",
    "href": "blog/Project 3/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo better understand the demand side of charitable giving, Karlan and List used a natural field experiment involving over 50,000 previous donors to a politically liberal nonprofit organization. The letters were randomized to test three main dimensions: whether a matching grant was offered, the ratio of the match (1:1, 2:1, or 3:1), and the size of the matching fund ($25,000, $50,000, $100,000, or unstated). In addition, suggested donation amounts were varied using multipliers of the recipient’s previous highest contribution. Their findings revealed that simply offering a match significantly increased both the likelihood of donating and the average amount raised per letter. However, increasing the match ratio beyond 1:1 did not yield additional benefits, challenging conventional fundraising wisdom. Notably, the effect of matching offers was more pronounced in conservative-leaning (“red”) states, suggesting that political context may shape donor responsiveness.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#introduction",
    "href": "blog/Project 3/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo better understand the demand side of charitable giving, Karlan and List used a natural field experiment involving over 50,000 previous donors to a politically liberal nonprofit organization. The letters were randomized to test three main dimensions: whether a matching grant was offered, the ratio of the match (1:1, 2:1, or 3:1), and the size of the matching fund ($25,000, $50,000, $100,000, or unstated). In addition, suggested donation amounts were varied using multipliers of the recipient’s previous highest contribution. Their findings revealed that simply offering a match significantly increased both the likelihood of donating and the average amount raised per letter. However, increasing the match ratio beyond 1:1 did not yield additional benefits, challenging conventional fundraising wisdom. Notably, the effect of matching offers was more pronounced in conservative-leaning (“red”) states, suggesting that political context may shape donor responsiveness.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#data",
    "href": "blog/Project 3/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nlibrary(haven)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(summarytools)\n\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\nglimpse(data)\n\nRows: 50,083\nColumns: 51\n$ treatment          &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, …\n$ control            &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, …\n$ ratio              &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1,…\n$ ratio2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ ratio3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ size               &lt;dbl+lbl&gt; 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4,…\n$ size25             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ size50             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ size100            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sizeno             &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ask                &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1,…\n$ askd1              &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ askd2              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ askd3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ ask1               &lt;dbl&gt; 55, 25, 55, 55, 35, 95, 125, 75, 250, 150, 125, 25,…\n$ ask2               &lt;dbl&gt; 70, 35, 70, 70, 45, 120, 160, 95, 315, 190, 160, 35…\n$ ask3               &lt;dbl&gt; 85, 50, 85, 85, 55, 145, 190, 120, 375, 225, 190, 5…\n$ amount             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gave               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ amountchange       &lt;dbl&gt; -45, -25, -50, -25, -15, -45, -50, -65, -100, -125,…\n$ hpa                &lt;dbl&gt; 45, 25, 50, 50, 25, 90, 100, 65, 200, 125, 100, 5, …\n$ ltmedmra           &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ freq               &lt;dbl&gt; 2, 2, 3, 15, 42, 20, 12, 13, 28, 4, 1, 1, 2, 80, 3,…\n$ years              &lt;dbl&gt; 4, 3, 2, 8, 95, 10, 8, 16, 19, 7, 3, 1, 6, 19, 3, 1…\n$ year5              &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, …\n$ mrm2               &lt;dbl&gt; 31, 5, 6, 1, 24, 3, 4, 4, 6, 35, 41, 8, 28, 15, 5, …\n$ dormant            &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ female             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ couple             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state50one         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nonlit             &lt;dbl&gt; 5, 0, 3, 1, 1, 0, 0, 4, 1, 4, 4, 1, 1, 4, 0, 3, 6, …\n$ cases              &lt;dbl&gt; 4, 2, 1, 2, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 2, …\n$ statecnt           &lt;dbl&gt; 4.5002995, 2.9822462, 9.6070213, 3.2814682, 2.30201…\n$ stateresponse      &lt;dbl&gt; 0.01994681, 0.02608696, 0.02304817, 0.02066869, 0.0…\n$ stateresponset     &lt;dbl&gt; 0.019502353, 0.027833002, 0.022158911, 0.024702653,…\n$ stateresponsec     &lt;dbl&gt; 0.020806242, 0.022494888, 0.024743512, 0.012681159,…\n$ stateresponsetminc &lt;dbl&gt; -0.001303889, 0.005338114, -0.002584601, 0.01202149…\n$ perbush            &lt;dbl&gt; 0.4900000, 0.4646465, 0.4081633, 0.4646465, 0.52525…\n$ close25            &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ red0               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, …\n$ blue0              &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, …\n$ redcty             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ bluecty            &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ pwhite             &lt;dbl&gt; 0.4464934, NA, 0.9357064, 0.8883309, 0.7590141, 0.8…\n$ pblack             &lt;dbl&gt; 0.527769208, NA, 0.011948366, 0.010760401, 0.127420…\n$ page18_39          &lt;dbl&gt; 0.3175913, NA, 0.2761282, 0.2794118, 0.4423889, 0.3…\n$ ave_hh_sz          &lt;dbl&gt; 2.10, NA, 2.48, 2.65, 1.85, 2.92, 2.10, 2.47, 2.49,…\n$ median_hhincome    &lt;dbl&gt; 28517, NA, 51175, 79269, 40908, 61779, 54655, 14152…\n$ powner             &lt;dbl&gt; 0.4998072, NA, 0.7219406, 0.9204314, 0.4160721, 0.9…\n$ psch_atlstba       &lt;dbl&gt; 0.32452780, NA, 0.19266793, 0.41214216, 0.43996516,…\n$ pop_propurban      &lt;dbl&gt; 1.0000000, NA, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\ndfSummary(data)\n\nData Frame Summary  \ndata  \nDimensions: 50083 x 51  \nDuplicates: 30  \n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nNo   Variable             Label                                      Stats / Values                  Freqs (% of Valid)      Graph                  Valid      Missing  \n---- -------------------- ------------------------------------------ ------------------------------- ----------------------- ---------------------- ---------- ---------\n1    treatment            Treatment                                  Min  : 0                        0 : 16687 (33.3%)       IIIIII                 50083      0        \n     [numeric]                                                       Mean : 0.7                      1 : 33396 (66.7%)       IIIIIIIIIIIII          (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n2    control              Control                                    Min  : 0                        0 : 33396 (66.7%)       IIIIIIIIIIIII          50083      0        \n     [numeric]                                                       Mean : 0.3                      1 : 16687 (33.3%)       IIIIII                 (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n3    ratio                Match ratio                                Mean (sd) : 1.3 (1.2)           0 : 16687 (33.3%)       IIIIII                 50083      0        \n     [haven_labelled,                                                min &lt; med &lt; max:                1 : 11133 (22.2%)       IIII                   (100.0%)   (0.0%)   \n     vctrs_vctr,                                                     0 &lt; 1 &lt; 3                       2 : 11134 (22.2%)       IIII                                       \n     double]                                                         IQR (CV) : 2 (0.9)              3 : 11129 (22.2%)       IIII                                       \n\n4    ratio2               2:1 match ratio                            Min  : 0                        0 : 38949 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]                                                       Mean : 0.2                      1 : 11134 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n5    ratio3               3:1 match ratio                            Min  : 0                        0 : 38954 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]                                                       Mean : 0.2                      1 : 11129 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n6    size                 Match threshold                            1. [0] Control                  16687 (33.3%)           IIIIII                 50083      0        \n     [haven_labelled,                                                2. [1] $25,000                   8350 (16.7%)           III                    (100.0%)   (0.0%)   \n     vctrs_vctr,                                                     3. [2] $50,000                   8345 (16.7%)           III                                        \n     double]                                                         4. [3] $100,000                  8350 (16.7%)           III                                        \n                                                                     5. [4] Unstated                  8351 (16.7%)           III                                        \n\n7    size25               $25,000 match threshold                    Min  : 0                        0 : 41733 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8350 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n8    size50               $50,000 match threshold                    Min  : 0                        0 : 41738 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8345 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n9    size100              $100,000 match threshold                   Min  : 0                        0 : 41733 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8350 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n10   sizeno               Unstated match threshold                   Min  : 0                        0 : 41732 (83.3%)       IIIIIIIIIIIIIIII       50083      0        \n     [numeric]                                                       Mean : 0.2                      1 :  8351 (16.7%)       III                    (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n11   ask                  Suggested donation amount                  1. [0] Control                  16687 (33.3%)           IIIIII                 50083      0        \n     [haven_labelled,                                                2. [1] 1x                       11134 (22.2%)           IIII                   (100.0%)   (0.0%)   \n     vctrs_vctr,                                                     3. [2] 1.25x                    11133 (22.2%)           IIII                                       \n     double]                                                         4. [3] 1.50x                    11129 (22.2%)           IIII                                       \n\n12   askd1                Suggested donation was highest previous    Min  : 0                        0 : 38949 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]            contribution                               Mean : 0.2                      1 : 11134 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n13   askd2                Suggested donation was 1.25 x highest      Min  : 0                        0 : 38950 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]            previous contribution                      Mean : 0.2                      1 : 11133 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n14   askd3                Suggested donation was 1.50 x highest      Min  : 0                        0 : 38954 (77.8%)       IIIIIIIIIIIIIII        50083      0        \n     [numeric]            previous contribution                      Mean : 0.2                      1 : 11129 (22.2%)       IIII                   (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n15   ask1                 Highest previous contribution (for         Mean (sd) : 71.5 (101.7)        18 distinct values      :                      50083      0        \n     [numeric]            suggestion)                                min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     25 &lt; 45 &lt; 1500                                          :                                          \n                                                                     IQR (CV) : 30 (1.4)                                     :                                          \n                                                                                                                             :                                          \n\n16   ask2                 1.25 x highest previous contribution       Mean (sd) : 91.8 (127.3)        18 distinct values      :                      50083      0        \n     [numeric]            (for suggestion)                           min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     35 &lt; 60 &lt; 1875                                          :                                          \n                                                                     IQR (CV) : 40 (1.4)                                     :                                          \n                                                                                                                             :                                          \n\n17   ask3                 1.50 x highest previous contribution       Mean (sd) : 111 (151.7)         18 distinct values      :                      50083      0        \n     [numeric]            (for suggestion)                           min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     50 &lt; 70 &lt; 2250                                          :                                          \n                                                                     IQR (CV) : 45 (1.4)                                     :                                          \n                                                                                                                             :                                          \n\n18   amount               Dollars given                              Mean (sd) : 0.9 (8.7)           43 distinct values      :                      50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 400                                             :                                          \n                                                                     IQR (CV) : 0 (9.5)                                      :                                          \n                                                                                                                             :                                          \n\n19   gave                 Gave anything                              Min  : 0                        0 : 49049 (97.9%)       IIIIIIIIIIIIIIIIIII    50083      0        \n     [numeric]                                                       Mean : 0                        1 :  1034 ( 2.1%)                              (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n20   amountchange         Change in amount given                     Mean (sd) : -52.7 (1267.2)      240 distinct values                       :    50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                                          :    (100.0%)   (0.0%)   \n                                                                     -200412.1 &lt; -30 &lt; 275                                                     :                        \n                                                                     IQR (CV) : 25 (-24.1)                                                     :                        \n                                                                                                                                               :                        \n\n21   hpa                  Highest previous contribution              Mean (sd) : 59.4 (71.2)         243 distinct values     :                      50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 45 &lt; 1000                                           :                                          \n                                                                     IQR (CV) : 30 (1.2)                                     :                                          \n                                                                                                                             : .                                        \n\n22   ltmedmra             Small prior donor: last gift was less      Min  : 0                        0 : 25356 (50.6%)       IIIIIIIIII             50083      0        \n     [numeric]            than median $35                            Mean : 0.5                      1 : 24727 (49.4%)       IIIIIIIII              (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n23   freq                 Number of prior donations                  Mean (sd) : 8 (11.4)            144 distinct values     :                      50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 4 &lt; 218                                             :                                          \n                                                                     IQR (CV) : 8 (1.4)                                      :                                          \n                                                                                                                             : .                                        \n\n24   years                Number of years since initial donation     Mean (sd) : 6.1 (5.5)           22 distinct values      :                      50082      1        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 5 &lt; 95                                              :                                          \n                                                                     IQR (CV) : 7 (0.9)                                      : .                                        \n                                                                                                                             : :                                        \n\n25   year5                At least 5 years since initial donation    Min  : 0                        0 : 24600 (49.1%)       IIIIIIIII              50083      0        \n     [numeric]                                                       Mean : 0.5                      1 : 25483 (50.9%)       IIIIIIIIII             (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n26   mrm2                 Number of months since last donation       Mean (sd) : 13 (12.1)           67 distinct values      :                      50082      1        \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (100.0%)   (0.0%)   \n                                                                     0 &lt; 8 &lt; 168                                             :                                          \n                                                                     IQR (CV) : 15 (0.9)                                     : .                                        \n                                                                                                                             : : .                                      \n\n27   dormant              Already donated in 2005                    Min  : 0                        0 : 23866 (47.7%)       IIIIIIIII              50083      0        \n     [numeric]                                                       Mean : 0.5                      1 : 26217 (52.3%)       IIIIIIIIII             (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n28   female               Female                                     Min  : 0                        0 : 35374 (72.2%)       IIIIIIIIIIIIII         48972      1111     \n     [numeric]                                                       Mean : 0.3                      1 : 13598 (27.8%)       IIIII                  (97.8%)    (2.2%)   \n                                                                     Max  : 1                                                                                           \n\n29   couple               Couple                                     Min  : 0                        0 : 44438 (90.8%)       IIIIIIIIIIIIIIIIII     48935      1148     \n     [numeric]                                                       Mean : 0.1                      1 :  4497 ( 9.2%)       I                      (97.7%)    (2.3%)   \n                                                                     Max  : 1                                                                                           \n\n30   state50one           State tag: 1 for one observation of each   Min  : 0                        0 : 50033 (99.9%)       IIIIIIIIIIIIIIIIIII    50083      0        \n     [numeric]            of 50 states; 0 otherwise                  Mean : 0                        1 :    50 ( 0.1%)                              (100.0%)   (0.0%)   \n                                                                     Max  : 1                                                                                           \n\n31   nonlit               Nonlitigation                              Mean (sd) : 2.5 (2)             0 : 10873 (21.9%)       IIII                   49631      452      \n     [numeric]                                                       min &lt; med &lt; max:                1 : 11944 (24.1%)       IIII                   (99.1%)    (0.9%)   \n                                                                     0 &lt; 3 &lt; 6                       2 :  1163 ( 2.3%)                                                  \n                                                                     IQR (CV) : 3 (0.8)              3 :  5148 (10.4%)       II                                         \n                                                                                                     4 : 11557 (23.3%)       IIII                                       \n                                                                                                     5 :  6835 (13.8%)       II                                         \n                                                                                                     6 :  2111 ( 4.3%)                                                  \n\n32   cases                Court cases from state in 2004-5 in        Mean (sd) : 1.5 (1.2)           0 : 10682 (21.5%)       IIII                   49631      452      \n     [numeric]            which organization was involved            min &lt; med &lt; max:                1 : 17435 (35.1%)       IIIIIII                (99.1%)    (0.9%)   \n                                                                     0 &lt; 1 &lt; 4                       2 :  9796 (19.7%)       III                                        \n                                                                     IQR (CV) : 1 (0.8)              3 :  9464 (19.1%)       III                                        \n                                                                                                     4 :  2254 ( 4.5%)                                                  \n\n33   statecnt             Percent of sample from state               Mean (sd) : 6 (5.7)             57 distinct values        :                    50083      0        \n     [numeric]                                                       min &lt; med &lt; max:                                          : .                  (100.0%)   (0.0%)   \n                                                                     0 &lt; 3.5 &lt; 17.4                                          : : :             :                        \n                                                                     IQR (CV) : 7.8 (1)                                      : : :     .       :                        \n                                                                                                                             : : : :   :       :                        \n\n34   stateresponse        Proportion of sample from the state who    Mean (sd) : 0 (0)               48 distinct values          :                  50083      0        \n     [numeric]            gave                                       min &lt; med &lt; max:                                            :                  (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                                 :                                      \n                                                                     IQR (CV) : 0 (0.3)                                          :                                      \n                                                                                                                               . : :                                    \n\n35   stateresponset       Proportion of treated sample from the      Mean (sd) : 0 (0)               47 distinct values        :                    50083      0        \n     [numeric]            state who gave                             min &lt; med &lt; max:                                          :                    (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                               : .                                      \n                                                                     IQR (CV) : 0 (0.3)                                        : :                                      \n                                                                                                                               : :                                      \n\n36   stateresponsec       Proportion of control sample from the      Mean (sd) : 0 (0)               41 distinct values          . :                50080      3        \n     [numeric]            state who gave                             min &lt; med &lt; max:                                            : :                (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                                 : : .                                  \n                                                                     IQR (CV) : 0 (0.4)                                          : : :                                  \n                                                                                                                             :   : : : .                                \n\n37   stateresponsetminc   stateresponset - stateresponsec            Mean (sd) : 0 (0)               49 distinct values            :                50080      3        \n     [numeric]                                                       min &lt; med &lt; max:                                            . :                (100.0%)   (0.0%)   \n                                                                     0 &lt; 0 &lt; 0.1                                                 : :                                    \n                                                                     IQR (CV) : 0 (2.1)                                          : :                                    \n                                                                                                                                 : : .                                  \n\n38   perbush              State vote share for Bush                  Mean (sd) : 0.5 (0.1)           41 distinct values                :            50048      35       \n     [numeric]                                                       min &lt; med &lt; max:                                                  : :          (99.9%)    (0.1%)   \n                                                                     0.1 &lt; 0.5 &lt; 0.7                                                   : :                              \n                                                                     IQR (CV) : 0.1 (0.2)                                            : : : :                            \n                                                                                                                                     : : : : :                          \n\n39   close25              State vote share for Bush between 47.5%    Min  : 0                        0 : 40754 (81.4%)       IIIIIIIIIIIIIIII       50048      35       \n     [numeric]            and 52.5%                                  Mean : 0.2                      1 :  9294 (18.6%)       III                    (99.9%)    (0.1%)   \n                                                                     Max  : 1                                                                                           \n\n40   red0                 Red state                                  Min  : 0                        0 : 29806 (59.6%)       IIIIIIIIIII            50048      35       \n     [numeric]                                                       Mean : 0.4                      1 : 20242 (40.4%)       IIIIIIII               (99.9%)    (0.1%)   \n                                                                     Max  : 1                                                                                           \n\n41   blue0                Blue state                                 Min  : 0                        0 : 20242 (40.4%)       IIIIIIII               50048      35       \n     [numeric]                                                       Mean : 0.6                      1 : 29806 (59.6%)       IIIIIIIIIII            (99.9%)    (0.1%)   \n                                                                     Max  : 1                                                                                           \n\n42   redcty               Red county                                 Min  : 0                        0 : 24477 (49.0%)       IIIIIIIII              49978      105      \n     [numeric]                                                       Mean : 0.5                      1 : 25501 (51.0%)       IIIIIIIIII             (99.8%)    (0.2%)   \n                                                                     Max  : 1                                                                                           \n\n43   bluecty              Blue county                                Min  : 0                        0 : 25553 (51.1%)       IIIIIIIIII             49978      105      \n     [numeric]                                                       Mean : 0.5                      1 : 24425 (48.9%)       IIIIIIIII              (99.8%)    (0.2%)   \n                                                                     Max  : 1                                                                                           \n\n44   pwhite               Proportion white within zip code           Mean (sd) : 0.8 (0.2)           10729 distinct values                     :    48217      1866     \n     [numeric]                                                       min &lt; med &lt; max:                                                        . :    (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.9 &lt; 1                                                             : :                        \n                                                                     IQR (CV) : 0.2 (0.2)                                                  . : :                        \n                                                                                                                                     . . : : : :                        \n\n45   pblack               Proportion black within zip code           Mean (sd) : 0.1 (0.1)           10540 distinct values   :                      48047      2036     \n     [numeric]                                                       min &lt; med &lt; max:                                        :                      (95.9%)    (4.1%)   \n                                                                     0 &lt; 0 &lt; 1                                               :                                          \n                                                                     IQR (CV) : 0.1 (1.6)                                    :                                          \n                                                                                                                             : : .                                      \n\n46   page18_39            Proportion age 18-39 within zip code       Mean (sd) : 0.3 (0.1)           10788 distinct values       : .                48217      1866     \n     [numeric]                                                       min &lt; med &lt; max:                                            : :                (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.3 &lt; 1                                                 : :                                    \n                                                                     IQR (CV) : 0.1 (0.3)                                        : : .                                  \n                                                                                                                               : : : : .                                \n\n47   ave_hh_sz            Average household size within zip code     Mean (sd) : 2.4 (0.4)           296 distinct values             :              48221      1862     \n     [numeric]                                                       min &lt; med &lt; max:                                                :              (96.3%)    (3.7%)   \n                                                                     0 &lt; 2.4 &lt; 5.3                                                   : .                                \n                                                                     IQR (CV) : 0.5 (0.2)                                          . : :                                \n                                                                                                                                   : : :                                \n\n48   median_hhincome      Median household income within zip code    Mean (sd) : 54815.7 (22027.3)   9569 distinct values      : :                  48209      1874     \n     [numeric]                                                       min &lt; med &lt; max:                                          : :                  (96.3%)    (3.7%)   \n                                                                     5000 &lt; 50673 &lt; 200001                                     : : .                                    \n                                                                     IQR (CV) : 26824 (0.4)                                    : : :                                    \n                                                                                                                             . : : : : .                                \n\n49   powner               Proportion house owner within zip code     Mean (sd) : 0.7 (0.2)           10794 distinct values                 : .      48214      1869     \n     [numeric]                                                       min &lt; med &lt; max:                                                    . : :      (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.7 &lt; 1                                                       . : : :                          \n                                                                     IQR (CV) : 0.3 (0.3)                                          . . : : : : .                        \n                                                                                                                               . : : : : : : : :                        \n\n50   psch_atlstba         Proportion who finished college within     Mean (sd) : 0.4 (0.2)           10790 distinct values     . : :                48215      1868     \n     [numeric]            zip code                                   min &lt; med &lt; max:                                          : : : : .            (96.3%)    (3.7%)   \n                                                                     0 &lt; 0.4 &lt; 1                                               : : : : : .                              \n                                                                     IQR (CV) : 0.3 (0.5)                                      : : : : : : :                            \n                                                                                                                             . : : : : : : :                            \n\n51   pop_propurban        Proportion of population urban within      Mean (sd) : 0.9 (0.3)           5607 distinct values                      :    48217      1866     \n     [numeric]            zip code                                   min &lt; med &lt; max:                                                          :    (96.3%)    (3.7%)   \n                                                                     0 &lt; 1 &lt; 1                                                                 :                        \n                                                                     IQR (CV) : 0.1 (0.3)                                                      :                        \n                                                                                                                             .             . . :                        \n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n#todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load dataset\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Split data by treatment\ntreat &lt;- filter(data, treatment == 1)\ncontrol &lt;- filter(data, treatment == 0)\n\n# Variables to test\nvars &lt;- c(\"mrm2\", \"hpa\", \"freq\",\"page18_39\",\"red0\")\n\n# Function to report results\nfor (var in vars) {\n  cat(\"\\n==============================\\n\")\n  cat(\"Variable:\", var, \"\\n\")\n  \n  # Means and SDs\n  treat_mean &lt;- mean(treat[[var]], na.rm = TRUE)\n  control_mean &lt;- mean(control[[var]], na.rm = TRUE)\n  treat_sd &lt;- sd(treat[[var]], na.rm = TRUE)\n  control_sd &lt;- sd(control[[var]], na.rm = TRUE)\n  overall_mean &lt;- mean(data[[var]], na.rm = TRUE)\n  overall_sd &lt;- sd(data[[var]], na.rm = TRUE)\n  \n  cat(\"Treatment Mean:\", round(treat_mean, 3), \"SD:\", round(treat_sd, 3), \"\\n\")\n  cat(\"Control Mean:  \", round(control_mean, 3), \"SD:\", round(control_sd, 3), \"\\n\")\n  \n  # T-test\n  ttest &lt;- t.test(as.formula(paste(var, \"~ treatment\")), data = data)\n  cat(\"T-test p-value:\", round(ttest$p.value, 4), \"\\n\")\n  \n  # Regression\n  model &lt;- lm(as.formula(paste(var, \"~ treatment\")), data = data)\n  coef_summary &lt;- tidy(model) %&gt;% filter(term == \"treatment\")\n  cat(\"Regression Coefficient:\", round(coef_summary$estimate, 4), \"\\n\")\n  cat(\"Regression p-value:    \", round(coef_summary$p.value, 4), \"\\n\")\n}\n\n\n==============================\nVariable: mrm2 \nTreatment Mean: 13.012 SD: 12.086 \nControl Mean:   12.998 SD: 12.074 \nT-test p-value: 0.9049 \nRegression Coefficient: 0.0137 \nRegression p-value:     0.9049 \n\n==============================\nVariable: hpa \nTreatment Mean: 59.597 SD: 73.052 \nControl Mean:   58.96 SD: 67.269 \nT-test p-value: 0.3318 \nRegression Coefficient: 0.6371 \nRegression p-value:     0.3451 \n\n==============================\nVariable: freq \nTreatment Mean: 8.035 SD: 11.39 \nControl Mean:   8.047 SD: 11.404 \nT-test p-value: 0.9117 \nRegression Coefficient: -0.012 \nRegression p-value:     0.9117 \n\n==============================\nVariable: page18_39 \nTreatment Mean: 0.322 SD: 0.103 \nControl Mean:   0.322 SD: 0.103 \nT-test p-value: 0.9011 \nRegression Coefficient: -1e-04 \nRegression p-value:     0.901 \n\n==============================\nVariable: red0 \nTreatment Mean: 0.407 SD: 0.491 \nControl Mean:   0.399 SD: 0.49 \nT-test p-value: 0.0605 \nRegression Coefficient: 0.0087 \nRegression p-value:     0.0608 \n\n\nAll tested variables show very similar means and standard deviations across treatment and control groups, as expected under successful randomization. The t-test and regression p-values are nearly identical and all above 0.05, indicating no statistically significant imbalance. These results replicate Table 1 in the Karlan & List (2007) paper, which is included to show that randomization produced comparable groups on observable characteristics."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#experimental-results",
    "href": "blog/Project 3/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Calculate proportion who donated by treatment group\nresponse_rate &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(proportion_donated = mean(gave, na.rm = TRUE)) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\n# Plot\nggplot(response_rate, aes(x = group, y = proportion_donated, fill = group)) +\n  geom_col(width = 0.5) +\n  labs(\n    title = \"Proportion of People Who Donated\",\n    x = \"\",\n    y = \"Proportion\",\n    fill = \"\"\n  ) +\n  ylim(0, max(response_rate$proportion_donated) * 1.1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# T-test\nttest_gave &lt;- t.test(gave ~ treatment, data = data)\ncat(\"T-test result:\\n\")\n\nT-test result:\n\nprint(ttest_gave)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n# Regression\nmodel_gave &lt;- lm(gave ~ treatment, data = data)\nreg_summary &lt;- tidy(model_gave)\ncat(\"\\nLinear regression result:\\n\")\n\n\nLinear regression result:\n\nprint(reg_summary)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.0179    0.00110     16.2  4.78e-59\n2 treatment    0.00418   0.00135      3.10 1.93e- 3\n\n\nThe t-test comparing the proportion of individuals who made any charitable donation between the treatment and control groups shows a small but statistically significant increase in response rate in the treatment group. This confirms the finding in Table 2A, Panel A, where the response rate increases from 1.8% to 2.2% when matching is introduced.\nThe bivariate linear regression yields an equivalent result: the coefficient on treatment is positive and significant, indicating that simply offering a match increases the likelihood of donation.\nInterpretation: Even a small increase in donation probability — from just under 2% to just over 2% — is meaningful in a large fundraising campaign. This suggests that people are psychologically responsive to matching offers, even when the absolute value of the match is not very large. The perception of increased impact (“my $10 becomes $20”) may be enough to nudge more people into giving. This supports the idea that framing and context matter in charitable behavior, not just individual preferences.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(margins)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Probit regression: gave ~ treatment\nprobit_model &lt;- glm(gave ~ treatment, data = data, family = binomial(link = \"probit\"))\n\n# Summary of the model\nsummary(probit_model)\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n# Tidy output (for coefficient and SE)\ntidy(probit_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -2.10      0.0233    -90.1  0      \n2 treatment     0.0868    0.0279      3.11 0.00185\n\n# Optional: Marginal effects (not required for Table 3, but helpful)\nmargins::margins(probit_model)\n\nAverage marginal effects\n\n\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"),     data = data)\n\n\n treatment\n  0.004313\n\nmodel_full &lt;- glm(\n  gave ~ treatment * ratio2 +\n         treatment * ratio3 +\n         treatment * size25 +\n         treatment * size50 +\n         treatment * size100 +\n         treatment * askd2 +\n         treatment * askd3,\n  data = data,\n  family = binomial(link = \"probit\")\n)\n\nlibrary(broom)\ntidy(model_full)\n\n# A tibble: 16 × 5\n   term              estimate std.error statistic p.value\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)       -2.10       0.0233  -90.1      0    \n 2 treatment          0.0475     0.0496    0.958    0.338\n 3 ratio2             0.0364     0.0377    0.967    0.334\n 4 ratio3             0.0384     0.0376    1.02     0.308\n 5 size25            -0.0117     0.0434   -0.270    0.787\n 6 size50             0.00460    0.0431    0.107    0.915\n 7 size100           -0.00221    0.0432   -0.0512   0.959\n 8 askd2              0.0192     0.0377    0.509    0.611\n 9 askd3              0.0293     0.0375    0.782    0.434\n10 treatment:ratio2  NA         NA        NA       NA    \n11 treatment:ratio3  NA         NA        NA       NA    \n12 treatment:size25  NA         NA        NA       NA    \n13 treatment:size50  NA         NA        NA       NA    \n14 treatment:size100 NA         NA        NA       NA    \n15 treatment:askd2   NA         NA        NA       NA    \n16 treatment:askd3   NA         NA        NA       NA    \n\n\n\n\nInterpretation of Probit Regression Result\nThe probit regression produced a positive and statistically significant coefficient on the treatment variable, with an estimated effect of 0.0868, a z-value of 3.11, and a p-value below 0.01. This confirms that assignment to the treatment group — those who received a matching donation offer — was associated with a higher probability of making a donation.\nThe result is statistically significant at the 1% level, providing strong evidence that the offer of a match influenced donor behavior. While the numerical value of the coefficient may seem modest, its significance within a nonlinear probit framework reinforces the idea that behavioral prompts like matching gifts can effectively shape outcomes.\nIn essence, this supports the broader finding from Karlan and List (2007): even relatively small psychological nudges can meaningfully impact decision-making in a real-world charitable context.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nlibrary(haven)\nlibrary(dplyr)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Filter only the people who were in a match group (i.e., treatment == 1)\nmatched &lt;- filter(data, treatment == 1)\n\n# Create a variable for 1:1 match (implied if ratio2 and ratio3 are both 0)\nmatched &lt;- matched %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# T-test: 2:1 match vs 1:1 match\nt_21_vs_11 &lt;- t.test(gave ~ ratio2, data = filter(matched, ratio1 == 1 | ratio2 == 1))\n\n# T-test: 3:1 match vs 1:1 match\nt_31_vs_11 &lt;- t.test(gave ~ ratio3, data = filter(matched, ratio1 == 1 | ratio3 == 1))\n\n# Output results\ncat(\"T-test: 2:1 vs 1:1 match rate\\n\")\n\nT-test: 2:1 vs 1:1 match rate\n\nprint(t_21_vs_11)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio2\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 0 mean in group 1 \n     0.02074912      0.02263338 \n\ncat(\"\\nT-test: 3:1 vs 1:1 match rate\\n\")\n\n\nT-test: 3:1 vs 1:1 match rate\n\nprint(t_31_vs_11)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio3\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 0 mean in group 1 \n     0.02074912      0.02273340 \n\n\n\n\nEffectiveness of Different Match Ratios: T-Test Results\nTo evaluate whether more generous match ratios (2:1 and 3:1) increase the likelihood of donating compared to the standard 1:1 match, I conducted two t-tests:\n\n2:1 vs 1:1 match rate\n3:1 vs 1:1 match rate\n\nIn both cases, the t-tests found no statistically significant difference in the probability of donation. The p-value for the 2:1 vs 1:1 comparison was 0.3345, and for the 3:1 vs 1:1 comparison it was 0.3101 — both well above the conventional significance threshold of 0.05.\nThe estimated differences in means were very small, with confidence intervals that include zero, indicating no reliable effect of increasing the match ratio.\nThese results support the authors’ comment on page 8 of the paper that “larger match ratios had no additional impact.” In other words, donors respond to the presence of a match offer, but increasing the match from 1:1 to 2:1 or 3:1 does not lead to a higher response rate.\nThis finding reinforces a key insight from behavioral economics: perceived opportunity or framing (i.e., the existence of a match), rather than the size of the match, drives behavior in charitable giving contexts.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Subset only treatment group (those who received any match)\nmatched &lt;- filter(data, treatment == 1)\n\n# Create ratio1 manually: it's the group not in ratio2 or ratio3\nmatched &lt;- matched %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# Run regression with dummy variables (ratio1 is implicit baseline)\nmodel_ratios &lt;- lm(gave ~ ratio2 + ratio3, data = matched)\n\n# Output tidy summary\ntidy(model_ratios)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.0207    0.00139    14.9   3.98e-50\n2 ratio2       0.00188   0.00197     0.958 3.38e- 1\n3 ratio3       0.00198   0.00197     1.01  3.13e- 1\n\n\n\n\nRegression Analysis: Impact of Match Ratio on Donation Behavior\nTo assess whether the generosity of the match offer affects the likelihood of donating, I ran a linear regression using data from individuals who were in the treatment group. The model includes dummy variables for the 2:1 and 3:1 match offers, with the 1:1 match offer serving as the reference category.\nThe regression results show that: - The coefficient on ratio2 is 0.00188 with a standard error of 0.00197, and a p-value of 0.338. - The coefficient on ratio3 is 0.00198 with a standard error of 0.00197, and a p-value of 0.313.\nThese coefficients represent the difference in donation probability relative to the 1:1 match group. However, both coefficients are small and statistically insignificant, indicating that neither the 2:1 nor 3:1 match rate led to a meaningful increase in the likelihood of donating compared to a 1:1 match.\nThese findings are entirely consistent with earlier t-test results and support the authors’ conclusion that increasing the match ratio has no additional effect. The behavioral impact appears to stem from the presence of a match offer itself, rather than its magnitude. Donors may not be especially sensitive to whether the match is 1:1 or 3:1 — the idea of leverage seems to matter more than the amount.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Keep only treatment group\nmatched &lt;- filter(data, treatment == 1)\n\n# Create 1:1 dummy\nmatched &lt;- matched %&gt;%\n  mutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0))\n\n# ----- APPROACH 1: Direct from the data -----\n\n# Compute response rates by match group\nresponse_rates &lt;- matched %&gt;%\n  mutate(ratio_type = case_when(\n    ratio1 == 1 ~ \"1:1\",\n    ratio2 == 1 ~ \"2:1\",\n    ratio3 == 1 ~ \"3:1\"\n  )) %&gt;%\n  group_by(ratio_type) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE))\n\n# Extract values for comparison\nrate_1_1 &lt;- response_rates$response_rate[response_rates$ratio_type == \"1:1\"]\nrate_2_1 &lt;- response_rates$response_rate[response_rates$ratio_type == \"2:1\"]\nrate_3_1 &lt;- response_rates$response_rate[response_rates$ratio_type == \"3:1\"]\n\n# Calculate differences\ndiff_21_11 &lt;- rate_2_1 - rate_1_1\ndiff_31_21 &lt;- rate_3_1 - rate_2_1\n\n# ----- APPROACH 2: From regression coefficients -----\n\n# Run regression using 1:1 (ratio1) as the baseline\nmodel &lt;- lm(gave ~ ratio2 + ratio3, data = matched)\nmodel_out &lt;- tidy(model)\n\n# Coefficient difference (they are relative to 1:1 group)\ncoef_21_11 &lt;- model_out$estimate[model_out$term == \"ratio2\"]\ncoef_31_21 &lt;- model_out$estimate[model_out$term == \"ratio3\"] - coef_21_11\n\n# Output results\ncat(\"== Response Rate Differences from Raw Data ==\\n\")\n\n== Response Rate Differences from Raw Data ==\n\ncat(\"2:1 - 1:1 =\", round(diff_21_11, 5), \"\\n\")\n\n2:1 - 1:1 = 0.00188 \n\ncat(\"3:1 - 2:1 =\", round(diff_31_21, 5), \"\\n\")\n\n3:1 - 2:1 = 1e-04 \n\ncat(\"\\n== Differences in Fitted Coefficients ==\\n\")\n\n\n== Differences in Fitted Coefficients ==\n\ncat(\"2:1 vs 1:1 =\", round(coef_21_11, 5), \"\\n\")\n\n2:1 vs 1:1 = 0.00188 \n\ncat(\"3:1 vs 2:1 =\", round(coef_31_21, 5), \"\\n\")\n\n3:1 vs 2:1 = 1e-04 \n\n\n\n\nMatch Ratio Differences: Response Rate and Regression Analysis\nTo understand whether higher match ratios increase the likelihood of donation, I calculated response rate differences in two ways:\n\n1. Differences in Raw Response Rates\n\n2:1 vs 1:1 match ratio: 0.00188\n3:1 vs 2:1 match ratio: 0.00010\n\n\n\n2. Differences in Fitted Coefficients (Linear Model)\n\n2:1 vs 1:1: 0.00188\n3:1 vs 2:1: 0.00010\n\nBoth approaches yielded the same differences, confirming that the estimated impact of increasing the match ratio is extremely small — around 0.1 to 0.2 percentage points.\n\n\n\nConclusion\nThese findings provide strong evidence that increasing the match ratio from 1:1 to 2:1 or from 2:1 to 3:1 does not significantly increase the probability that someone donates.\nThis aligns with the authors’ conclusion: the presence of a match offer, rather than its generosity, is what motivates donations. Donors respond to the idea of a match, but larger matches do not provide additional behavioral lift.\nIn sum, the marginal effectiveness of more generous matches is negligible — a key insight for fundraisers seeking efficient campaign strategies.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# T-test: Compare average donation amount\nt_test_amount &lt;- t.test(amount ~ treatment, data = data)\ncat(\"== T-test: Donation Amount by Treatment Status ==\\n\")\n\n== T-test: Donation Amount by Treatment Status ==\n\nprint(t_test_amount)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = -1.9183, df = 36216, p-value = 0.05509\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.310555423  0.003344493\nsample estimates:\nmean in group 0 mean in group 1 \n      0.8132678       0.9668733 \n\n# Regression: Amount ~ Treatment\nmodel_amount &lt;- lm(amount ~ treatment, data = data)\ncat(\"\\n== Linear Regression: Amount ~ Treatment ==\\n\")\n\n\n== Linear Regression: Amount ~ Treatment ==\n\ntidy(model_amount)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.813    0.0674     12.1  1.84e-33\n2 treatment      0.154    0.0826      1.86 6.28e- 2\n\n\n\n\nEffect of Match Offers on Donation Amounts\nTo assess whether being offered a matching donation opportunity affects the amount individuals donate, I conducted both a t-test and a bivariate linear regression using the full dataset.\nThe t-test comparing average donation amounts between treatment and control groups yielded a p-value of 0.055, which is just above the traditional 5% significance threshold. The average donation was $0.81 in the control group and $0.97 in the treatment group — a difference of approximately $0.15.\nThe linear regression of donation amount on treatment status produced a coefficient of 0.154 with a p-value of 0.0628, again marginally above the 5% threshold. This suggests a small, positive effect of treatment on donation amount, but not strong enough to be deemed statistically significant at conventional levels.\n\n\nInterpretation\nThese results indicate that being offered a match may slightly increase the average amount donated, but the evidence is not strong enough to make a confident causal claim. The p-values suggest that the observed difference might be due to chance, though the direction of the effect (positive) is consistent with the broader narrative of the study.\nIn summary, while matching offers clearly increase the likelihood that someone donates, the evidence that they increase the size of the donation (unconditionally) is suggestive but not definitive.\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(broom)\n\n# Load the data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Filter to donors only (those who gave something)\ndonors_only &lt;- filter(data, gave == 1)\n\n# T-test: Compare average amount given among donors\nt_test_donors &lt;- t.test(amount ~ treatment, data = donors_only)\ncat(\"== T-test: Donation Amount by Treatment Status (Donors Only) ==\\n\")\n\n== T-test: Donation Amount by Treatment Status (Donors Only) ==\n\nprint(t_test_donors)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = 0.58461, df = 557.46, p-value = 0.559\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -3.937240  7.274027\nsample estimates:\nmean in group 0 mean in group 1 \n       45.54027        43.87188 \n\n# Regression: Amount ~ Treatment (among donors)\nmodel_donors &lt;- lm(amount ~ treatment, data = donors_only)\ncat(\"\\n== Linear Regression: Amount ~ Treatment (Donors Only) ==\\n\")\n\n\n== Linear Regression: Amount ~ Treatment (Donors Only) ==\n\ntidy(model_donors)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    45.5       2.42    18.8   5.47e-68\n2 treatment      -1.67      2.87    -0.581 5.61e- 1\n\n\n\n\nConditional Effect of Treatment on Donation Amount (Among Donors Only)\nTo examine whether match offers influenced how much people donated — conditional on having donated — I restricted the dataset to only those individuals who gave a positive amount.\n\nT-Test Results\nThe average donation among the control group was $45.54, while the average in the treatment group was $43.87. The t-test revealed no statistically significant difference between the two groups (p = 0.559). The 95% confidence interval for the difference in means includes zero, reinforcing this conclusion.\n\n\nRegression Results\nThe linear regression of donation amount on treatment (among donors only) produced a coefficient of -1.67, indicating that those in the treatment group donated slightly less on average. However, the effect is not statistically significant (p = 0.561).\n\n\nInterpretation and Causal Insights\nThese results suggest that being offered a match does not influence the size of the donation, once someone has decided to donate. In fact, if anything, treated individuals gave slightly less, though the difference is not meaningful.\nImportantly, this analysis is conditional on donation behavior, which was itself affected by the treatment. Because we are conditioning on a post-treatment variable (gave), the regression coefficient does not have a causal interpretation. The group of donors in the treatment group may differ in unobservable ways from donors in the control group, and this could bias the estimate.\nIn summary, this analysis tells us that the treatment affects the extensive margin (whether people give), not the intensive margin (how much they give) — a conclusion that aligns with the broader findings of Karlan and List (2007).\n\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load data\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Filter to donors only\ndonors &lt;- filter(data, gave == 1)\n\n# Calculate group means\ngroup_means &lt;- donors %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(mean_amount = mean(amount, na.rm = TRUE))\n\n# Plot for Treatment Group\nggplot(filter(donors, treatment == 1), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = group_means$mean_amount[group_means$treatment == 1],\n             color = \"red\", size = 1) +\n  labs(\n    title = \"Donation Amounts (Treatment Group)\",\n    x = \"Amount Donated\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n# Plot for Control Group\nggplot(filter(donors, treatment == 0), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"lightgreen\", color = \"white\") +\n  geom_vline(xintercept = group_means$mean_amount[group_means$treatment == 0],\n             color = \"red\", size = 1) +\n  labs(\n    title = \"Donation Amounts (Control Group)\",\n    x = \"Amount Donated\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDonation Amount Distributions: Treatment vs. Control (Donors Only)\nThe two histograms above show the distribution of donation amounts among individuals who made a donation, separated by treatment assignment. The red vertical line in each plot marks the average donation amount within that group.\n\nIn the treatment group, the distribution of donations is right-skewed, with most donations falling between $10 and $100. The average donation is just below $44.\nIn the control group, the pattern is similar, though the average donation is slightly higher, around $45.5.\n\nVisually, the distributions are quite similar. Both groups show large clustering at common donation levels (e.g., $25, $50, $100), suggesting donors may be responding to standard ask amounts or psychological anchors.\n\n\nInterpretation\nThese plots support the earlier statistical analysis: being offered a match does not appear to increase the average amount donated among those who give. In fact, the average donation is marginally lower in the treatment group. However, this difference is not statistically significant, and the shapes of the two distributions are nearly identical.\nThis visualization reinforces the paper’s broader conclusion that matching gifts increase the likelihood of giving, but not the size of the donation once that decision has been made. The behavioral nudge seems to influence the extensive margin (give vs. not give), not the intensive margin (how much to give)."
  },
  {
    "objectID": "blog/Project 3/hw1_questions.html#simulation-experiment",
    "href": "blog/Project 3/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate 100,000 control group donations (mean = 0.813, sd = 10)\ncontrol &lt;- rnorm(100000, mean = 0.813, sd = 10)\n\n# Simulate 10,000 treatment group donations (mean = 0.966, sd = 10)\ntreatment &lt;- rnorm(10000, mean = 0.966, sd = 10)\n\n# Calculate 10,000 differences (each treatment value - sampled control value)\ndiffs &lt;- treatment - sample(control, size = 10000)\n\n# Compute cumulative average\ncum_avg &lt;- cumsum(diffs) / seq_along(diffs)\n\n# Create data for plotting\nplot_data &lt;- data.frame(\n  x = 1:10000,\n  cum_avg = cum_avg\n)\n\n# Plot the convergence\nlibrary(ggplot2)\nggplot(plot_data, aes(x = x, y = cum_avg)) +\n  geom_line(linewidth = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = mean(treatment) - mean(control), color = \"red\", linetype = \"dashed\") +\n  coord_cartesian(ylim = c(-1, 1)) +  # focus the Y-axis for cleaner view\n  labs(\n    title = \"Cumulative Average of Treatment-Control Differences\",\n    subtitle = \"Simulating the Law of Large Numbers\",\n    x = \"Sample Size\",\n    y = \"Cumulative Average Difference\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSimulating the Law of Large Numbers\nThe plot above shows the cumulative average of 10,000 differences between randomly drawn treatment and control values. Specifically, we simulate 100,000 values from the control distribution and 10,000 from the treatment distribution, then compute the difference in means one pair at a time. The blue line shows the evolving average of those differences as the sample size increases.\nAt the beginning of the plot, we see high variability — the cumulative average fluctuates wildly with the first few draws. This reflects the influence of random noise when the sample size is small. However, as the number of observations increases, the cumulative average begins to stabilize, eventually converging toward the red dashed line, which represents the true difference in means between the treatment and control groups.\nThis behavior is a textbook demonstration of the Law of Large Numbers: as sample size increases, the sample average becomes a more accurate estimate of the population average. In practical terms, it tells us that while individual comparisons may be noisy, large samples can reveal reliable patterns. Here, the cumulative average confirms the treatment group tends to donate more than the control group — and that this effect emerges clearly as sample size grows.\n\n\nCentral Limit Theorem\n\nset.seed(123)\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Simulate sampling distribution of differences\nsimulate_diff_means &lt;- function(n, reps = 1000, mu_c = 0.813, mu_t = 0.966, sd = 10) {\n  diffs &lt;- replicate(reps, {\n    mean(rnorm(n, mean = mu_t, sd = sd)) - mean(rnorm(n, mean = mu_c, sd = sd))\n  })\n  data.frame(diff = diffs, sample_size = paste0(\"n = \", n))\n}\n\n# Generate datasets\nsizes &lt;- c(50, 200, 500, 1000)\nsim_data &lt;- do.call(rbind, lapply(sizes, simulate_diff_means))\n\n# Plot histograms\nggplot(sim_data, aes(x = diff)) +\n  geom_histogram(bins = 40, fill = \"steelblue\", color = \"white\") +\n  facet_wrap(~sample_size, scales = \"free\", ncol = 2) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Sampling Distribution of Differences in Means\",\n    subtitle = \"Simulated at Varying Sample Sizes\",\n    x = \"Difference in Means (Treatment - Control)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSampling Distributions at Different Sample Sizes\nThe four histograms above show the sampling distribution of the difference in means between treatment and control groups at sample sizes of 50, 200, 500, and 1000. Each plot represents 1,000 simulations where, for each simulation, we draw n observations from each group, compute their sample means, and record the difference.\n\nAt n = 50, the distribution is wide and noisy, with a substantial spread. In this case, zero is near the center of the distribution, meaning we would often observe a difference close to zero just by chance.\nAt n = 200, the distribution is more concentrated, though still displays some variability. Zero is still within the range of plausible outcomes, but less frequently near the center.\nAt n = 500, the sampling distribution narrows significantly, and zero starts to move toward the edge of the distribution — suggesting stronger evidence of a real difference.\nAt n = 1000, the distribution is tight and centered around the true mean difference. Zero is now well into the tails, meaning it’s unlikely we would observe a difference this close to zero if the treatment and control distributions were truly the same.\n\n\n\nWhat We Learn\nThese plots visually demonstrate the power of larger sample sizes. As the sample size increases: - The sampling variability decreases - The sampling distribution becomes more precise - We are better able to detect small true differences in population means\nThis is a direct illustration of the Central Limit Theorem and the Law of Large Numbers in action. At larger sample sizes, we can distinguish signal from noise more effectively — and zero becomes an unlikely outcome when a real treatment effect exists."
  },
  {
    "objectID": "blog/Project 1/index.html",
    "href": "blog/Project 1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n\nlibrary(tidyverse)\n\nmtcars |&gt;\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()"
  },
  {
    "objectID": "blog/Project 1/index.html#section-1-data",
    "href": "blog/Project 1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/Project 1/index.html#section-2-analysis",
    "href": "blog/Project 1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n\nlibrary(tidyverse)\n\nmtcars |&gt;\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting GLM Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nRishikumar Mathiazhagan\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nRishikumar Mathiazhagan\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Project 4/inclass.html",
    "href": "blog/Project 4/inclass.html",
    "title": "Fitting GLM Regression",
    "section": "",
    "text": "library(readr)\n\n# Read the CSV file\ndata &lt;- read_csv(\"purchase.csv\")\n\nRows: 2000 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): idx, purchase\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View the first few rows\nhead(data)\n\n# A tibble: 6 × 2\n    idx purchase\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  50          0\n2  75          0\n3  28          0\n4  76          0\n5  50          0\n6   8.9        0\n\n\n\n# Load data\ndata &lt;- read.csv(\"purchase.csv\")\n\n# Fit a GLM: replace 'response' and 'predictor' with actual column names\nmodel &lt;- glm(purchase ~ idx, data = data, family = binomial(link = \"logit\"))\n\n# Summary of model\nsummary(model)\n\n\nCall:\nglm(formula = purchase ~ idx, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.219622   0.159587  -20.18   &lt;2e-16 ***\nidx          0.032527   0.002919   11.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1669.9  on 1999  degrees of freedom\nResidual deviance: 1538.6  on 1998  degrees of freedom\nAIC: 1542.6\n\nNumber of Fisher Scoring iterations: 5\n\n# Confidence interval for coefficients\nconfint(model)\n\nWaiting for profiling to be done...\n\n\n                  2.5 %      97.5 %\n(Intercept) -3.53919601 -2.91328111\nidx          0.02685243  0.03830481\n\n\n\n# Load data\ndata &lt;- read.csv(\"purchase.csv\")\nx &lt;- data$idx\ny &lt;- data$purchase  # should be 0/1\n\n# Define the negative log-likelihood for logistic regression\nneg_log_likelihood_logit &lt;- function(par, x, y) {\n  beta0 &lt;- par[1]\n  beta1 &lt;- par[2]\n  eta &lt;- beta0 + beta1 * x\n  p &lt;- 1 / (1 + exp(-eta))  # logistic link\n  -sum(y * log(p) + (1 - y) * log(1 - p))  # negative log-likelihood\n}\n\n# Initial values\nstart_par &lt;- c(0, 0)\n\n# Optimize\nfit &lt;- optim(par = start_par,\n             fn = neg_log_likelihood_logit,\n             x = x, y = y,\n             hessian = TRUE,\n             method = \"BFGS\")\n\n# Extract estimates and SEs\nestimates &lt;- fit$par\nhessian &lt;- fit$hessian\ncov_matrix &lt;- solve(hessian)\nse &lt;- sqrt(diag(cov_matrix))\n\n# CI for slope (parameter 2)\nz &lt;- qnorm(0.975)\nci_slope &lt;- c(\n  estimates[2] - z * se[2],\n  estimates[2] + z * se[2]\n)\n\nlist(Estimate = estimates[2], SE = se[2], CI = ci_slope)\n\n$Estimate\n[1] 0.03250968\n\n$SE\n[1] 0.002920978\n\n$CI\n[1] 0.02678467 0.03823469\n\n\n\n# Load data\ndata &lt;- read.csv(\"purchase.csv\")\nx &lt;- data$idx\ny &lt;- data$purchase\ndf &lt;- data.frame(x = x, y = y)\n\nB &lt;- 1000\nslope_estimates &lt;- numeric(B)\nset.seed(123)\n\nfor (i in 1:B) {\n  sample_indices &lt;- sample(1:nrow(df), replace = TRUE)\n  sample_data &lt;- df[sample_indices, ]\n  fit &lt;- glm(y ~ x, data = sample_data, family = binomial(link = \"logit\"))\n  slope_estimates[i] &lt;- coef(fit)[\"x\"]\n}\n\n# 1. CI using standard deviation approach\nslope_mean &lt;- mean(slope_estimates)\nslope_sd &lt;- sd(slope_estimates)\nz &lt;- qnorm(0.975)\nci_sd &lt;- c(slope_mean - z * slope_sd, slope_mean + z * slope_sd)\n\n# 2. CI using quantile approach\nci_quantile &lt;- quantile(slope_estimates, probs = c(0.025, 0.975))\n\nlist(CI_SD = ci_sd, CI_Quantile = ci_quantile)\n\n$CI_SD\n[1] 0.02689833 0.03843410\n\n$CI_Quantile\n      2.5%      97.5% \n0.02705333 0.03848676"
  },
  {
    "objectID": "blog/Project 5/hw2_questions.html",
    "href": "blog/Project 5/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n#_todo: Read in data._\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read in the Blueprinty dataset\nblueprinty_data &lt;- read_csv(\"blueprinty.csv\")\n\n# Preview the first few rows\nhead(blueprinty_data)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\nlibrary(tidyverse)\n\n# Histogram of patents by customer status\nggplot(blueprinty_data, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Patents by Customer Status\",\n       x = \"Number of Patents (Last 5 Years)\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean patents by customer status\nblueprinty_data %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  iscustomer mean_patents\n       &lt;dbl&gt;        &lt;dbl&gt;\n1          0         3.47\n2          1         4.13\n\n\nThe histogram shows that both Blueprinty customers and non-customers are most frequently awarded between 2 to 6 patents over the last 5 years. However, customers tend to have a rightward shift in the distribution—meaning they are more likely to fall into higher patent count bins. This is consistent with the summary statistics, which show that the average number of patents among non-customers is 3.47, while for customers it is 4.13.\nThis suggests that, on average, firms using Blueprinty’s software are granted more patents. However, this relationship is purely descriptive at this stage and does not account for potential confounders such as firm age or geographic location. Further modeling is needed to isolate the effect of software usage.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nggplot(blueprinty_data, aes(x = age, fill = as.factor(iscustomer))) +\n  geom_histogram(alpha = 0.6, position = \"identity\", binwidth = 2, color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Firm Age by Customer Status\",\n       x = \"Years Since Incorporation\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Proportional bar plot of region by customer status\nggplot(blueprinty_data, aes(x = region, fill = as.factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Regional Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the histogram of firm age, we see that Blueprinty customers tend to be slightly older on average than non-customers. While both groups are centered around 25–30 years since incorporation, the distribution for customers appears more spread out into the higher-age range (35–45 years), whereas non-customers are more concentrated around the mean. This suggests that firm age could be a confounding factor in our analysis and may need to be controlled for when estimating the effect of using Blueprinty software.\nThe regional bar chart shows that Blueprinty’s customer base is not evenly distributed across regions. Notably, a majority of firms in the Northeast region are customers, while other regions like the Midwest, Northwest, South, and Southwest have lower adoption rates. This uneven distribution suggests that regional effects—such as varying patenting activity or industry concentrations—might influence both Blueprinty adoption and patent success, and should be accounted for in any causal analysis.\n\n\n\n\n\n\n\n\n\nEstimation of Simple Poisson Model\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  }\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Apply it to the observed patent counts\nY_vals &lt;- blueprinty_data$patents\n\n# Evaluate the log-likelihood at the sample mean as a starting point\npoisson_loglikelihood(lambda = mean(Y_vals), Y = Y_vals)\n\n[1] -3367.684\n\n\nThe Poisson log-likelihood curve rises steeply for small values of 𝜆 λ, reaches a peak, and then gradually declines. This confirms the log-likelihood function is unimodal, meaning it has a single maximum—ideal for maximum likelihood estimation. The red dashed line indicates the value of 𝜆 λ that maximizes the log-likelihood, representing the maximum likelihood estimate (MLE). This value closely matches the sample mean of the observed patent counts, which is consistent with the theoretical property that the MLE of 𝜆 λ in a simple Poisson model equals the sample mean.\n\n# Define a sequence of lambda values to evaluate\nlambda_vals &lt;- seq(0.5, 8, by = 0.1)\n\n# Calculate log-likelihood for each lambda\nlogliks &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y_vals))\n\n# Create a data frame for plotting\nloglik_df &lt;- data.frame(lambda = lambda_vals, loglik = logliks)\n\n# Plot\nggplot(loglik_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#0072B2\", size = 1) +\n  geom_vline(xintercept = lambda_vals[which.max(logliks)], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Poisson Log-Likelihood Curve\",\n       x = expression(lambda),\n       y = \"Log-Likelihood\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nWe begin with the log-likelihood function for independent observations ( Y_1, , Y_n () ):\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nThis simplifies to:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log(\\lambda) + \\text{const}\n\\]\nNow, take the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolving for ( ) gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator of ( ) is the sample mean of the observed data.\n\n# Define the negative log-likelihood to minimize\nneg_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(Inf)  # invalid values return high cost\n  }\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# Use optim() to minimize the negative log-likelihood\noptim_result &lt;- optim(par = 1,  # initial guess for lambda\n                      fn = neg_loglikelihood,\n                      Y = Y_vals,\n                      method = \"Brent\",\n                      lower = 0.01,\n                      upper = 10)\n\n# Extract MLE\nlambda_hat_optim &lt;- optim_result$par\nlambda_hat_optim\n\n[1] 3.684667\n\n\nTo estimate the Poisson rate parameter 𝜆 λ, we used R’s optim() function to numerically maximize the log-likelihood. Since optim() minimizes by default, we passed it the negative of our log-likelihood function. Using Brent’s method with bounds between 0.01 and 10, the optimizer returned a value of 𝜆 λ that aligns closely with the sample mean of the data. This confirms both our analytical derivation and visual inspection from the log-likelihood plot.\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Define the log-likelihood function for Poisson regression\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Compute lambda_i for all observations\n  lambda &lt;- exp(X %*% beta)\n  \n  # Compute the log-likelihood\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n\n# Create model matrix (design matrix) with intercept, age, age^2, region dummies, and iscustomer\nblueprinty_data &lt;- blueprinty_data %&gt;%\n  mutate(age_sq = age^2)\n\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty_data)\nY &lt;- blueprinty_data$patents\n\n\n# Use optim to estimate beta\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)\n  -sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Run optimization\noptim_result &lt;- optim(par = rep(0, ncol(X)),\n                      fn = poisson_regression_loglikelihood,\n                      Y = Y, X = X,\n                      hessian = TRUE,\n                      method = \"BFGS\",\n                      control = list(reltol = 1e-10))\n\n# Extract coefficients\nbeta_hat &lt;- optim_result$par\n\n# Compute standard errors using the inverse of the Hessian\nhessian &lt;- optim_result$hessian\nvar_cov_matrix &lt;- solve(hessian)\nstd_errors &lt;- sqrt(diag(var_cov_matrix))\n\n\n# Summarize coefficients and standard errors in a table\ncoef_table &lt;- data.frame(\n  Term = colnames(X),\n  Estimate = round(beta_hat, 4),\n  Std_Error = round(std_errors, 4)\n)\n\nknitr::kable(coef_table, caption = \"Poisson Regression Estimates and Standard Errors\")\n\n\nPoisson Regression Estimates and Standard Errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nI(age^2)\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\n# Fit the Poisson regression model using glm()\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, \n                 data = blueprinty_data, \n                 family = poisson())\n\n# Display a summary\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty_data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe results from both the optim() function and the glm() function are generally consistent, which confirms that the manual maximum likelihood estimation was implemented correctly. The coefficient for the iscustomer variable is positive and statistically significant in the glm() output (estimate = 0.208, p &lt; 0.001). This means that, holding other variables constant, firms that use Blueprinty’s software tend to receive more patents than non-customers. Since the model uses a log link function, this coefficient translates to approximately a 23% increase in the expected number of patents (because exp(0.208) is about 1.23).\nThe firm’s age also plays a significant role. The positive coefficient on age and the negative coefficient on age squared suggest a concave relationship, where patent activity increases with age up to a certain point and then declines. This is typical in lifecycle patterns of firm innovation.\nThe region variables are not statistically significant, which implies that, after accounting for age and customer status, there are no strong regional differences in patent outcomes.\nIn summary, the glm() results confirm the manual estimation and support the conclusion that using Blueprinty’s software is associated with a higher number of patents, even after adjusting for firm age and location.\n\n# Make copies of the data\ndata_0 &lt;- blueprinty_data\ndata_1 &lt;- blueprinty_data\n\n# Set customer status to 0 and 1 respectively\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\n# Use the fitted glm model to predict expected patent counts\ny_pred_0 &lt;- predict(glm_model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = data_1, type = \"response\")\n\n# Compute the average effect\naverage_difference &lt;- mean(y_pred_1 - y_pred_0)\naverage_difference\n\n[1] 0.7927681\n\n\nUsing the fitted Poisson regression model, we estimated the expected number of patents for each firm under two hypothetical scenarios: one where no firms use Blueprinty’s software and one where all firms do. The average difference between these two sets of predictions is approximately 0.79 patents per firm over five years. This suggests that, controlling for age and region, firms that use Blueprinty’s software are expected to receive nearly one additional patent on average compared to similar firms that do not use the software. This result provides strong evidence that Blueprinty’s tool may be associated with a meaningful improvement in patenting success."
  },
  {
    "objectID": "blog/Project 5/hw2_questions.html#blueprinty-case-study",
    "href": "blog/Project 5/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n#_todo: Read in data._\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read in the Blueprinty dataset\nblueprinty_data &lt;- read_csv(\"blueprinty.csv\")\n\n# Preview the first few rows\nhead(blueprinty_data)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\nlibrary(tidyverse)\n\n# Histogram of patents by customer status\nggplot(blueprinty_data, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Patents by Customer Status\",\n       x = \"Number of Patents (Last 5 Years)\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Mean patents by customer status\nblueprinty_data %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents), .groups = \"drop\")\n\n# A tibble: 2 × 2\n  iscustomer mean_patents\n       &lt;dbl&gt;        &lt;dbl&gt;\n1          0         3.47\n2          1         4.13\n\n\nThe histogram shows that both Blueprinty customers and non-customers are most frequently awarded between 2 to 6 patents over the last 5 years. However, customers tend to have a rightward shift in the distribution—meaning they are more likely to fall into higher patent count bins. This is consistent with the summary statistics, which show that the average number of patents among non-customers is 3.47, while for customers it is 4.13.\nThis suggests that, on average, firms using Blueprinty’s software are granted more patents. However, this relationship is purely descriptive at this stage and does not account for potential confounders such as firm age or geographic location. Further modeling is needed to isolate the effect of software usage.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nggplot(blueprinty_data, aes(x = age, fill = as.factor(iscustomer))) +\n  geom_histogram(alpha = 0.6, position = \"identity\", binwidth = 2, color = \"black\") +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Distribution of Firm Age by Customer Status\",\n       x = \"Years Since Incorporation\",\n       y = \"Number of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Proportional bar plot of region by customer status\nggplot(blueprinty_data, aes(x = region, fill = as.factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_fill_manual(values = c(\"#999999\", \"#0072B2\"),\n                    name = \"Customer Status\",\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Regional Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion of Firms\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the histogram of firm age, we see that Blueprinty customers tend to be slightly older on average than non-customers. While both groups are centered around 25–30 years since incorporation, the distribution for customers appears more spread out into the higher-age range (35–45 years), whereas non-customers are more concentrated around the mean. This suggests that firm age could be a confounding factor in our analysis and may need to be controlled for when estimating the effect of using Blueprinty software.\nThe regional bar chart shows that Blueprinty’s customer base is not evenly distributed across regions. Notably, a majority of firms in the Northeast region are customers, while other regions like the Midwest, Northwest, South, and Southwest have lower adoption rates. This uneven distribution suggests that regional effects—such as varying patenting activity or industry concentrations—might influence both Blueprinty adoption and patent success, and should be accounted for in any causal analysis.\n\n\n\n\n\n\n\n\n\nEstimation of Simple Poisson Model\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(-Inf)  # log-likelihood is undefined for non-positive lambda\n  }\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Apply it to the observed patent counts\nY_vals &lt;- blueprinty_data$patents\n\n# Evaluate the log-likelihood at the sample mean as a starting point\npoisson_loglikelihood(lambda = mean(Y_vals), Y = Y_vals)\n\n[1] -3367.684\n\n\nThe Poisson log-likelihood curve rises steeply for small values of 𝜆 λ, reaches a peak, and then gradually declines. This confirms the log-likelihood function is unimodal, meaning it has a single maximum—ideal for maximum likelihood estimation. The red dashed line indicates the value of 𝜆 λ that maximizes the log-likelihood, representing the maximum likelihood estimate (MLE). This value closely matches the sample mean of the observed patent counts, which is consistent with the theoretical property that the MLE of 𝜆 λ in a simple Poisson model equals the sample mean.\n\n# Define a sequence of lambda values to evaluate\nlambda_vals &lt;- seq(0.5, 8, by = 0.1)\n\n# Calculate log-likelihood for each lambda\nlogliks &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y_vals))\n\n# Create a data frame for plotting\nloglik_df &lt;- data.frame(lambda = lambda_vals, loglik = logliks)\n\n# Plot\nggplot(loglik_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#0072B2\", size = 1) +\n  geom_vline(xintercept = lambda_vals[which.max(logliks)], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Poisson Log-Likelihood Curve\",\n       x = expression(lambda),\n       y = \"Log-Likelihood\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nWe begin with the log-likelihood function for independent observations ( Y_1, , Y_n () ):\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nThis simplifies to:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log(\\lambda) + \\text{const}\n\\]\nNow, take the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolving for ( ) gives:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator of ( ) is the sample mean of the observed data.\n\n# Define the negative log-likelihood to minimize\nneg_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) {\n    return(Inf)  # invalid values return high cost\n  }\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# Use optim() to minimize the negative log-likelihood\noptim_result &lt;- optim(par = 1,  # initial guess for lambda\n                      fn = neg_loglikelihood,\n                      Y = Y_vals,\n                      method = \"Brent\",\n                      lower = 0.01,\n                      upper = 10)\n\n# Extract MLE\nlambda_hat_optim &lt;- optim_result$par\nlambda_hat_optim\n\n[1] 3.684667\n\n\nTo estimate the Poisson rate parameter 𝜆 λ, we used R’s optim() function to numerically maximize the log-likelihood. Since optim() minimizes by default, we passed it the negative of our log-likelihood function. Using Brent’s method with bounds between 0.01 and 10, the optimizer returned a value of 𝜆 λ that aligns closely with the sample mean of the data. This confirms both our analytical derivation and visual inspection from the log-likelihood plot.\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Define the log-likelihood function for Poisson regression\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Compute lambda_i for all observations\n  lambda &lt;- exp(X %*% beta)\n  \n  # Compute the log-likelihood\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n\n# Create model matrix (design matrix) with intercept, age, age^2, region dummies, and iscustomer\nblueprinty_data &lt;- blueprinty_data %&gt;%\n  mutate(age_sq = age^2)\n\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty_data)\nY &lt;- blueprinty_data$patents\n\n\n# Use optim to estimate beta\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)\n  -sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n# Run optimization\noptim_result &lt;- optim(par = rep(0, ncol(X)),\n                      fn = poisson_regression_loglikelihood,\n                      Y = Y, X = X,\n                      hessian = TRUE,\n                      method = \"BFGS\",\n                      control = list(reltol = 1e-10))\n\n# Extract coefficients\nbeta_hat &lt;- optim_result$par\n\n# Compute standard errors using the inverse of the Hessian\nhessian &lt;- optim_result$hessian\nvar_cov_matrix &lt;- solve(hessian)\nstd_errors &lt;- sqrt(diag(var_cov_matrix))\n\n\n# Summarize coefficients and standard errors in a table\ncoef_table &lt;- data.frame(\n  Term = colnames(X),\n  Estimate = round(beta_hat, 4),\n  Std_Error = round(std_errors, 4)\n)\n\nknitr::kable(coef_table, caption = \"Poisson Regression Estimates and Standard Errors\")\n\n\nPoisson Regression Estimates and Standard Errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nI(age^2)\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\n\n# Fit the Poisson regression model using glm()\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, \n                 data = blueprinty_data, \n                 family = poisson())\n\n# Display a summary\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty_data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe results from both the optim() function and the glm() function are generally consistent, which confirms that the manual maximum likelihood estimation was implemented correctly. The coefficient for the iscustomer variable is positive and statistically significant in the glm() output (estimate = 0.208, p &lt; 0.001). This means that, holding other variables constant, firms that use Blueprinty’s software tend to receive more patents than non-customers. Since the model uses a log link function, this coefficient translates to approximately a 23% increase in the expected number of patents (because exp(0.208) is about 1.23).\nThe firm’s age also plays a significant role. The positive coefficient on age and the negative coefficient on age squared suggest a concave relationship, where patent activity increases with age up to a certain point and then declines. This is typical in lifecycle patterns of firm innovation.\nThe region variables are not statistically significant, which implies that, after accounting for age and customer status, there are no strong regional differences in patent outcomes.\nIn summary, the glm() results confirm the manual estimation and support the conclusion that using Blueprinty’s software is associated with a higher number of patents, even after adjusting for firm age and location.\n\n# Make copies of the data\ndata_0 &lt;- blueprinty_data\ndata_1 &lt;- blueprinty_data\n\n# Set customer status to 0 and 1 respectively\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\n# Use the fitted glm model to predict expected patent counts\ny_pred_0 &lt;- predict(glm_model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = data_1, type = \"response\")\n\n# Compute the average effect\naverage_difference &lt;- mean(y_pred_1 - y_pred_0)\naverage_difference\n\n[1] 0.7927681\n\n\nUsing the fitted Poisson regression model, we estimated the expected number of patents for each firm under two hypothetical scenarios: one where no firms use Blueprinty’s software and one where all firms do. The average difference between these two sets of predictions is approximately 0.79 patents per firm over five years. This suggests that, controlling for age and region, firms that use Blueprinty’s software are expected to receive nearly one additional patent on average compared to similar firms that do not use the software. This result provides strong evidence that Blueprinty’s tool may be associated with a meaningful improvement in patenting success."
  },
  {
    "objectID": "blog/Project 5/hw2_questions.html#airbnb-case-study",
    "href": "blog/Project 5/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\nlibrary(tidyverse)\n\n# Load the data\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# View variable summaries\nsummary(airbnb)\n\n      ...1             id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    1   Length:40628      \n 1st Qu.:10158   1st Qu.: 4889868   1st Qu.:  542   Class :character  \n Median :20315   Median : 9862878   Median :  996   Mode  :character  \n Mean   :20315   Mean   : 9698889   Mean   : 1102                     \n 3rd Qu.:30471   3rd Qu.:14667894   3rd Qu.: 1535                     \n Max.   :40628   Max.   :18009669   Max.   :42828                     \n                                                                      \n  host_since         room_type           bathrooms        bedrooms     \n Length:40628       Length:40628       Min.   :0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Mode  :character   Median :1.000   Median : 1.000  \n                                       Mean   :1.125   Mean   : 1.147  \n                                       3rd Qu.:1.000   3rd Qu.: 1.000  \n                                       Max.   :8.000   Max.   :10.000  \n                                       NA's   :160     NA's   :76      \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  0.0     Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  1.0     1st Qu.: 9.000           \n Median :  100.0   Median :  4.0     Median :10.000           \n Mean   :  144.8   Mean   : 15.9     Mean   : 9.198           \n 3rd Qu.:  170.0   3rd Qu.: 17.0     3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.0     Max.   :10.000           \n                                     NA's   :10195            \n review_scores_location review_scores_value instant_bookable\n Min.   : 2.000         Min.   : 2.000      Mode :logical   \n 1st Qu.: 9.000         1st Qu.: 9.000      FALSE:32759     \n Median :10.000         Median :10.000      TRUE :7869      \n Mean   : 9.414         Mean   : 9.332                      \n 3rd Qu.:10.000         3rd Qu.:10.000                      \n Max.   :10.000         Max.   :10.000                      \n NA's   :10254          NA's   :10256                       \n\n\n\n# Check for missing values in relevant columns\nairbnb %&gt;%\n  select(number_of_reviews, bathrooms, bedrooms, price,\n         review_scores_cleanliness, review_scores_location,\n         review_scores_value, instant_bookable, room_type) %&gt;%\n  summarise_all(~sum(is.na(.)))\n\n# A tibble: 1 × 9\n  number_of_reviews bathrooms bedrooms price review_scores_cleanliness\n              &lt;int&gt;     &lt;int&gt;    &lt;int&gt; &lt;int&gt;                     &lt;int&gt;\n1                 0       160       76     0                     10195\n# ℹ 4 more variables: review_scores_location &lt;int&gt;, review_scores_value &lt;int&gt;,\n#   instant_bookable &lt;int&gt;, room_type &lt;int&gt;\n\n# Drop rows with any missing values in relevant variables\nairbnb_clean &lt;- airbnb %&gt;%\n  filter(!is.na(number_of_reviews),\n         !is.na(bathrooms),\n         !is.na(bedrooms),\n         !is.na(price),\n         !is.na(review_scores_cleanliness),\n         !is.na(review_scores_location),\n         !is.na(review_scores_value),\n         !is.na(instant_bookable),\n         !is.na(room_type))\n\n\n# Number of rows before cleaning\nn_before &lt;- nrow(airbnb)\n\n# Number of rows after cleaning\nn_after &lt;- nrow(airbnb_clean)\n\n# Create a simple comparison table\ntibble(\n  Stage = c(\"Before Cleaning\", \"After Cleaning\"),\n  Rows = c(n_before, n_after)\n) %&gt;%\n  knitr::kable(caption = \"Number of Rows Before and After Dropping Missing Values\")\n\n\nNumber of Rows Before and After Dropping Missing Values\n\n\nStage\nRows\n\n\n\n\nBefore Cleaning\n40628\n\n\nAfter Cleaning\n30160\n\n\n\n\n\n\n# Histogram of number of reviews\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(binwidth = 10, fill = \"#0072B2\", color = \"black\") +\n  labs(title = \"Distribution of Number of Reviews\",\n       x = \"Number of Reviews\",\n       y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram of price\nggplot(airbnb_clean, aes(x = price)) +\n  geom_histogram(binwidth = 25, fill = \"#009E73\", color = \"black\") +\n  labs(title = \"Distribution of Price per Night\",\n       x = \"Price (USD)\",\n       y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram of bedrooms\nggplot(airbnb_clean, aes(x = bedrooms)) +\n  geom_histogram(binwidth = 1, fill = \"#D55E00\", color = \"black\") +\n  labs(title = \"Distribution of Bedrooms\",\n       x = \"Number of Bedrooms\",\n       y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\n\n\n\n\n\n# Fit a Poisson regression model\npoisson_model &lt;- glm(number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n                       review_scores_cleanliness + review_scores_location +\n                       review_scores_value + instant_bookable,\n                     data = airbnb_clean,\n                     family = poisson())\n\n# Display model summary\nsummary(poisson_model)\n\n\nCall:\nglm(formula = number_of_reviews ~ room_type + bathrooms + bedrooms + \n    price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson(), \n    data = airbnb_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\ninstant_bookableTRUE       3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe Poisson model assumes that the mean and variance of the outcome are equal. However, in practice (especially with review counts), overdispersion is common — meaning the variance is greater than the mean.\nTo account for this, we fit a Negative Binomial Regression, which introduces a dispersion parameter to relax the equal-mean-variance constraint. It’s a natural extension of the Poisson model.\n\n# Load MASS for negative binomial\nlibrary(MASS)\n\n# Fit negative binomial model\nnb_model &lt;- glm.nb(number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n                     review_scores_cleanliness + review_scores_location +\n                     review_scores_value + instant_bookable,\n                   data = airbnb_clean)\n\n# Display model summary\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = number_of_reviews ~ room_type + bathrooms + \n    bedrooms + price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, data = airbnb_clean, \n    init.theta = 0.7014236159, link = log)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                4.200e+00  9.429e-02  44.548  &lt; 2e-16 ***\nroom_typePrivate room      8.269e-03  1.514e-02   0.546    0.585    \nroom_typeShared room      -2.221e-01  4.351e-02  -5.105 3.30e-07 ***\nbathrooms                 -1.150e-01  2.029e-02  -5.669 1.44e-08 ***\nbedrooms                   7.404e-02  1.135e-02   6.522 6.96e-11 ***\nprice                     -1.253e-06  4.128e-05  -0.030    0.976    \nreview_scores_cleanliness  1.981e-01  8.045e-03  24.626  &lt; 2e-16 ***\nreview_scores_location    -1.099e-01  9.419e-03 -11.670  &lt; 2e-16 ***\nreview_scores_value       -2.119e-01  1.048e-02 -20.217  &lt; 2e-16 ***\ninstant_bookableTRUE       3.254e-01  1.766e-02  18.429  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.7014) family taken to be 1)\n\n    Null deviance: 35578  on 30159  degrees of freedom\nResidual deviance: 34576  on 30150  degrees of freedom\nAIC: 242052\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.70142 \n          Std. Err.:  0.00522 \n\n 2 x log-likelihood:  -242029.73500 \n\n\nBoth the Poisson and Negative Binomial regression models aim to explain variation in the number of Airbnb reviews (used as a proxy for bookings) as a function of property characteristics.\nKey Findings: Room Type: Compared to the baseline category (Entire home/apt), listings that are Private rooms or Shared rooms consistently receive fewer reviews. In the Poisson model, these have large negative coefficients, and in the Negative Binomial model, Shared room remains significantly negative, while Private room becomes statistically insignificant. This suggests that whole units are more popular/booked more frequently.\nBathrooms: The coefficient for bathrooms is negative and statistically significant in both models. This may reflect diminishing returns — beyond a certain point, more bathrooms don’t contribute to more bookings.\nBedrooms: The number of bedrooms has a strong, positive, and significant effect. Each additional bedroom is associated with a substantial increase in the expected number of reviews, suggesting that larger accommodations are more frequently booked.\nPrice: The effect of price is small and not consistently significant. In both models, the coefficients are near zero and not statistically meaningful, implying that price alone is not a strong predictor of bookings when other factors are accounted for.\nReview Scores:\nCleanliness has a positive and highly significant effect, indicating that higher cleanliness ratings are associated with more reviews/bookings.\nLocation and Value scores both show significant negative coefficients, which may seem counterintuitive. However, these scores are often compressed near the upper limit (e.g., most listings get 8–10), and the negative relationship may reflect lower variance or interactions with other unobserved factors.\nInstant Bookable: Listings that are instantly bookable receive significantly more reviews. In the Negative Binomial model, being instant bookable is associated with about a 38% increase in expected review count (exp(0.325) ≈ 1.38), holding all else constant.\nWhy Use Negative Binomial? The Negative Binomial model is preferred in this case due to overdispersion — the variance in number of reviews greatly exceeds the mean. The Negative Binomial model accounts for this with a dispersion parameter and provides a better fit, as shown by a substantially lower AIC (Poisson AIC: ~1,058,014 vs. NB AIC: ~242,052)."
  }
]